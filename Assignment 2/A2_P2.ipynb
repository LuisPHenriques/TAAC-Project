{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:48:14.088462900Z",
     "start_time": "2023-12-02T21:48:01.202349100Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we anticipate numerous out-of-vocabulary (OOV) words during test, the preferred tokenization method would be **subword tokenization**, because it performs well at handling OOV words by decomposing them into smaller, more common subunits known as subwords. By representing OOV words using subwords, the model can still capture meaningful information from these words, even if they haven't been explicitly encountered during training.\n",
    "\n",
    "\n",
    "Subword tokenization approaches fall into two main categories. **WordPiece** which splits words into character subsequences based on a predefined vocabulary of subwords. It's particularly effective as it can generate subwords that correspond to meaningful morphemes.\n",
    "**Byte Pair Encoding (BPE)** iteratively merges the most frequent byte pairs in the corpus until it reaches a specified vocabulary size. Unlike WordPiece, BPE doesn't need a predefined vocabulary, making it more adaptable to new words and languages.\n",
    "\n",
    "\n",
    "\n",
    "In situations where OOV words are prevalent, **subword tokenization** offers several advantages like:\n",
    " 1) **Reduced OOV Rate**, because Subwords effectively decompose OOV words into known units, significantly reducing the number of true OOV tokens encountered by the model;\n",
    " 2) **Improved Representation** by representing OOV words using subwords, the model can still extract contextual information and semantic relationships from these words, even if they haven't been explicitly trained on;\n",
    " 3) **Vocabulary Flexibility**: Subword tokenization techniques don't require a fixed vocabulary, allowing the model to adapt to new words and languages without explicit vocabulary updates.\n",
    "\n",
    "Therefore, **subword tokenization** is the preferred choice for scenarios where OOV words are expected, as it effectively handles these rare words while still preserving meaningful information for model training and prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### References:\n",
    " - https://www.datacamp.com/blog/what-is-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2. (B) I.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:48:14.107797800Z",
     "start_time": "2023-12-02T21:48:14.092462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 0, 0, 1, 0, 1, 1, 1, 1, 0]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(sentence, vocabulary):\n",
    "    # Initialize a vector with zeros for each word in the vocabulary\n",
    "    bag_of_words_vector = [0] * len(vocabulary)\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Count the frequency of each word in the sentence\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            index = vocabulary.index(word)\n",
    "            bag_of_words_vector[index] += 1\n",
    "\n",
    "    return bag_of_words_vector\n",
    "\n",
    "vocabulary = ['and', 'apple', 'banana', 'eat', 'hate', 'I', 'pie', 'strawberry', 'the', 'they']\n",
    "\n",
    "input_sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "result = bag_of_words(input_sentence, vocabulary)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (B) II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:48:14.229463100Z",
     "start_time": "2023-12-02T21:48:14.116325600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: and, TF: 0.142857, IDF: 0.105361, TF-IDF: 0.015052\n",
      "Word: apple, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "Word: banana, TF: 0.0, IDF: 1.89712, TF-IDF: 0.0\n",
      "Word: eat, TF: 0.142857, IDF: 0.916291, TF-IDF: 0.130899\n",
      "Word: hate, TF: 0.0, IDF: 2.302585, TF-IDF: 0.0\n",
      "Word: i, TF: 0.142857, IDF: 0.510826, TF-IDF: 0.072975\n",
      "Word: pie, TF: 0.142857, IDF: 1.609438, TF-IDF: 0.229919\n",
      "Word: strawberry, TF: 0.142857, IDF: 2.995732, TF-IDF: 0.427961\n",
      "Word: the, TF: 0.142857, IDF: 0.162519, TF-IDF: 0.023217\n",
      "Word: they, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "\n",
      "TF-IDF representation of vocabulary for sentence:\n",
      "[0.015052, 0.0, 0.0, 0.130899, 0.0, 0.072975, 0.229919, 0.427961, 0.023217, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"and\", \"apple\", \"banana\", \"eat\", \"hate\", \"I\", \"pie\", \"strawberry\", \"the\", \"they\"]\n",
    "# lower case all words in the vocabulary\n",
    "vocabulary = [word.lower() for word in vocabulary]\n",
    "\n",
    "document_counts = [90, 30, 15, 40, 10, 60, 20, 5, 85, 30]\n",
    "\n",
    "total_documents = 100\n",
    "\n",
    "sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.lower().split()\n",
    "\n",
    "# Compute TF-IDF representation\n",
    "tf_representation = []\n",
    "idf_representation = []\n",
    "tfidf_representation = []\n",
    "\n",
    "for term in vocabulary:\n",
    "    # Compute TF (Term Frequency)\n",
    "    tf = round(words.count(term) / len(words) if len(words) > 0 else 0, 6)\n",
    "    tf_representation.append(tf)\n",
    "\n",
    "    # Compute IDF (Inverse Document Frequency)\n",
    "    idf = round(math.log(total_documents / document_counts[vocabulary.index(term)]), 6)\n",
    "    idf_representation.append(idf)\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    tfidf = round(tf * idf, 6)\n",
    "    # Append TF-IDF value to the representation\n",
    "    tfidf_representation.append(tfidf)\n",
    "\n",
    "for word, tf, idf, tfidf in zip(vocabulary, tf_representation, idf_representation, tfidf_representation):\n",
    "    print(f\"Word: {word}, TF: {tf}, IDF: {idf}, TF-IDF: {tfidf}\")\n",
    "\n",
    "print()\n",
    "print(\"TF-IDF representation of vocabulary for sentence:\")\n",
    "print(tfidf_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "References\n",
    " - https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/#:~:text=The%20TF%2DIDF%20of%20a,multiplying%20TF%20and%20IDF%20scores.&text=Translated%20into%20plain%20English%2C%20importance,between%20documents%20measured%20by%20IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2. (c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \"\"\"RNN-based language model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        embedding_dim: The dimension of the word embeddings.\n",
    "        hidden_dim: The dimension of the hidden state of the RNN.\n",
    "        rnn_type: The type of RNN cell to use ('lstm' or 'gru').\n",
    "        num_layers: The number of layers of the RNN.\n",
    "        dropout: The dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: The word embeddings layer.\n",
    "        rnn: The recurrent neural network.\n",
    "        output_layer: The output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, rnn_type, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        rnn_class = nn.LSTM if rnn_type == 'lstm' else nn.GRU\n",
    "\n",
    "        self.rnn = rnn_class(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the language model.\n",
    "\n",
    "        Args:\n",
    "            x: A tensor of input sequences (with shape (batch_size, seq_len)).\n",
    "\n",
    "        Returns:\n",
    "            logits: A tensor of logits for the next word in the sequence (with shape (batch_size, seq_len, vocab_size)).\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = self.embeddings(x)\n",
    "        outputs, (h_n, _) = self.rnn(embeddings)\n",
    "        logits = self.output_layer(outputs)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, x, h0, no):\n",
    "        \"\"\"Generate text using the greedy decoding algorithm.\n",
    "\n",
    "        Args:\n",
    "            x: A tensor of input tokens (with shape (batch_size, 1)).\n",
    "            h0: The initial state of the RNN (with shape (batch_size, num_layers, hidden_dim)).\n",
    "            no: The desired number of tokens to be generated.\n",
    "\n",
    "        Returns:\n",
    "            decoded_text: A tensor of decoded text as a sequence of token indices (with shape (batch_size, no)).\n",
    "        \"\"\"\n",
    "\n",
    "        decoded_text = torch.zeros(x.size(0), no, dtype=torch.long, device=x.device)\n",
    "        output = x\n",
    "\n",
    "        for i in range(no):\n",
    "            embeddings = self.embeddings(output)\n",
    "            rnn_output, (h_n, _) = self.rnn(embeddings, h0)\n",
    "            logits = self.output_layer(rnn_output)\n",
    "            topk, topk_indices = logits.topk(1, dim=2)\n",
    "            output = topk_indices\n",
    "            decoded_text[:, i] = output[:, 0]\n",
    "            h0 = h_n.detach()\n",
    "\n",
    "        return decoded_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:48:14.229463100Z",
     "start_time": "2023-12-02T21:48:14.147092800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
