{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:12:05.729747300Z",
     "start_time": "2023-12-01T21:12:02.953693Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we anticipate numerous out-of-vocabulary (OOV) words during test, the preferred tokenization method would be **subword tokenization**, because it excels at handling OOV words by decomposing them into smaller, more common subunits known as subwords. By representing OOV words using subwords, the model can still capture meaningful information from these words, even if they haven't been explicitly encountered during training.\n",
    "\n",
    "\n",
    "Subword tokenization approaches fall into two main categories. **WordPiece** which splits words into character subsequences based on a predefined vocabulary of subwords. It's particularly effective as it can generate subwords that correspond to meaningful morphemes.\n",
    "**Byte Pair Encoding (BPE)** iteratively merges the most frequent byte pairs in the corpus until it reaches a specified vocabulary size. Unlike WordPiece, BPE doesn't need a predefined vocabulary, making it more adaptable to new words and languages.\n",
    "\n",
    "\n",
    "\n",
    "In situations where OOV words are prevalent, **subword tokenization** offers several advantages over traditional word-based tokenization:\n",
    "**Reduced OOV Rate**: Subwords effectively decompose OOV words into known units, significantly reducing the number of true OOV tokens encountered by the model.\n",
    "\n",
    "**Improved Representation**: By representing OOV words using subwords, the model can still extract contextual information and semantic relationships from these words, even if they haven't been explicitly trained on.\n",
    "\n",
    "**Vocabulary Flexibility**: Subword tokenization techniques don't require a fixed vocabulary, allowing the model to adapt to new words and languages without explicit vocabulary updates.\n",
    "\n",
    "Therefore, **subword tokenization** is the preferred choice for scenarios where OOV words are expected, as it effectively handles these rare words while still preserving meaningful information for model training and prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### References:\n",
    " - https://www.datacamp.com/blog/what-is-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2. (B) I.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:01:04.550685400Z",
     "start_time": "2023-11-29T15:01:04.529620300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(sentence, vocabulary):\n",
    "    # Initialize a vector with zeros for each word in the vocabulary\n",
    "    bag_of_words_vector = [0] * len(vocabulary)\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Count the frequency of each word in the sentence\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            index = vocabulary.index(word)\n",
    "            bag_of_words_vector[index] += 1\n",
    "\n",
    "    return bag_of_words_vector\n",
    "\n",
    "vocabulary = ['and', 'apple', 'banana', 'eat', 'hate', 'I', 'pie', 'strawberry', 'the', 'they']\n",
    "\n",
    "input_sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "result = bag_of_words(input_sentence, vocabulary)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (B) II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T21:12:15.483330700Z",
     "start_time": "2023-12-01T21:12:15.456780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: and, TF: 0.142857, IDF: 0.105361, TF-IDF: 0.015052\n",
      "Word: apple, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "Word: banana, TF: 0.0, IDF: 1.89712, TF-IDF: 0.0\n",
      "Word: eat, TF: 0.142857, IDF: 0.916291, TF-IDF: 0.130899\n",
      "Word: hate, TF: 0.0, IDF: 2.302585, TF-IDF: 0.0\n",
      "Word: i, TF: 0.142857, IDF: 0.510826, TF-IDF: 0.072975\n",
      "Word: pie, TF: 0.142857, IDF: 1.609438, TF-IDF: 0.229919\n",
      "Word: strawberry, TF: 0.142857, IDF: 2.995732, TF-IDF: 0.427961\n",
      "Word: the, TF: 0.142857, IDF: 0.162519, TF-IDF: 0.023217\n",
      "Word: they, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "\n",
      "TF-IDF representation of vocabulary for sentence:\n",
      "[0.015052, 0.0, 0.0, 0.130899, 0.0, 0.072975, 0.229919, 0.427961, 0.023217, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"and\", \"apple\", \"banana\", \"eat\", \"hate\", \"I\", \"pie\", \"strawberry\", \"the\", \"they\"]\n",
    "# lower case all words in the vocabulary\n",
    "vocabulary = [word.lower() for word in vocabulary]\n",
    "\n",
    "document_counts = [90, 30, 15, 40, 10, 60, 20, 5, 85, 30]\n",
    "\n",
    "total_documents = 100\n",
    "\n",
    "sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.lower().split()\n",
    "\n",
    "# Compute TF-IDF representation\n",
    "tf_representation = []\n",
    "idf_representation = []\n",
    "tfidf_representation = []\n",
    "\n",
    "for term in vocabulary:\n",
    "    # Compute TF (Term Frequency)\n",
    "    tf = round(words.count(term) / len(words) if len(words) > 0 else 0, 6)\n",
    "    tf_representation.append(tf)\n",
    "\n",
    "    # Compute IDF (Inverse Document Frequency)\n",
    "    idf = round(math.log(total_documents / document_counts[vocabulary.index(term)]), 6)\n",
    "    idf_representation.append(idf)\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    tfidf = round(tf * idf, 6)\n",
    "    # Append TF-IDF value to the representation\n",
    "    tfidf_representation.append(tfidf)\n",
    "\n",
    "for word, tf, idf, tfidf in zip(vocabulary, tf_representation, idf_representation, tfidf_representation):\n",
    "    print(f\"Word: {word}, TF: {tf}, IDF: {idf}, TF-IDF: {tfidf}\")\n",
    "\n",
    "print()\n",
    "print(\"TF-IDF representation of vocabulary for sentence:\")\n",
    "print(tfidf_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "References\n",
    " - https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/#:~:text=The%20TF%2DIDF%20of%20a,multiplying%20TF%20and%20IDF%20scores.&text=Translated%20into%20plain%20English%2C%20importance,between%20documents%20measured%20by%20IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:03:43.860357400Z",
     "start_time": "2023-11-28T17:03:39.407274100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMLanguageModel.\n",
    "\n",
    "        Parameters:\n",
    "            - vocab_size (int): Size of the vocabulary.\n",
    "            - embed_size (int): Size of the word embeddings.\n",
    "            - hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            - num_layers (int): Number of layers in the LSTM.\n",
    "        \"\"\"\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the LSTMLanguageModel.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): Input sequence of token indices (batch_size, sequence_length).\n",
    "            - h0 (torch.Tensor, optional): Initial hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            - output (torch.Tensor): Output sequence from the language model (batch_size, sequence_length, vocab_size).\n",
    "            - hn (torch.Tensor): Final hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        output, hn = self.lstm(embedded, h0)\n",
    "        output = self.fc(output)\n",
    "        return output, hn\n",
    "\n",
    "    def generate(self, x, h0=None, no=10):\n",
    "        \"\"\"\n",
    "        Generates a sequence of token indices using greedy decoding.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): Input token index (batch_size, 1).\n",
    "            - h0 (torch.Tensor, optional): Initial hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "            - no (int): Number of tokens to be generated.\n",
    "\n",
    "        Returns:\n",
    "            - generated_sequence (torch.Tensor): Decoded sequence of token indices (batch_size, no).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            generated_sequence = []\n",
    "            current_token = x\n",
    "\n",
    "            for _ in range(no):\n",
    "                output, hn = self.forward(current_token, h0)\n",
    "                probabilities = nn.functional.softmax(output[:, -1, :], dim=1)\n",
    "                next_token = torch.argmax(probabilities, dim=1, keepdim=True)\n",
    "                generated_sequence.append(next_token)\n",
    "                current_token = next_token\n",
    "\n",
    "            generated_sequence = torch.cat(generated_sequence, dim=1)\n",
    "            return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:09:18.527840100Z",
     "start_time": "2023-11-29T15:09:18.507841300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \"\"\"Recurrent Neural Network (RNN) Language Model\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, rnn_type='lstm'):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNNLM model\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of token indices (integers)\n",
    "                Shape: (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Unnormalized log probabilities of the next word for each token in the input sequence\n",
    "                Shape: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, x, h0, no):\n",
    "        \"\"\"\n",
    "        Generate text using the greedy decoding algorithm\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token (integer)\n",
    "                Shape: (1,)\n",
    "            h0 (tuple): Initial hidden state of the RNN\n",
    "                Shape: (num_layers, batch_size, hidden_dim)\n",
    "            no (int): Desired number of tokens to be generated\n",
    "\n",
    "        Returns:\n",
    "            decoded_tokens (torch.Tensor): Sequence of token indices (integers) representing the generated text\n",
    "                Shape: (no,)\n",
    "        \"\"\"\n",
    "        decoded_tokens = torch.empty(no, dtype=torch.long, device=x.device)\n",
    "        for i in range(no):\n",
    "            embedded = self.embedding(x.unsqueeze(0))\n",
    "            output, h0 = self.rnn(embedded, h0)\n",
    "            logits = self.linear(output.squeeze(0))\n",
    "            pred = logits.argmax(dim=1)\n",
    "            decoded_tokens[i] = pred\n",
    "\n",
    "            x = pred.unsqueeze(0)\n",
    "\n",
    "        return decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
