{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:00:42.193241700Z",
     "start_time": "2023-11-29T15:00:37.112580600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (B) I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:01:04.550685400Z",
     "start_time": "2023-11-29T15:01:04.529620300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(sentence, vocabulary):\n",
    "    # Initialize a vector with zeros for each word in the vocabulary\n",
    "    bag_of_words_vector = [0] * len(vocabulary)\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Count the frequency of each word in the sentence\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            index = vocabulary.index(word)\n",
    "            bag_of_words_vector[index] += 1\n",
    "\n",
    "    return bag_of_words_vector\n",
    "\n",
    "vocabulary = ['and', 'apple', 'banana', 'eat', 'hate', 'I', 'pie', 'strawberry', 'the', 'they']\n",
    "\n",
    "input_sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "result = bag_of_words(input_sentence, vocabulary)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2. (B) II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:07:10.585049Z",
     "start_time": "2023-11-29T15:07:10.562989600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: and, TF: 1, IDF: 0.10536051565782635, TF-IDF: 0.10536051565782635\n",
      "Word: I, TF: 1, IDF: 0.5108256237659907, TF-IDF: 0.5108256237659907\n",
      "Word: eat, TF: 1, IDF: 0.9162907318741551, TF-IDF: 0.9162907318741551\n",
      "Word: the, TF: 1, IDF: 0.16251892949777494, TF-IDF: 0.16251892949777494\n",
      "Word: strawberry, TF: 1, IDF: 2.995732273553991, TF-IDF: 2.995732273553991\n",
      "Word: pie, TF: 1, IDF: 1.6094379124341003, TF-IDF: 1.6094379124341003\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {\n",
    "    'and': 90,\n",
    "    'apple': 30,\n",
    "    'banana': 15,\n",
    "    'eat': 40,\n",
    "    'hate': 10,\n",
    "    'I': 60,\n",
    "    'pie': 20,\n",
    "    'strawberry': 5,\n",
    "    'the': 85,\n",
    "    'they': 30\n",
    "}\n",
    "\n",
    "# Given sentence\n",
    "sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "# Calculate TF\n",
    "tf = {}\n",
    "for word in sentence.split():\n",
    "    tf[word] = tf.get(word, 0) + 1\n",
    "\n",
    "# Calculate IDF\n",
    "idf = {}\n",
    "total_documents = 100\n",
    "for word in vocabulary:\n",
    "    document_frequency = vocabulary[word]\n",
    "    idf[word] = math.log(total_documents / document_frequency)\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = {}\n",
    "for word in tf:\n",
    "    try:\n",
    "        tf_idf[word] = tf[word] * idf[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "# Print TF-IDF representation\n",
    "for word in tf_idf:\n",
    "    print(f\"Word: {word}, TF: {tf[word]}, IDF: {idf[word]}, TF-IDF: {tf_idf[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T18:45:48.185124400Z",
     "start_time": "2023-11-28T18:45:48.176324100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: and, TF: 0.142857, IDF: 0.105361, TF-IDF: 0.015052\n",
      "Word: apple, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "Word: banana, TF: 0.0, IDF: 1.89712, TF-IDF: 0.0\n",
      "Word: eat, TF: 0.142857, IDF: 0.916291, TF-IDF: 0.130899\n",
      "Word: hate, TF: 0.0, IDF: 2.302585, TF-IDF: 0.0\n",
      "Word: i, TF: 0.142857, IDF: 0.510826, TF-IDF: 0.072975\n",
      "Word: pie, TF: 0.142857, IDF: 1.609438, TF-IDF: 0.229919\n",
      "Word: strawberry, TF: 0.142857, IDF: 2.995732, TF-IDF: 0.427961\n",
      "Word: the, TF: 0.142857, IDF: 0.162519, TF-IDF: 0.023217\n",
      "Word: they, TF: 0.0, IDF: 1.203973, TF-IDF: 0.0\n",
      "\n",
      "TF-IDF representation of vocabulary for sentence:\n",
      "[0.015052, 0.0, 0.0, 0.130899, 0.0, 0.072975, 0.229919, 0.427961, 0.023217, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"and\", \"apple\", \"banana\", \"eat\", \"hate\", \"I\", \"pie\", \"strawberry\", \"the\", \"they\"]\n",
    "# lower case all words in the vocabulary\n",
    "vocabulary = [word.lower() for word in vocabulary]\n",
    "\n",
    "document_counts = [90, 30, 15, 40, 10, 60, 20, 5, 85, 30]\n",
    "\n",
    "total_documents = 100\n",
    "\n",
    "sentence = \"You and I eat the strawberry pie\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.lower().split()\n",
    "\n",
    "# Compute TF-IDF representation\n",
    "tf_representation = []\n",
    "idf_representation = []\n",
    "tfidf_representation = []\n",
    "\n",
    "for term in vocabulary:\n",
    "    # Compute TF (Term Frequency)\n",
    "    tf = round(words.count(term) / len(words) if len(words) > 0 else 0, 6)\n",
    "    tf_representation.append(tf)\n",
    "\n",
    "    # Compute IDF (Inverse Document Frequency)\n",
    "    idf = round(math.log(total_documents / document_counts[vocabulary.index(term)]), 6)\n",
    "    idf_representation.append(idf)\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    tfidf = round(tf * idf, 6)\n",
    "    # Append TF-IDF value to the representation\n",
    "    tfidf_representation.append(tfidf)\n",
    "\n",
    "for word, tf, idf, tfidf in zip(vocabulary, tf_representation, idf_representation, tfidf_representation):\n",
    "    print(f\"Word: {word}, TF: {tf}, IDF: {idf}, TF-IDF: {tfidf}\")\n",
    "\n",
    "print()\n",
    "print(\"TF-IDF representation of vocabulary for sentence:\")\n",
    "print(tfidf_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "References\n",
    " - https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/#:~:text=The%20TF%2DIDF%20of%20a,multiplying%20TF%20and%20IDF%20scores.&text=Translated%20into%20plain%20English%2C%20importance,between%20documents%20measured%20by%20IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:03:43.860357400Z",
     "start_time": "2023-11-28T17:03:39.407274100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMLanguageModel.\n",
    "\n",
    "        Parameters:\n",
    "            - vocab_size (int): Size of the vocabulary.\n",
    "            - embed_size (int): Size of the word embeddings.\n",
    "            - hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            - num_layers (int): Number of layers in the LSTM.\n",
    "        \"\"\"\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the LSTMLanguageModel.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): Input sequence of token indices (batch_size, sequence_length).\n",
    "            - h0 (torch.Tensor, optional): Initial hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            - output (torch.Tensor): Output sequence from the language model (batch_size, sequence_length, vocab_size).\n",
    "            - hn (torch.Tensor): Final hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        output, hn = self.lstm(embedded, h0)\n",
    "        output = self.fc(output)\n",
    "        return output, hn\n",
    "\n",
    "    def generate(self, x, h0=None, no=10):\n",
    "        \"\"\"\n",
    "        Generates a sequence of token indices using greedy decoding.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): Input token index (batch_size, 1).\n",
    "            - h0 (torch.Tensor, optional): Initial hidden state (num_layers * num_directions, batch_size, hidden_size).\n",
    "            - no (int): Number of tokens to be generated.\n",
    "\n",
    "        Returns:\n",
    "            - generated_sequence (torch.Tensor): Decoded sequence of token indices (batch_size, no).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            generated_sequence = []\n",
    "            current_token = x\n",
    "\n",
    "            for _ in range(no):\n",
    "                output, hn = self.forward(current_token, h0)\n",
    "                probabilities = nn.functional.softmax(output[:, -1, :], dim=1)\n",
    "                next_token = torch.argmax(probabilities, dim=1, keepdim=True)\n",
    "                generated_sequence.append(next_token)\n",
    "                current_token = next_token\n",
    "\n",
    "            generated_sequence = torch.cat(generated_sequence, dim=1)\n",
    "            return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:09:18.527840100Z",
     "start_time": "2023-11-29T15:09:18.507841300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \"\"\"Recurrent Neural Network (RNN) Language Model\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, rnn_type='lstm'):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNNLM model\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence of token indices (integers)\n",
    "                Shape: (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Unnormalized log probabilities of the next word for each token in the input sequence\n",
    "                Shape: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, x, h0, no):\n",
    "        \"\"\"\n",
    "        Generate text using the greedy decoding algorithm\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token (integer)\n",
    "                Shape: (1,)\n",
    "            h0 (tuple): Initial hidden state of the RNN\n",
    "                Shape: (num_layers, batch_size, hidden_dim)\n",
    "            no (int): Desired number of tokens to be generated\n",
    "\n",
    "        Returns:\n",
    "            decoded_tokens (torch.Tensor): Sequence of token indices (integers) representing the generated text\n",
    "                Shape: (no,)\n",
    "        \"\"\"\n",
    "        decoded_tokens = torch.empty(no, dtype=torch.long, device=x.device)\n",
    "        for i in range(no):\n",
    "            embedded = self.embedding(x.unsqueeze(0))\n",
    "            output, h0 = self.rnn(embedded, h0)\n",
    "            logits = self.linear(output.squeeze(0))\n",
    "            pred = logits.argmax(dim=1)\n",
    "            decoded_tokens[i] = pred\n",
    "\n",
    "            x = pred.unsqueeze(0)\n",
    "\n",
    "        return decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
