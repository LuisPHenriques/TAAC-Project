{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Custom Functions & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T14:50:51.762292300Z",
     "start_time": "2023-12-21T14:50:49.382402900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n",
    "import spacy \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Ignore RuntimeWarning and UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:02:13.334929800Z",
     "start_time": "2023-12-21T15:02:13.276939Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub('http\\S*', ' ', text)\n",
    "    \n",
    "    # remove non-alphabetic\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove one character word\n",
    "    text = re.sub(\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(\"^[a-zA-Z]\\s+\", '', text)\n",
    "    \n",
    "    # replace double space to one space\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    \n",
    "    # tokenize, lemmatize, remove stop words\n",
    "    doc = nlp(text)\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def train_BPE_tokenizer(train_text_df, text_column, vocab_size=5000, min_frequency=2):\n",
    "    # Extract the text from the specified column\n",
    "    texts = train_text_df[text_column].tolist()\n",
    "\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Train the BPE tokenizer on the text\n",
    "    tokenizer.train_from_iterator(texts, vocab_size=vocab_size, min_frequency=min_frequency)\n",
    "\n",
    "    # Tokenize the text in the DataFrame and remove \"Ġ\" character\n",
    "    train_text_df[text_column + '_tokenized'] = train_text_df[text_column].apply(lambda x: [token.replace(\"Ġ\", \"\") for token in tokenizer.encode(x).tokens])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize_text(tokenizer, df, text_column):\n",
    "    # Extract the text from the specified column\n",
    "    texts = df[text_column].tolist()\n",
    "\n",
    "    # Tokenize the text in the DataFrame using the pre-trained tokenizer and remove \"Ġ\" character\n",
    "    df[text_column + '_tokenized'] = [[token.replace(\"Ġ\", \"\") for token in tokenizer.encode(text).tokens] for text in texts]\n",
    "\n",
    "\n",
    "class BPE():\n",
    "    \"\"\"Byte-Pair Encoding: Subword-based tokenization algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train BPE tokenizer.\"\"\"\n",
    "    # (1) Compute the frequencies of each word in the corpus\n",
    "    # (2) Compute the base vocabulary of all characters in the corpus\n",
    "    # (3) Split each word into individual characters before training\n",
    "    # (4) Merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "\n",
    "        # compute the frequencies of each word in the corpus\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # compute the base vocabulary of all characters in the corpus\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.append(letter)\n",
    "        alphabet.sort()\n",
    "\n",
    "        # add the special token </w> at the beginning of the vocabulary\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # split each word into individual characters before training\n",
    "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
    "\n",
    "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "        while len(vocab) < self.vocab_size:\n",
    "\n",
    "            # compute the frequency of each pair\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "\n",
    "            # find the most frequent pair\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # merge the most frequent pair\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"Merge the given pair.\"\"\"\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "    \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
    "        \n",
    "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result\n",
    "    \n",
    "\n",
    "def get_tfidf_matrix(df, vectorizer):\n",
    "    \n",
    "    # Convert the TF-IDF matrix to a dense NumPy array\n",
    "    matrix = df.todense()\n",
    "\n",
    "    # Convert the dense matrix to a DataFrame\n",
    "    matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# The sequences being in the formar ['word1', 'word2', 'word3', ...], preprocess it\n",
    "def string2embedding_idx(text_sequence, model):\n",
    "\n",
    "    sequence = []\n",
    "    for token in text_sequence:\n",
    "        try:\n",
    "            sequence.append(model.wv.key_to_index[token])\n",
    "        except:\n",
    "            sequence.append(2899)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def TSNE_10ClosestWords(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "            \n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity = 10)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "class TweetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, word2vec_model):\n",
    "        self.df = df\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx, -1 if self.word2vec_model == 'skipgram' else -2]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        # Convert sequence to a 1D tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Convert label to a 1D tensor (scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return sequence_tensor, label_tensor\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print('Device:', device)\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_acc_hist (list): List containing training accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_fn', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predicted = (output > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in pbar:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (output > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_BERT(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_acc_hist (list): List containing training accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_fn', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(input_ids, attention_mask=attn_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.squeeze(-1)\n",
    "\n",
    "            loss = loss_fn(logits, target.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = (logits.data > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].to(device)\n",
    "                model.zero_grad()\n",
    "                output = model(input_ids, attention_mask=attn_mask)\n",
    "                logits = output.logits\n",
    "                logits = logits.squeeze(-1)\n",
    "\n",
    "                loss = loss_fn(logits, target.float())\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (logits.data > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings\n",
    "class CustomLSTM(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings = torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "        return output\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings\n",
    "class CustomGRU(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings = torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "        return output\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, output):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attn(output)\n",
    "\n",
    "        return torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomGRU_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomGRU_Attention, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        self.attention = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        attention_weights = torch.nn.functional.softmax(self.attention(output), dim = 1)\n",
    "        output = torch.sum(attention_weights * output, dim = 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomLSTM_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomLSTM_Attention, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.attention = Attention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embedding of each token in the sequence\n",
    "        x = self.embedding(x)\n",
    "        # Apply LSTM to the sequence of embeddings\n",
    "        output, _ = self.lstm(x)\n",
    "        # Apply the attention mechanism and get attention weights\n",
    "        attn_weights = self.attention(output)\n",
    "        # Compute the weighted sum of the hidden states\n",
    "        output = torch.sum(attn_weights * output, dim = 1)\n",
    "        # Compute the logits\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and multi-head attention mechanism\n",
    "class CustomLSTM_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, dropout=0.1, num_layers=1, bidirectional=False, freeze_embeddings=True, num_heads=8):\n",
    "        super(CustomLSTM_MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.sequence_size = 49\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attention = torch.nn.MultiheadAttention(embed_dim=hidden_size * (2 if bidirectional else 1), num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Fully-connected layers for classification head\n",
    "        self.fc1 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1) * self.sequence_size, hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc2 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size * (2 if bidirectional else 1))\n",
    "        self.classification_head = torch.nn.Sequential(self.fc1, self.relu, self.bn, self.dropout, self.fc2)\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization\n",
    "        for layer in self.classification_head:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity = 'relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the multi-head attention\n",
    "        torch.nn.init.kaiming_uniform_(self.multihead_attention.in_proj_weight, nonlinearity = 'relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the LSTM\n",
    "        for layer in self.lstm._all_weights:\n",
    "            for param_name in layer:\n",
    "                if 'weight' in param_name:\n",
    "                    torch.nn.init.kaiming_uniform_(getattr(self.lstm, param_name), nonlinearity = 'relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embedding of each token in the sequence\n",
    "        embed = self.embedding(x)\n",
    "        # Apply LSTM to the sequence of embeddings\n",
    "        hx, cx = self.lstm(embed)\n",
    "        # Apply multihead attention and get attention weights\n",
    "        attn_output, attn_weights = self.multihead_attention(hx, hx, hx)\n",
    "        # Flatten or pool the multihead attention outputs across the sequence dimension\n",
    "        flattened_output = attn_output.reshape(attn_output.size(0), -1)\n",
    "        # Compute the logits considering the weighted values (V) of the multihead attention\n",
    "        logits = self.classification_head(flattened_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.positional_encoding = torch.zeros((1, max_len, d_model))\n",
    "        self.positional_encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, embedding_dim, num_heads, hidden_dim, num_layers, max_len=512, dropout=0.1, freeze_embeddings = True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_len)\n",
    "        \n",
    "        self.transformer_encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "                                                                          d_model=embedding_dim,\n",
    "                                                                          nhead=num_heads,\n",
    "                                                                          dim_feedforward=hidden_dim,\n",
    "                                                                          dropout=dropout\n",
    "                                                                          )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(self.transformer_encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.permute(1, 0, 2)  # Change from (batch_size, seq_len, embedding_dim) to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Change back to (batch_size, seq_len, embedding_dim)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEncoderForClassification(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, embedding_dim, num_heads, hidden_dim, num_layers, max_len=512, dropout=0.1, freeze_embeddings = True, num_classes=1):\n",
    "        super(TransformerEncoderForClassification, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder = TransformerEncoder(word2vec_model, num_heads, hidden_dim, num_layers, max_len, dropout, freeze_embeddings)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = torch.nn.Linear(self.embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        \n",
    "        # Global average pooling along the sequence dimension\n",
    "        pooled_output = torch.nn.functional.adaptive_avg_pool1d(encoder_output.permute(0, 2, 1), 1).squeeze(-1)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# FCNN model to be used with the TF-IDF features\n",
    "class CustomFCNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomFCNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:27:53.432052200Z",
     "start_time": "2023-12-21T15:27:53.373916800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                 Just happened a terrible car crash       1\n",
       "1  Heard about #earthquake is different cities, s...       1\n",
       "2  there is a forest fire at spot pond, geese are...       1\n",
       "3           Apocalypse lighting. #Spokane #wildfires       1\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_train = pd.read_csv('../data/tweets_data/train.csv')[['text', 'target']].reset_index(drop=True)\n",
    "tweets_test = pd.read_csv('../data/tweets_data/test.csv')[['id', 'text']]\n",
    "tweets_labels = pd.read_csv('../data/tweets_data/test_labels.csv', encoding='latin-1')[['choose_one', 'text']]\n",
    "\n",
    "tweets_labels['target'] = (tweets_labels['choose_one']=='Relevant').astype(int)\n",
    "tweets_labels['id'] = tweets_labels.index\n",
    "\n",
    "tweets_test = pd.merge(left = tweets_test, right = tweets_labels, on='id', how = 'left')[['id', 'text_x', 'target']]\n",
    "tweets_test.rename(columns={'text_x': 'text'}, inplace=True)\n",
    "tweets_test = tweets_test[['text', 'target']]\n",
    "\n",
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text using key-words and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.782176300Z",
     "start_time": "2023-12-21T15:27:58.466800200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_train['clean_text'] = tweets_train['text'].apply(preprocess)\n",
    "tweets_test['clean_text'] = tweets_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.798553700Z",
     "start_time": "2023-12-21T15:29:01.785391100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \n",
       "0               deed reason earthquake allah forgive  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3    people receive wildfire evacuation order cal...  \n",
       "4  got send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \n",
       "0                     happen terrible car crash  \n",
       "1      hear earthquake different city stay safe  \n",
       "2  forest fire spot pond goose flee street save  \n",
       "3          apocalypse lighting spokane wildfire  \n",
       "4            typhoon soudelor kill china taiwan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word tokenization with BERT Tokenizer for Byte-Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the BPE tokenizer on the training data\n",
    "Tokenizer = train_BPE_tokenizer(train_text_df = tweets_train, text_column = 'clean_text', vocab_size = 20000, min_frequency = 1)\n",
    "\n",
    "# Assuming Tokenizer is the pre-trained tokenizer from the training data\n",
    "# Tokenize the test data using the pre-trained tokenizer\n",
    "tokenize_text(tokenizer = Tokenizer, df = tweets_test, text_column = 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ron, ge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[, people, receive, wildfire, evacuation, orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0         [deed, reason, earthquake, allah, forgive]  \n",
       "1    [forest, fire, near, la, ron, ge, sask, canada]  \n",
       "2  [resident, ask, shelter, place, notify, office...  \n",
       "3  [, people, receive, wildfire, evacuation, orde...  \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, sp, okane, wildfire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0                     [happen, terrible, car, crash]  \n",
       "1    [hear, earthquake, different, city, stay, safe]  \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...  \n",
       "3        [apocalypse, lighting, sp, okane, wildfire]  \n",
       "4           [typhoon, soudelor, kill, china, taiwan]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:30:29.849421800Z",
     "start_time": "2023-12-21T15:29:11.378522300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# set the hyperparameter of vocabulary size\\nvocab_size = 3000\\ncorpus = tweets_train['clean_text'].tolist()\\n\\n# create a BPE tokenizer object\\nMyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\\n\\n# train BPE tokenizer with Wikipedia corpus\\nMyBPE.train()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# set the hyperparameter of vocabulary size\n",
    "vocab_size = 3000\n",
    "corpus = tweets_train['clean_text'].tolist()\n",
    "\n",
    "# create a BPE tokenizer object\n",
    "MyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\n",
    "\n",
    "# train BPE tokenizer with Wikipedia corpus\n",
    "MyBPE.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:13.493640100Z",
     "start_time": "2023-12-21T15:32:21.919839300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tweets_train['tokenized_text'] = tweets_train['clean_text'].apply(lambda x: MyBPE.tokenize(x))\\ntweets_test['tokenized_text'] = tweets_test['clean_text'].apply(lambda x: MyBPE.tokenize(x))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tweets_train['tokenized_text'] = tweets_train['clean_text'].apply(lambda x: MyBPE.tokenize(x))\n",
    "tweets_test['tokenized_text'] = tweets_test['clean_text'].apply(lambda x: MyBPE.tokenize(x))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text into Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.476189500Z",
     "start_time": "2023-12-21T15:33:21.430017200Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(tweets_train['clean_text_tokenized'].apply(lambda tokens: ' '.join(tokens)))\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_train['TFIDF'] = X_train.toarray().tolist()\n",
    "\n",
    "X_test = vectorizer.transform(tweets_test['clean_text_tokenized'].apply(lambda tokens: ' '.join(tokens)))\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_test['TFIDF'] = X_test.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.661826500Z",
     "start_time": "2023-12-21T15:33:23.626279500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ron, ge, sask, canada]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[, people, receive, wildfire, evacuation, orde...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0         [deed, reason, earthquake, allah, forgive]   \n",
       "1    [forest, fire, near, la, ron, ge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [, people, receive, wildfire, evacuation, orde...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, sp, okane, wildfire]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0                     [happen, terrible, car, crash]   \n",
       "1    [hear, earthquake, different, city, stay, safe]   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3        [apocalypse, lighting, sp, okane, wildfire]   \n",
       "4           [typhoon, soudelor, kill, china, taiwan]   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (CBOW and Skip-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:26.004312100Z",
     "start_time": "2023-12-21T15:33:25.980689800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized text in training data: 26\n",
      "Maximum length of tokenized text in testing data: 45\n"
     ]
    }
   ],
   "source": [
    "print('Maximum length of tokenized text in training data:', max([len(sent) for sent in tweets_train['clean_text_tokenized']]))\n",
    "print('Maximum length of tokenized text in testing data:', max([len(sent) for sent in tweets_test['clean_text_tokenized']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:07.773767Z",
     "start_time": "2023-12-21T15:33:29.941367700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "cbow_model = Word2Vec(sentences = tweets_train['clean_text_tokenized'], vector_size = 1024, window = 5, min_count = 1, workers = 4, sg = 0, epochs = 256)\n",
    "# Add the <pad> token to the cbow_model so that the last token is the <pad> token\n",
    "cbow_model.wv.key_to_index['<pad>'] = len(cbow_model.wv)\n",
    "# Add the embedding of <pad> token to the cbow_model\n",
    "cbow_model.wv.vectors = np.append(cbow_model.wv.vectors, np.zeros((1, 1024)), axis=0)\n",
    "\n",
    "\n",
    "skipgram_model = Word2Vec(sentences = tweets_train['clean_text_tokenized'], vector_size = 1024, window = 5, min_count = 1, workers = 4, sg = 1, epochs = 256)\n",
    "# Add the <pad> token to the skipgram_model so that the last token is the <pad> token\n",
    "skipgram_model.wv.key_to_index['<pad>'] = len(skipgram_model.wv)\n",
    "# Add the embedding of <pad> token to the skipgram_model\n",
    "skipgram_model.wv.vectors = np.append(skipgram_model.wv.vectors, np.zeros((1, 1024)), axis=0)\n",
    "\n",
    "# Add the <pad> token to all the tokenized text until the length of each tokenized text is 50\n",
    "tweets_train['clean_text_tokenized'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: tokens + ['<pad>'] * (45 - len(tokens)))\n",
    "tweets_test['clean_text_tokenized'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: tokens + ['<pad>'] * (45 - len(tokens)))\n",
    "\n",
    "# Apply the string2embedding_idx function to create a new column\n",
    "tweets_train['CBOW_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_train['SkipGram_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))\n",
    "tweets_test['CBOW_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_test['SkipGram_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:36.442786700Z",
     "start_time": "2023-12-21T15:34:36.381065300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, &lt;pa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3996, 461, 209, 1422, 2511, 13815, 13815, 138...</td>\n",
       "      <td>[3996, 461, 209, 1422, 2511, 13815, 13815, 138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ron, ge, sask, canada...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[120, 2, 176, 514, 2045, 1500, 6753, 1137, 138...</td>\n",
       "      <td>[120, 2, 176, 514, 2045, 1500, 6753, 1137, 138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1679, 515, 1901, 382, 6750, 322, 205, 1901, 3...</td>\n",
       "      <td>[1679, 515, 1901, 382, 6750, 322, 205, 1901, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[, people, receive, wildfire, evacuation, orde...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 8, 2988, 84, 205, 292, 38, 13815, 13815, 1...</td>\n",
       "      <td>[0, 8, 2988, 84, 205, 292, 38, 13815, 13815, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[312, 207, 126, 6038, 1457, 217, 84, 3009, 125...</td>\n",
       "      <td>[312, 207, 126, 6038, 1457, 217, 84, 3009, 125...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [deed, reason, earthquake, allah, forgive, <pa...   \n",
       "1  [forest, fire, near, la, ron, ge, sask, canada...   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [, people, receive, wildfire, evacuation, orde...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [3996, 461, 209, 1422, 2511, 13815, 13815, 138...   \n",
       "1  [120, 2, 176, 514, 2045, 1500, 6753, 1137, 138...   \n",
       "2  [1679, 515, 1901, 382, 6750, 322, 205, 1901, 3...   \n",
       "3  [0, 8, 2988, 84, 205, 292, 38, 13815, 13815, 1...   \n",
       "4  [312, 207, 126, 6038, 1457, 217, 84, 3009, 125...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [3996, 461, 209, 1422, 2511, 13815, 13815, 138...  \n",
       "1  [120, 2, 176, 514, 2045, 1500, 6753, 1137, 138...  \n",
       "2  [1679, 515, 1901, 382, 6750, 322, 205, 1901, 3...  \n",
       "3  [0, 8, 2988, 84, 205, 292, 38, 13815, 13815, 1...  \n",
       "4  [312, 207, 126, 6038, 1457, 217, 84, 3009, 125...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, &lt;pad&gt;, &lt;pad&gt;, &lt;...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[201, 1920, 43, 16, 13815, 13815, 13815, 13815...</td>\n",
       "      <td>[201, 1920, 43, 16, 13815, 13815, 13815, 13815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[141, 209, 1039, 122, 390, 1008, 13815, 13815,...</td>\n",
       "      <td>[141, 209, 1039, 122, 390, 1008, 13815, 13815,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[120, 2, 580, 3275, 6, 2899, 2406, 431, 104, 1...</td>\n",
       "      <td>[120, 2, 580, 3275, 6, 2899, 2406, 431, 104, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, sp, okane, wildfire, &lt;p...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[373, 10784, 1663, 2899, 84, 13815, 13815, 138...</td>\n",
       "      <td>[373, 10784, 1663, 2899, 84, 13815, 13815, 138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, &lt;pad&gt;...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[171, 539, 11, 268, 1021, 13815, 13815, 13815,...</td>\n",
       "      <td>[171, 539, 11, 268, 1021, 13815, 13815, 13815,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [happen, terrible, car, crash, <pad>, <pad>, <...   \n",
       "1  [hear, earthquake, different, city, stay, safe...   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3  [apocalypse, lighting, sp, okane, wildfire, <p...   \n",
       "4  [typhoon, soudelor, kill, china, taiwan, <pad>...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [201, 1920, 43, 16, 13815, 13815, 13815, 13815...   \n",
       "1  [141, 209, 1039, 122, 390, 1008, 13815, 13815,...   \n",
       "2  [120, 2, 580, 3275, 6, 2899, 2406, 431, 104, 1...   \n",
       "3  [373, 10784, 1663, 2899, 84, 13815, 13815, 138...   \n",
       "4  [171, 539, 11, 268, 1021, 13815, 13815, 13815,...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [201, 1920, 43, 16, 13815, 13815, 13815, 13815...  \n",
       "1  [141, 209, 1039, 122, 390, 1008, 13815, 13815,...  \n",
       "2  [120, 2, 580, 3275, 6, 2899, 2406, 431, 104, 1...  \n",
       "3  [373, 10784, 1663, 2899, 84, 13815, 13815, 138...  \n",
       "4  [171, 539, 11, 268, 1021, 13815, 13815, 13815,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:38.609225400Z",
     "start_time": "2023-12-21T15:34:37.441115300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'earthquake', 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:39.868591200Z",
     "start_time": "2023-12-21T15:34:39.426708900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'earthquake', 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:40.997210400Z",
     "start_time": "2023-12-21T15:34:40.562299400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'disaster', 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:42.087371100Z",
     "start_time": "2023-12-21T15:34:41.642630900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'disaster', 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Testing our Pre-Trained Embeddings to the PyTorch's Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:46.149953400Z",
     "start_time": "2023-12-21T15:34:46.078377300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1576, -0.1942,  0.0403,  ...,  0.0539, -0.2986, -0.4338],\n",
       "        [-0.4534, -0.5085,  0.0155,  ...,  0.1787,  0.2013, -0.9456],\n",
       "        [-0.2642,  0.1099, -0.0788,  ...,  0.2809, -0.0191,  0.0766],\n",
       "        ...,\n",
       "        [ 0.1621,  0.1817,  0.1331,  ...,  0.1250, -0.1614, -0.0876],\n",
       "        [-0.0478,  0.1445,  0.1973,  ...,  0.1166, -0.0971, -0.2256],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embedding layer with pre-trained weights\n",
    "pretrained_embeddings_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(skipgram_model.wv.vectors))\n",
    "# check weights of the pre-trained embedding layer\n",
    "pretrained_embeddings_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:46.893954100Z",
     "start_time": "2023-12-21T15:34:46.867690500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4318,   430,   193,  1331,  2326, 13845, 13845, 13845, 13845, 13845,\n",
       "         13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845,\n",
       "         13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845,\n",
       "         13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845,\n",
       "         13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845, 13845]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainTweetsDataset = TweetsDataset(tweets_train, 'skipgram')\n",
    "TrainTweetsDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:48.356003700Z",
     "start_time": "2023-12-21T15:34:48.329539400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the first token:          tensor([ 1.4912e-01, -8.4140e-04,  2.2404e-01,  5.5450e-02,  8.9583e-02,\n",
      "         2.1848e-01,  6.8861e-02, -2.4482e-01,  1.0519e-01, -2.1327e-01,\n",
      "        -6.6395e-02, -1.2278e-01,  2.5271e-01, -1.2421e-01, -8.0806e-02,\n",
      "         5.9938e-02,  2.1069e-01,  5.2437e-02, -2.9754e-01, -1.0679e-01,\n",
      "         2.1623e-01,  4.4079e-03,  9.8411e-02, -2.5258e-01, -2.4562e-01,\n",
      "        -1.0521e-01,  4.5064e-03, -1.2764e-01,  1.3645e-01,  5.8157e-02,\n",
      "         5.0418e-02,  1.8263e-01,  6.1557e-02, -1.7144e-01, -2.7553e-02,\n",
      "        -1.0233e-01, -8.1485e-02, -2.9838e-01, -3.3989e-01, -1.7210e-01,\n",
      "        -1.3923e-01,  1.9787e-02, -5.6020e-02,  2.5737e-01,  1.1951e-02,\n",
      "         9.7361e-02,  8.8058e-02, -1.9404e-01,  1.5763e-01,  3.5381e-02,\n",
      "         1.5040e-01,  3.1605e-01, -4.8728e-02,  2.1368e-01, -3.1739e-01,\n",
      "        -2.3778e-01,  1.2408e-01,  2.7924e-02,  5.3034e-02,  3.2247e-01,\n",
      "         2.9198e-01,  2.7474e-03, -1.4979e-01,  1.4639e-01, -9.2595e-02,\n",
      "        -1.4465e-01, -3.3764e-02,  1.7008e-01, -1.4191e-01, -9.9884e-02,\n",
      "         1.4062e-01, -1.4466e-01, -2.0797e-01,  3.6675e-02,  1.3404e-01,\n",
      "        -9.4450e-02,  1.4211e-01,  1.0500e-01, -1.1125e-01, -1.1426e-01,\n",
      "        -6.9038e-02, -7.2620e-02, -5.8071e-02, -7.3586e-03, -3.1275e-01,\n",
      "         9.7527e-02,  1.7346e-01,  3.8334e-02, -2.4665e-01,  8.6217e-02,\n",
      "         2.9979e-02,  1.2964e-01, -1.1674e-01, -4.9991e-02, -3.8981e-02,\n",
      "        -1.2873e-01, -3.5438e-01, -2.6041e-01, -6.9485e-02,  1.1124e-01,\n",
      "        -2.2054e-01, -2.2202e-01,  6.8421e-02, -6.0612e-02, -1.1032e-01,\n",
      "        -3.6303e-03, -3.6870e-03, -1.0158e-01,  1.2775e-01,  9.5124e-02,\n",
      "        -6.0825e-02, -3.4924e-01,  1.6533e-01,  1.1440e-01, -1.3223e-01,\n",
      "         1.8682e-02,  8.5287e-02,  9.6925e-02,  5.7687e-02,  4.0272e-02,\n",
      "        -1.0225e-01,  2.3052e-01,  1.2339e-01, -1.4193e-01, -6.9900e-02,\n",
      "        -2.0196e-01, -1.9176e-02,  1.1920e-01, -8.5291e-02,  5.0287e-02,\n",
      "        -9.8287e-02, -3.6158e-02,  2.1466e-01, -2.0025e-01,  4.1449e-01,\n",
      "        -4.6629e-03,  2.6388e-01, -1.1335e-01, -4.0981e-02, -8.2920e-02,\n",
      "         1.4954e-01, -2.4049e-01,  4.6309e-02,  1.5438e-02, -1.8563e-01,\n",
      "         3.3326e-02, -2.9181e-02, -3.2685e-01,  2.5986e-02,  5.8694e-02,\n",
      "         4.0950e-02, -1.0005e-01, -5.0601e-03, -1.2096e-01,  3.1271e-02,\n",
      "        -5.3065e-02,  2.7934e-01,  6.4006e-03, -1.0936e-01,  6.4893e-02,\n",
      "         1.2412e-01, -2.5473e-01, -1.8700e-01,  9.4310e-02,  3.5620e-01,\n",
      "         1.4792e-02, -4.4747e-02, -5.8993e-02,  1.6954e-01,  2.4181e-01,\n",
      "         5.2130e-02, -1.7274e-01, -2.7763e-01, -1.6011e-01,  1.1440e-01,\n",
      "         2.1368e-02,  4.8215e-02,  1.4916e-01,  2.3322e-01, -4.4729e-01,\n",
      "        -1.3820e-01,  1.2951e-01, -1.7153e-01,  1.0258e-02, -7.6714e-02,\n",
      "         7.3500e-02, -3.5996e-02, -3.2829e-01, -5.4563e-02, -9.0729e-02,\n",
      "        -2.4436e-01, -1.6025e-01,  3.2902e-01, -2.1774e-01, -1.2222e-02,\n",
      "         6.2768e-02,  3.4777e-01, -5.0723e-02, -1.2388e-01, -2.2223e-01,\n",
      "        -9.9820e-02,  1.0484e-03,  2.2067e-03,  4.4654e-02,  4.5004e-01,\n",
      "         7.3142e-02,  1.5749e-01,  1.1562e-01,  1.4420e-01,  2.6187e-02,\n",
      "        -2.5275e-01, -3.3218e-02,  1.2036e-01,  7.6528e-02, -1.6447e-02,\n",
      "         2.4136e-02, -1.4579e-01, -7.6036e-02, -1.7165e-01,  4.9239e-02,\n",
      "        -1.5023e-01, -9.9267e-02,  1.9950e-02,  1.2127e-01,  6.2118e-03,\n",
      "        -5.5839e-02, -3.1764e-01, -2.2058e-01,  1.0343e-01,  6.2073e-02,\n",
      "        -2.2199e-01, -4.7413e-01, -1.2091e-01, -2.5491e-01, -9.9075e-02,\n",
      "         2.8446e-01,  1.1066e-01,  2.1739e-02,  8.0727e-02, -1.4600e-01,\n",
      "         7.5155e-02, -2.2191e-01,  2.8124e-02,  2.2598e-02,  3.0916e-01,\n",
      "         1.2102e-01, -1.5414e-01, -2.0537e-01, -5.4992e-02, -1.9937e-01,\n",
      "        -2.4985e-01,  1.0884e-01,  2.3460e-01, -1.8516e-03, -2.5438e-01,\n",
      "         1.6226e-01, -5.4317e-02, -2.1675e-02,  1.5953e-01,  1.9998e-01,\n",
      "         1.7397e-01, -1.0848e-01,  1.7003e-01, -4.3931e-02, -3.2832e-01,\n",
      "         4.3658e-02, -1.7700e-01, -4.0814e-03, -1.8042e-01,  4.7059e-03,\n",
      "         1.9250e-01,  1.5522e-01,  7.8526e-03,  1.7974e-02, -3.4533e-03,\n",
      "         2.8335e-01, -1.1175e-01, -2.2321e-01, -1.7187e-01, -1.0992e-01,\n",
      "         1.9930e-01, -2.6048e-01, -3.2453e-01, -1.8760e-01,  4.2676e-02,\n",
      "        -1.1348e-03, -4.3564e-02, -2.3732e-01,  6.3323e-03,  1.9993e-01,\n",
      "         3.6724e-02, -3.5904e-02,  1.2786e-01, -4.2555e-01, -5.1300e-03,\n",
      "        -9.4759e-02,  1.4704e-02,  1.5230e-01, -1.0193e-01,  1.7321e-01,\n",
      "        -1.6430e-01,  4.3134e-02,  1.1742e-01, -3.1191e-01,  4.5776e-02,\n",
      "         4.4369e-01,  2.9981e-01,  1.2699e-01,  4.7320e-02, -3.0585e-01,\n",
      "         7.5594e-02, -1.6842e-02,  5.1471e-02, -2.5866e-01, -1.2536e-01,\n",
      "         3.0197e-02,  2.0685e-02, -8.7980e-02, -7.3883e-02, -1.8392e-01,\n",
      "         1.9870e-02,  2.1518e-01,  2.1321e-02,  2.1157e-01, -1.1121e-01,\n",
      "        -1.3170e-01, -2.1874e-02,  8.4313e-02,  1.1243e-01, -1.7732e-01,\n",
      "         1.8501e-01, -1.5913e-01, -8.2584e-02, -1.8603e-01, -2.0012e-01,\n",
      "         8.1281e-03, -2.5216e-01, -2.1523e-01,  3.5114e-01,  5.0168e-02,\n",
      "         3.4320e-03, -1.1785e-01,  1.5186e-01, -4.9399e-02, -1.7857e-01,\n",
      "        -1.4798e-01,  9.1580e-02, -4.0930e-02, -1.1624e-01,  5.7919e-02,\n",
      "         7.8417e-02,  6.6439e-02, -7.6645e-02, -4.6979e-02, -4.9126e-03,\n",
      "        -1.5060e-01,  8.4420e-02, -3.5560e-01,  3.3579e-01, -8.9669e-02,\n",
      "        -2.0882e-02,  4.6565e-02,  1.2560e-01,  9.4795e-02,  6.0161e-02,\n",
      "        -5.4849e-02,  1.6696e-02,  2.6900e-01, -2.8878e-01, -2.2246e-01,\n",
      "         9.7911e-02,  2.8673e-02, -1.4360e-01,  6.9125e-02,  1.6720e-01,\n",
      "        -5.5893e-02, -9.8411e-02,  1.5035e-01, -6.7746e-02,  2.5418e-01,\n",
      "         6.9382e-02, -1.1618e-02,  1.0380e-01,  2.9852e-02,  1.1988e-02,\n",
      "        -1.6790e-01,  5.1948e-02,  1.0198e-01, -3.7232e-03, -1.1378e-01,\n",
      "         4.6032e-02, -2.1949e-01,  1.0386e-01,  1.0152e-01,  2.3036e-01,\n",
      "         9.6953e-02,  1.5371e-01,  1.9291e-01, -9.3026e-02,  2.3956e-02,\n",
      "        -2.4541e-01, -1.0630e-01,  1.1933e-01, -5.4808e-02,  5.9111e-02,\n",
      "         3.7762e-01, -1.4378e-01,  3.8233e-02, -3.0402e-03, -3.1000e-02,\n",
      "        -5.6439e-02, -4.7834e-03, -1.3790e-01,  1.9610e-01, -1.5202e-01,\n",
      "        -1.3334e-01,  1.1430e-01,  6.0821e-02,  6.5921e-02, -9.9430e-02,\n",
      "        -1.2796e-01, -7.2628e-02,  1.6476e-01,  2.4201e-02,  5.9301e-02,\n",
      "        -7.7443e-02,  1.0557e-01, -2.5146e-01, -1.2193e-01, -8.8367e-02,\n",
      "         9.3336e-02, -1.0031e-01,  2.0968e-01,  1.2176e-01,  2.6865e-01,\n",
      "        -3.7414e-01,  1.8501e-02,  1.4888e-01,  2.2593e-02, -2.9854e-02,\n",
      "         9.1001e-04, -1.5436e-01, -2.5238e-01,  1.9477e-01, -2.8296e-01,\n",
      "        -8.6344e-02,  7.7596e-02, -9.9020e-02,  1.8678e-01, -9.7832e-02,\n",
      "        -1.9885e-01,  1.2286e-01,  1.7915e-01, -1.7185e-01, -1.2980e-01,\n",
      "         1.3792e-01, -7.7635e-02,  6.2604e-02,  1.4275e-01, -2.2491e-01,\n",
      "        -7.7351e-02, -1.1304e-02,  5.6020e-03,  1.2020e-01,  1.0723e-02,\n",
      "         1.6696e-01,  2.2128e-01,  1.8529e-01, -5.4284e-02,  2.1936e-01,\n",
      "         6.1865e-02, -9.7096e-02, -6.2544e-02, -1.2617e-01,  9.2877e-05,\n",
      "         1.4083e-01, -1.0133e-01,  1.3938e-01,  2.9513e-02,  2.8001e-02,\n",
      "         2.6745e-02, -9.6216e-02, -8.2862e-02, -1.2658e-01, -3.0388e-01,\n",
      "        -1.6333e-01, -5.2427e-03,  9.4298e-02, -2.2931e-02, -3.1084e-01,\n",
      "        -2.2758e-02, -1.2464e-01,  9.7462e-02, -7.7484e-02, -5.2744e-02,\n",
      "        -2.1141e-01,  1.4191e-01,  1.8372e-01, -5.1247e-02, -1.9882e-01,\n",
      "         2.6021e-01, -1.2430e-01,  6.2665e-02,  1.2985e-01,  7.5701e-02,\n",
      "        -8.7951e-02, -2.3957e-01,  1.1280e-01, -2.1369e-04, -1.7923e-03,\n",
      "        -1.4569e-01, -1.7062e-01])\n",
      "Embedding of the second token:         tensor([-0.1302, -0.1004,  0.4253, -0.0453,  0.2276, -0.0566,  0.3489, -0.2600,\n",
      "        -0.0454,  0.0418, -0.0576, -0.4389,  0.7659, -0.1422,  0.4233,  0.0870,\n",
      "         0.5871, -0.1190, -0.7132, -0.3447,  0.2681, -0.1247,  0.4442, -0.8123,\n",
      "        -0.4304,  0.2968, -0.1109,  0.0969,  0.2612,  0.3340,  0.5632,  0.2128,\n",
      "         0.5522, -0.3323, -0.2059, -0.2557,  0.0265, -0.3631, -0.1358, -0.3598,\n",
      "        -0.0157, -0.1630, -0.6509,  0.3361,  0.2455,  0.4083, -0.3693, -0.4520,\n",
      "         0.6622, -0.1669, -0.1962,  0.3579, -0.8690,  0.0157,  0.0916, -0.6626,\n",
      "        -0.1844,  0.2807,  0.4189,  0.5206,  0.5031,  0.2096, -0.7939,  0.6703,\n",
      "         0.2840, -0.1913, -0.7574,  0.3511, -0.3610, -0.2046,  1.1359,  0.0501,\n",
      "         0.1464,  0.1353, -0.4679,  0.2049,  0.5096, -0.2803, -0.4361,  0.5976,\n",
      "        -0.0941, -0.5952,  0.3260, -0.0933,  0.0469,  0.0655,  0.4429,  0.7023,\n",
      "         0.0493,  0.1783, -0.7370,  0.0480,  0.2160,  0.1424,  0.0293, -0.5652,\n",
      "        -0.5816,  0.1203, -0.1979,  0.0597, -0.4768, -0.5995, -0.0456,  0.3356,\n",
      "        -0.1938,  0.5912,  0.4191,  0.0630,  0.5161, -0.2341, -0.3968, -0.6499,\n",
      "         0.1627,  0.2053, -0.9243,  0.0856,  0.6588,  0.4387, -0.2601,  0.4810,\n",
      "        -0.1858,  0.6134, -0.1147,  0.2237, -0.2625, -0.1621,  0.1242, -0.4238,\n",
      "        -0.7876, -0.2679, -0.1701,  0.2233,  0.6639,  0.3979,  0.2459,  0.2184,\n",
      "         0.0768, -0.3642, -0.2219,  0.2952, -0.1428, -0.4867,  0.2584,  0.2043,\n",
      "        -0.1844, -0.1893,  0.1654, -0.6538, -0.1718,  0.1414,  0.0278, -0.7724,\n",
      "        -0.0384,  0.5767, -0.6329,  0.2178,  0.5264,  0.0161, -0.0975, -0.2847,\n",
      "         0.1663,  0.1620, -0.2559,  0.1238,  0.5715, -0.1790,  0.3669, -0.1657,\n",
      "         0.3102, -0.4920,  0.2615,  0.1406, -0.0271, -0.7220, -0.1963,  0.3316,\n",
      "        -0.3382,  0.1066, -0.1226, -0.6512,  0.3436, -0.1057, -0.7129,  0.2116,\n",
      "        -0.1295, -0.3051,  0.1556, -0.7247, -0.2602,  0.2737,  0.0906, -0.5174,\n",
      "         0.6549,  0.3298, -0.1381,  0.3625,  0.0306, -0.2759, -0.4835, -0.5426,\n",
      "        -0.1240,  0.3226,  0.4857, -0.0426,  0.2070,  0.2559,  0.4687, -0.5180,\n",
      "        -0.3340,  0.2528, -0.0631, -0.3096, -0.1032,  0.4475,  0.0489,  0.3649,\n",
      "         0.1438, -0.3952,  0.4426, -0.4118,  0.2327, -0.3556,  0.0167,  0.7296,\n",
      "         0.1861, -0.1578, -0.2197, -0.5255,  0.4856, -0.0763, -0.6786, -0.4996,\n",
      "        -0.4667, -0.4348, -0.2673,  0.0683,  0.2362, -0.1044,  0.0846,  0.5948,\n",
      "        -0.1595, -0.1839, -0.1924,  0.0748, -0.3313,  0.4553,  0.1303, -0.0856,\n",
      "         0.0749, -0.3466, -0.2688,  0.0448,  0.3586, -0.3159, -0.6442, -0.2029,\n",
      "        -0.1494, -0.4193,  0.2213,  0.1254,  0.2130,  0.2217, -0.0575,  0.1905,\n",
      "        -0.1760,  0.1061, -0.0894,  0.0687, -0.5148,  0.3709,  0.7515,  0.3842,\n",
      "        -0.3355,  0.3150,  0.2699,  0.2954, -0.1989, -0.1800,  0.0046, -0.1355,\n",
      "        -0.8547, -0.5838, -0.0722, -0.5279, -0.1038,  0.0480,  0.1220, -0.3923,\n",
      "        -0.2661,  0.4475, -0.1130,  0.0071,  0.1800, -0.9145,  0.2942,  0.3112,\n",
      "        -0.3459, -0.4317,  0.0573, -0.1091, -0.1826,  0.0416,  0.2205, -0.4742,\n",
      "         0.0073,  0.5449,  0.3046,  0.2691,  0.6586, -0.7665,  0.7335, -0.6745,\n",
      "        -0.0194, -0.4744, -0.1109,  0.1468, -0.2732,  0.1979, -0.6571, -0.5592,\n",
      "         0.0778,  0.2939,  0.4186,  0.4623, -0.8202,  0.0349,  0.0108,  0.2403,\n",
      "         0.2920,  0.0124,  0.1184, -0.6623,  0.2114, -0.0508, -0.2035,  0.6490,\n",
      "        -0.5314, -0.5410,  0.6540, -0.1027, -0.2364,  0.2622,  0.5233, -0.1185,\n",
      "         0.0117, -0.0029,  0.5243, -0.3079, -0.1288, -0.5405,  0.2507,  0.3587,\n",
      "        -0.5492, -0.7200, -0.1713, -0.3788,  0.0516, -0.2805,  0.5878,  0.0309,\n",
      "        -0.5993,  0.1394,  0.6970, -0.1293, -0.1184,  0.0330, -0.4108,  0.2103,\n",
      "        -0.1279, -0.5745,  0.1655, -0.1535,  0.0814,  0.4010, -0.0279, -0.7775,\n",
      "        -0.3924,  0.0844,  0.0824,  0.3772, -0.5009, -0.2213,  0.3735, -0.0417,\n",
      "        -0.1187, -0.4462,  0.8159, -0.4176,  0.4446, -0.3699, -0.4691, -0.0909,\n",
      "        -0.4934, -0.2267,  0.4321, -0.2133,  0.0393,  0.4397,  0.0670, -0.5215,\n",
      "        -0.2378,  0.2413,  0.4912,  0.6352,  0.1660,  0.4019,  0.1296,  0.3057,\n",
      "         0.0825, -0.1233, -0.3820,  0.3735, -0.1390,  0.0215, -0.4834,  0.1878,\n",
      "         0.3926,  0.2279,  0.4052,  0.2083, -0.2632,  0.4222, -0.2458, -0.0322,\n",
      "        -0.2141, -0.2054,  0.1105, -0.7341, -0.1994, -0.0967,  0.0792, -0.2766,\n",
      "         0.6475,  0.0668,  0.4581, -0.2225, -0.0813, -0.2943, -0.0980,  0.2295,\n",
      "         0.1288, -0.1888, -0.5861,  0.0449, -0.1884, -0.0763,  0.2506, -0.1953,\n",
      "         0.4408,  0.2053, -0.0906, -0.2338,  0.2390, -0.5541,  0.0340,  0.1908,\n",
      "         0.1450, -0.2243,  0.1871, -0.0094, -0.0542,  0.0318,  0.0360, -0.1294,\n",
      "        -0.1327,  0.7937,  0.1105,  0.0088, -0.1433, -0.1505,  0.1859, -0.1904,\n",
      "        -0.0547, -0.2555, -0.0176,  0.5656, -0.2094,  0.1212,  0.4505, -0.0571,\n",
      "         0.1718, -0.6719, -0.1126, -0.3942, -0.0381,  0.2055,  0.3193,  0.0364,\n",
      "        -0.3693, -0.6126,  0.0372, -0.7256,  0.1212, -0.0692, -0.4054,  0.1380,\n",
      "         0.1171,  0.4932, -0.0833, -0.0266,  0.7529, -0.0291,  0.3155,  0.0449,\n",
      "        -0.1799,  0.1346, -0.7267, -0.0798, -0.1353,  0.3584, -0.6293, -0.2613])\n",
      "Embedding of the third token:          tensor([-1.2204e-01, -6.5062e-02, -2.6390e-01, -5.7196e-01,  1.5992e-01,\n",
      "        -5.1331e-02,  7.2117e-02,  5.7536e-03, -2.2151e-01,  5.5749e-01,\n",
      "         5.6083e-02,  1.6843e-01,  6.1932e-01,  6.1598e-01,  6.3563e-02,\n",
      "         9.0116e-01,  2.2198e-01,  4.4471e-01, -2.0319e-01,  3.7219e-01,\n",
      "         2.4044e-01, -2.0062e-01,  5.8529e-02,  2.6419e-02, -5.7646e-01,\n",
      "         3.2168e-01,  1.6795e-01,  5.5859e-01,  2.6310e-01,  1.8646e-01,\n",
      "         3.6799e-02,  1.0511e-01,  1.2560e-01, -3.4125e-01,  2.2058e-01,\n",
      "         6.3303e-01, -4.0365e-01, -5.0675e-01, -6.5794e-01, -6.7614e-01,\n",
      "         2.3810e-02, -3.6727e-01,  2.3807e-01,  7.1964e-01,  2.0698e-01,\n",
      "        -3.1980e-01,  1.9288e-01,  1.7491e-03,  2.6220e-01,  2.7011e-01,\n",
      "         3.7630e-02,  4.3491e-01, -2.0202e-01,  1.6437e-01, -5.7753e-01,\n",
      "        -8.1311e-01,  5.4918e-01, -3.3604e-01, -6.9959e-02,  7.3834e-01,\n",
      "        -5.3046e-02,  4.0523e-01,  1.3644e-01, -7.8921e-02, -3.0490e-01,\n",
      "        -3.2138e-01, -6.0493e-02, -5.0895e-01, -3.8406e-02, -8.6035e-02,\n",
      "         6.1894e-02, -3.1693e-01, -5.8776e-01, -2.8206e-01,  5.5048e-01,\n",
      "         1.7993e-01,  5.7752e-01,  3.1950e-01,  3.3419e-01, -3.5278e-01,\n",
      "        -1.0449e-01, -5.7399e-01,  1.5831e-01,  5.9989e-02, -8.7744e-01,\n",
      "        -2.1238e-01,  1.6281e-01,  1.3008e-01, -2.4580e-01,  2.9930e-01,\n",
      "        -1.8923e-01, -2.7508e-01, -1.8951e-01, -5.4704e-01, -4.1122e-01,\n",
      "        -2.5714e-01, -5.6292e-01, -6.8048e-01, -8.5138e-02,  1.1559e-01,\n",
      "        -1.7879e-01, -2.4888e-02,  6.8016e-01,  1.2715e-01, -2.8803e-01,\n",
      "         1.6216e-01, -4.0114e-01,  2.3309e-01,  4.0930e-01, -3.1609e-01,\n",
      "         5.7691e-01, -6.1910e-01, -3.3346e-03,  3.3912e-01, -4.7310e-01,\n",
      "        -5.3905e-02,  2.8178e-01,  4.3231e-01,  3.0254e-01, -2.2506e-01,\n",
      "        -6.0326e-01,  3.0607e-01,  2.5960e-01, -1.1994e-01, -5.5592e-01,\n",
      "        -1.6598e-01, -1.2544e-01, -5.1240e-02, -1.2688e-01,  5.4894e-01,\n",
      "        -5.5983e-01, -5.2164e-01,  7.4552e-01, -1.3393e-01,  4.2914e-01,\n",
      "        -1.3964e-01,  7.5437e-02, -3.7038e-01, -2.4473e-01, -2.5850e-01,\n",
      "         9.5215e-02, -4.8726e-01, -2.3441e-01,  4.3439e-01, -5.5256e-01,\n",
      "        -5.5633e-02, -8.0243e-01, -6.8883e-01,  3.0750e-01, -1.5669e-01,\n",
      "        -1.2211e-01,  1.0562e-01,  2.5738e-01, -5.1264e-02,  2.1066e-01,\n",
      "         4.3622e-02,  5.8255e-01,  2.8506e-01, -1.4234e-01,  3.2808e-01,\n",
      "         8.5001e-02, -5.2005e-01, -2.3949e-01,  4.3216e-01,  7.8765e-01,\n",
      "         3.0010e-01, -6.6807e-01, -8.7000e-03, -7.6970e-02,  2.1519e-01,\n",
      "         2.5987e-01,  9.4493e-02, -4.9036e-01, -4.6284e-01,  1.6713e-01,\n",
      "         1.2122e-01, -5.5249e-02, -6.1834e-02, -2.1633e-01,  1.3625e-02,\n",
      "        -1.8399e-01, -5.6128e-01, -2.1800e-01,  1.9027e-01,  2.9077e-01,\n",
      "        -1.7012e-01,  1.0380e-01,  1.0060e-01, -1.1765e-01, -6.2887e-01,\n",
      "         4.9030e-02, -4.1078e-01,  2.7810e-01, -1.8356e-01,  3.4405e-01,\n",
      "         3.3470e-02,  2.2814e-01,  3.1072e-01, -6.1530e-01, -6.9906e-02,\n",
      "        -6.1378e-01,  2.4001e-01, -2.0979e-01, -1.5817e-01,  5.1540e-01,\n",
      "         3.9351e-01, -3.7676e-01, -1.4405e-01,  2.7552e-01, -4.6233e-01,\n",
      "         1.2887e-01,  6.9322e-02, -4.0821e-02,  5.1324e-02, -1.0805e-01,\n",
      "        -1.5946e-01, -3.2800e-01, -1.4416e-01, -1.9538e-01,  3.4286e-01,\n",
      "        -1.3333e-01,  2.0460e-01, -9.5154e-02, -5.9289e-01,  3.3409e-01,\n",
      "        -2.5272e-02, -2.7171e-01, -6.7318e-01,  3.3956e-01, -2.4372e-01,\n",
      "         8.0385e-02, -4.3129e-01, -6.7527e-01, -1.9543e-02, -7.3545e-01,\n",
      "         9.0290e-01,  1.8113e-01, -6.9744e-02, -1.0880e-01,  6.7031e-02,\n",
      "         3.9342e-01, -3.0834e-01,  2.9459e-02,  7.6626e-02,  4.3301e-01,\n",
      "         5.0707e-01, -3.7041e-01,  1.4461e-01, -5.2387e-01, -7.2168e-02,\n",
      "        -1.5545e-01, -3.3288e-01,  2.2553e-01, -5.0822e-01, -3.0548e-01,\n",
      "         4.3335e-01,  4.6452e-02, -5.1156e-01,  2.3756e-01,  3.6102e-01,\n",
      "        -1.1721e-01, -2.9300e-01,  1.3005e-01,  5.3747e-01, -3.5519e-01,\n",
      "        -2.9109e-01, -4.5043e-01,  4.3134e-01,  2.9136e-01, -1.5980e-01,\n",
      "         8.3942e-02,  1.9670e-01, -3.9990e-02,  4.8928e-01, -2.4630e-01,\n",
      "         9.0279e-01, -6.2253e-02, -2.1388e-01,  2.5226e-01,  1.9071e-01,\n",
      "        -3.4867e-02, -2.6195e-01, -3.0420e-01,  8.3733e-04, -1.0098e+00,\n",
      "         1.4275e-01,  2.5221e-01,  2.9618e-02,  3.4164e-01,  5.1350e-01,\n",
      "        -1.5996e-01,  3.0559e-01,  1.2153e-01, -4.5763e-01,  2.5857e-01,\n",
      "         2.2802e-03, -1.5663e-01,  1.7372e-01,  7.9179e-03,  2.2767e-01,\n",
      "        -2.3166e-01, -3.0502e-01,  6.4737e-03, -3.9213e-01,  9.7122e-02,\n",
      "         4.2643e-01,  4.2505e-02, -3.7370e-01, -1.5748e-01,  3.2872e-01,\n",
      "        -1.0441e-01, -3.6075e-01,  1.6754e-01, -1.7966e-01,  7.0500e-02,\n",
      "        -3.6391e-02, -7.3432e-02, -2.6088e-01,  7.0646e-02, -2.1988e-01,\n",
      "         6.1525e-02,  3.1941e-01,  5.5027e-01,  6.9135e-01,  1.7796e-01,\n",
      "        -3.1526e-01, -1.1181e-01,  1.5838e-01,  4.1064e-01, -1.2121e-01,\n",
      "         2.2499e-01, -2.5826e-01, -3.7979e-01, -8.1381e-01,  1.4897e-01,\n",
      "         3.9953e-01, -5.0073e-01, -7.8500e-01,  6.2117e-01, -2.8928e-01,\n",
      "        -8.8495e-02, -1.6145e-01, -1.1050e-01, -5.4604e-01,  3.9075e-02,\n",
      "        -8.9989e-01, -1.3253e-01,  5.0678e-02, -6.6636e-01, -3.1576e-01,\n",
      "         2.7099e-01,  4.4298e-01, -3.6004e-02,  2.0847e-02, -2.4394e-01,\n",
      "         1.6531e-01, -2.8385e-01, -1.1278e-01,  5.0047e-01,  8.2354e-02,\n",
      "         4.2542e-01,  1.3945e-01,  1.0909e-01,  3.3943e-01, -2.1813e-01,\n",
      "         7.7290e-02,  1.2667e-02,  1.0654e-01,  1.9287e-02,  1.5684e-01,\n",
      "         8.7500e-02, -5.9902e-02, -1.7515e-01,  1.0050e-01,  3.1338e-01,\n",
      "         9.1126e-02, -1.0287e-01, -2.5334e-01, -8.5278e-01,  4.1771e-01,\n",
      "        -1.6114e-01,  5.0432e-02,  2.5624e-01,  3.8143e-01,  7.5333e-01,\n",
      "        -1.8898e-01,  1.4093e-01,  5.7960e-01,  3.5334e-02, -1.6485e-01,\n",
      "        -3.1945e-01,  2.0932e-01, -3.6864e-01, -4.9812e-01,  4.9098e-01,\n",
      "         3.2886e-01,  1.8689e-02,  2.3316e-01, -9.9136e-03, -4.0874e-01,\n",
      "        -8.1523e-01,  2.9521e-01,  4.5554e-02,  3.6587e-02,  6.1807e-03,\n",
      "        -3.3086e-02, -2.9246e-01, -1.9665e-01,  1.2415e-02, -3.2961e-02,\n",
      "        -7.7908e-01,  5.8119e-01, -3.7650e-01,  2.6595e-01, -3.6777e-01,\n",
      "         2.7638e-01, -1.4110e-01, -7.8275e-02,  4.4843e-01, -1.0153e-01,\n",
      "        -2.8892e-01, -1.1152e-01,  4.2684e-01, -1.8557e-01,  4.2622e-01,\n",
      "        -7.7838e-02, -8.8344e-02, -5.8873e-01,  2.2798e-01, -3.9387e-01,\n",
      "        -5.7702e-01,  1.9229e-01,  1.9237e-01,  5.4482e-01,  4.2725e-02,\n",
      "        -2.6688e-01,  4.7867e-02,  7.0403e-02, -1.9111e-01,  7.2075e-02,\n",
      "         3.3266e-01, -3.4507e-02, -4.2612e-01,  3.9620e-01, -7.5241e-01,\n",
      "        -1.9978e-01,  1.9768e-01, -2.9310e-01,  1.0930e-01, -1.1012e-01,\n",
      "        -5.1500e-01, -3.4673e-01,  2.2815e-01,  1.2759e-02,  3.5447e-01,\n",
      "         1.2067e-01, -1.6573e-01, -2.3228e-01,  2.5420e-01, -8.0274e-01,\n",
      "         3.4575e-01, -6.1433e-01, -2.6269e-01, -1.0313e-01,  8.4430e-02,\n",
      "        -1.8277e-01,  8.3103e-01, -1.0324e-01, -2.1113e-01,  3.0341e-01,\n",
      "         5.6799e-01,  3.0173e-01, -2.2056e-01,  2.9343e-02, -4.9380e-02,\n",
      "         2.7070e-01, -2.0351e-01,  1.5901e-01, -5.4232e-02, -1.1259e-01,\n",
      "         3.3843e-01, -4.7668e-01, -8.2649e-02, -2.8308e-01, -6.0972e-02,\n",
      "        -2.0240e-01,  6.2148e-01,  2.5616e-01,  3.0333e-01, -4.0359e-01,\n",
      "         3.9050e-01, -5.0039e-01,  3.2087e-01,  2.4158e-01,  1.6213e-01,\n",
      "         1.9809e-01, -1.9053e-01,  6.5924e-01, -7.3282e-02, -3.1370e-01,\n",
      "         1.2903e+00, -3.9566e-01, -2.0090e-01,  7.4413e-01, -1.5448e-01,\n",
      "        -1.9947e-02, -4.3249e-02,  1.0440e-01, -1.8387e-01,  2.0131e-01,\n",
      "        -3.3062e-01, -1.6152e-01])\n",
      "Embedding of the fourth token:         tensor([ 0.0637,  0.3886,  0.6810,  0.3111,  0.2020,  0.3055,  0.1295,  0.0017,\n",
      "         0.2082, -0.0258, -0.1207, -0.1156,  0.3355, -0.2661, -0.0181,  0.2350,\n",
      "         0.0555, -0.1754, -0.0818,  0.1879,  0.0932,  0.2093, -0.0731, -0.3407,\n",
      "        -0.3268,  0.2184, -0.1250, -0.0933,  0.7788, -0.2839,  0.3434,  0.1554,\n",
      "        -0.1941, -0.2693, -0.2550, -0.1205,  0.1105, -0.0603, -0.5769, -0.1081,\n",
      "         0.1189, -0.1930,  0.2060, -0.1353,  0.2522, -0.0201, -0.0117, -0.4183,\n",
      "         0.2952, -0.0273,  0.1570,  0.2188, -0.5907,  0.2141, -0.1070,  0.1054,\n",
      "        -0.2519,  0.1805, -0.0498,  0.3737,  0.2553,  0.5484,  0.0854,  0.2040,\n",
      "        -0.0655,  0.1580, -0.0797,  0.1773, -0.3868,  0.2372,  0.0937, -0.1543,\n",
      "        -0.2179,  0.2256, -0.0295, -0.1882,  0.0310, -0.0875, -0.0168, -0.4308,\n",
      "        -0.2126, -0.3388, -0.2490, -0.0204, -0.6310,  0.2103, -0.3485, -0.3125,\n",
      "        -0.0354,  0.1550, -0.3592,  0.0415, -0.1993, -0.5900, -0.0731,  0.0433,\n",
      "        -0.4429, -0.3601, -0.0861, -0.2499,  0.1245, -0.1058,  0.1424, -0.4309,\n",
      "        -0.4159, -0.0385, -0.1270, -0.2718,  0.0613,  0.0425,  0.2222, -0.6033,\n",
      "         0.3759,  0.3518, -0.6019,  0.2465, -0.0076,  0.4459,  0.7410,  0.1146,\n",
      "        -0.0346,  0.4558,  0.1883, -0.2148, -0.1284, -0.4234, -0.0711, -0.0437,\n",
      "        -0.3381, -0.2008, -0.4083,  0.2220,  0.1414, -0.2501,  0.5860,  0.0891,\n",
      "         0.2409,  0.2356,  0.1284, -0.2873, -0.3320, -0.3927,  0.3393,  0.1812,\n",
      "        -0.2098, -0.0382,  0.0531, -0.7759,  0.3822, -0.2080, -0.2614, -0.2227,\n",
      "        -0.0536,  0.0262,  0.0123,  0.3239,  0.3334, -0.3499, -0.1748,  0.0706,\n",
      "         0.3090, -0.0992, -0.0405,  0.1732,  0.1944, -0.2314, -0.1757, -0.0834,\n",
      "        -0.1411,  0.1480,  0.1486, -0.0626, -0.1295,  0.0784,  0.0148,  0.0554,\n",
      "         0.1710,  0.2811,  0.4886, -0.5460, -0.2495,  0.0692, -0.2971, -0.3179,\n",
      "        -0.0927,  0.0379, -0.2726, -0.3660,  0.0401, -0.3574, -0.2944, -0.1519,\n",
      "         0.3250, -0.2776, -0.3680,  0.0609,  0.3320, -0.3531, -0.0358, -0.1444,\n",
      "        -0.1652, -0.2147, -0.1517, -0.2227,  0.4490,  0.3093,  0.4089, -0.6884,\n",
      "         0.1574,  0.1273, -0.4156, -0.1076,  0.2009,  0.4559, -0.0305,  0.1152,\n",
      "        -0.4720, -0.2154, -0.3741,  0.0788, -0.4136,  0.2074, -0.0180,  0.3201,\n",
      "         0.2309, -0.1915, -0.3703, -0.3129,  0.2659,  0.0734, -0.0348, -0.2129,\n",
      "         0.1683,  0.0412, -0.0370,  0.5546,  0.3604, -0.0049,  0.0405, -0.1396,\n",
      "         0.2411, -0.3355, -0.4018, -0.0768,  0.3175,  0.5159, -0.2077,  0.0440,\n",
      "        -0.2850, -0.5141, -0.3824, -0.3196,  0.3546, -0.1552, -0.1164, -0.1501,\n",
      "         0.0117, -0.0918,  0.6814, -0.0104, -0.1203, -0.2407, -0.1546, -0.1118,\n",
      "        -0.4193,  0.1392, -0.2616,  0.1314, -0.2513, -0.1722, -0.2308,  0.3259,\n",
      "        -0.0216,  0.1564, -0.1301,  0.5035, -0.1306, -0.2149, -0.0601, -0.5382,\n",
      "        -0.1071, -0.3656, -0.2851, -0.1517,  0.1995,  0.1210,  0.3465, -0.4557,\n",
      "        -0.2497,  0.1947, -0.3552,  0.4182,  0.1700, -0.7463,  0.1886, -0.5382,\n",
      "        -0.1175,  0.4791,  0.0125,  0.3071, -0.2307,  0.3073, -0.1284, -0.4397,\n",
      "        -0.1324,  0.4698,  0.6710,  0.3088,  0.3334, -0.5152,  0.0504,  0.3351,\n",
      "         0.2528, -0.3434, -0.0651,  0.0175,  0.1004, -0.5914,  0.4546, -0.3414,\n",
      "        -0.0336,  0.4644,  0.5337,  0.3895, -0.3995, -0.3140,  0.0522,  0.0037,\n",
      "         0.1045, -0.0996, -0.1413, -0.1689, -0.1858, -0.3577,  0.1918, -0.0941,\n",
      "        -0.3496, -0.1151,  0.2591, -0.0429, -0.0586,  0.0847,  0.0043, -0.1703,\n",
      "        -0.3415, -0.2340, -0.3660,  0.0658,  0.0842, -0.0272, -0.0743,  0.0534,\n",
      "        -0.5871, -0.4521,  0.1861, -0.4591,  0.2502, -0.7212,  0.2790,  0.0104,\n",
      "         0.0740,  0.1405, -0.2145, -0.0062, -0.0885,  0.2778,  0.2743,  0.2398,\n",
      "         0.2074, -0.2309,  0.0765,  0.1565, -0.1997,  0.1538,  0.0845, -0.2491,\n",
      "         0.1342,  0.1963, -0.0566,  0.5325, -0.1361,  0.2721,  0.4834,  0.1425,\n",
      "         0.2252, -0.2562,  0.6512,  0.1889,  0.0091, -0.1210,  0.2293, -0.5631,\n",
      "         0.0974, -0.1522, -0.0546, -0.2015,  0.1607,  0.1690, -0.4175,  0.1060,\n",
      "        -0.9720, -0.0032,  0.1915, -0.0253, -0.0543,  0.1309, -0.2588, -0.0121,\n",
      "         0.2316,  0.3923, -0.1283,  0.4580, -0.0264,  0.4193,  0.2096, -0.2615,\n",
      "         0.1073,  0.0311, -0.1158, -0.1905,  0.2324, -0.1880, -0.3532,  0.1578,\n",
      "         0.0663,  0.3684, -0.3643, -0.1857, -0.2887,  0.3746, -0.1518, -0.3577,\n",
      "         0.2060,  0.1892,  0.1857,  0.1339,  0.3751,  0.0062,  0.0895, -0.5248,\n",
      "        -0.1516,  0.0235, -0.1322,  0.0571, -0.2541,  0.1778,  0.1180,  0.0208,\n",
      "         0.0207, -0.2403, -0.3846, -0.1907,  0.3931, -0.6421, -0.0761, -0.0371,\n",
      "         0.1672,  0.0999,  0.1333, -0.5488, -0.0964,  0.0880, -0.1909,  0.1848,\n",
      "        -0.1870,  0.0077,  0.2005,  0.0759, -0.2992,  0.2432,  0.2038, -0.3650,\n",
      "        -0.3197, -0.1089, -0.0042, -0.3417,  0.2802, -0.0170,  0.2338, -0.1720,\n",
      "         0.5715, -0.0369, -0.3421, -0.3168,  0.0092, -0.4215, -0.2494,  0.3507,\n",
      "        -0.0519, -0.4227, -0.0654, -0.5679,  0.6521, -0.1819,  0.0619, -0.2378,\n",
      "        -0.0014,  0.6326,  0.2148, -0.3301,  0.5008, -0.0067,  0.1690,  0.3078,\n",
      "        -0.1246, -0.4032, -0.0718,  0.0372, -0.0174, -0.0162,  0.0237, -0.6778])\n",
      "Embedding of the fifth token:          tensor([-1.3217e-01, -3.8331e-02,  1.9150e-01,  2.7553e-01,  2.5363e-01,\n",
      "         4.5013e-01,  3.6290e-01, -1.6952e-01,  1.7004e-02, -2.5037e-01,\n",
      "        -1.9789e-01, -3.5479e-01,  5.7513e-02, -1.1730e-01, -1.0705e-01,\n",
      "         6.3373e-02, -2.0515e-01, -4.2421e-02, -2.7782e-01, -1.0838e-01,\n",
      "        -5.3132e-03,  1.1539e-01,  2.6162e-01,  5.7319e-02, -1.3820e-01,\n",
      "         2.3676e-01, -8.1284e-02, -1.6638e-01,  2.6203e-01,  1.8114e-01,\n",
      "         1.5915e-01,  4.0316e-02,  3.5122e-01, -3.0880e-01, -1.0095e-01,\n",
      "        -1.8958e-02, -3.1417e-01, -1.5085e-01, -3.2822e-01, -1.7976e-01,\n",
      "        -3.2329e-01, -2.7301e-01,  1.1996e-01,  1.9546e-01, -9.3692e-02,\n",
      "         1.8308e-01,  4.9101e-02, -2.8450e-01,  5.5615e-03,  6.0764e-03,\n",
      "        -6.4428e-02,  1.9896e-02, -4.1455e-01,  1.1829e-01, -4.8191e-01,\n",
      "        -4.3282e-01,  8.2687e-02, -9.1880e-02,  6.3543e-02,  2.9513e-01,\n",
      "         1.2787e-01,  3.1456e-01, -3.4748e-01,  3.8418e-01,  1.8146e-01,\n",
      "         1.3999e-01, -2.3327e-02, -1.4467e-02, -2.7580e-01, -3.5248e-02,\n",
      "         6.7053e-02,  1.0968e-02, -2.8030e-01, -1.9603e-01,  6.1428e-02,\n",
      "         4.8099e-02,  3.2350e-01, -3.5963e-03,  6.6289e-02, -2.2044e-02,\n",
      "        -9.6056e-02, -6.6917e-03,  6.7295e-02, -2.3397e-01, -4.1315e-01,\n",
      "         1.0370e-01, -1.3555e-03, -4.4220e-02, -1.5951e-02,  2.3159e-04,\n",
      "         4.8206e-03, -3.8806e-02, -2.5325e-01, -1.2977e-01, -1.3751e-01,\n",
      "         2.0590e-02, -6.3340e-01, -3.2528e-01,  5.9234e-02,  1.7863e-01,\n",
      "         1.8082e-01,  7.1326e-02,  9.9614e-02,  1.4019e-01,  3.1701e-01,\n",
      "         7.8507e-02, -9.2057e-02, -2.6396e-01,  2.7181e-01,  3.8063e-02,\n",
      "        -2.6787e-01, -4.9416e-01,  7.5799e-02,  4.3206e-01, -2.2078e-01,\n",
      "         3.0188e-01,  2.6020e-01,  2.9417e-01,  3.1087e-01,  6.6888e-02,\n",
      "         4.3323e-02,  3.8341e-01, -2.2329e-02, -5.0691e-02, -3.1700e-01,\n",
      "        -2.0762e-01, -3.3661e-01,  9.4370e-02, -1.0999e-01,  3.2079e-01,\n",
      "        -2.3610e-01, -1.7588e-01,  2.9403e-01, -3.5117e-01,  6.1793e-01,\n",
      "        -7.1713e-02,  3.7452e-01, -3.2725e-01,  4.6301e-02,  1.9134e-01,\n",
      "        -1.4879e-01,  8.1898e-02, -3.1645e-01,  3.6840e-01, -5.3324e-01,\n",
      "        -1.7367e-01,  1.1553e-01, -6.2396e-01,  3.5315e-01, -1.5171e-01,\n",
      "        -7.7496e-02, -2.2966e-01, -2.4626e-02,  2.7602e-01,  1.9710e-01,\n",
      "        -1.2866e-01,  4.6293e-01,  1.1802e-01, -1.6505e-01,  8.5840e-02,\n",
      "         3.3150e-03, -3.3864e-01, -1.2605e-01,  1.3629e-01,  5.4530e-01,\n",
      "        -5.0465e-02, -9.2777e-02, -1.0230e-01,  1.1215e-01,  8.1415e-03,\n",
      "         8.4136e-02, -2.3181e-01, -1.5565e-01, -5.8137e-02,  4.7583e-02,\n",
      "        -1.2574e-01,  2.7921e-01,  1.6016e-01,  2.2309e-01, -2.4779e-01,\n",
      "        -1.4744e-01,  4.2575e-03, -2.5605e-01, -1.1679e-01, -8.5193e-02,\n",
      "        -7.6666e-02, -2.0767e-01, -1.0625e-01,  7.0439e-02, -2.3651e-01,\n",
      "        -1.5300e-01, -1.4626e-01,  3.3594e-01, -9.6664e-02, -2.1236e-02,\n",
      "         1.0047e-01,  2.3175e-01, -2.0265e-01, -1.0590e-01, -1.0211e-01,\n",
      "        -4.5708e-02,  2.0388e-01, -2.8101e-01,  8.4115e-02,  7.2794e-01,\n",
      "         2.7479e-01, -1.4727e-02,  8.7335e-02,  5.0715e-02, -2.4518e-01,\n",
      "        -2.8781e-01,  3.0477e-02,  1.0317e-01, -1.6344e-01,  2.0404e-01,\n",
      "         1.4299e-01, -1.3815e-01,  7.6956e-02, -5.0162e-01, -2.5032e-01,\n",
      "        -4.2120e-01, -2.2980e-01,  1.4354e-01,  8.1396e-02, -3.5074e-02,\n",
      "        -1.9300e-01, -3.1684e-01, -2.9468e-01,  3.6869e-01, -1.9275e-01,\n",
      "         1.8501e-01, -3.9476e-01, -7.5239e-02, -1.7412e-01, -2.1761e-01,\n",
      "         9.5333e-02,  1.8914e-01,  1.9825e-01, -8.5384e-02,  3.2743e-01,\n",
      "         4.3240e-01, -4.9467e-01, -1.9129e-01, -9.3610e-02,  2.8015e-01,\n",
      "         3.8092e-01, -1.8287e-02, -1.1219e-02, -3.0908e-01, -8.3223e-02,\n",
      "        -7.1349e-02, -2.9955e-02,  7.6734e-02, -4.4943e-01, -3.5794e-01,\n",
      "         1.6103e-01,  1.9428e-01, -9.8793e-03,  4.2468e-01,  1.1035e-01,\n",
      "        -2.5443e-02,  1.3313e-02, -1.2803e-02, -1.6271e-01, -3.6511e-01,\n",
      "        -3.6258e-02, -2.1635e-01, -7.2591e-02, -3.1670e-01, -3.8425e-01,\n",
      "         2.4062e-01,  2.3166e-02,  3.0389e-02, -8.2874e-03, -2.2369e-01,\n",
      "         2.1554e-01, -1.8209e-01,  5.8991e-02,  4.9622e-02, -2.1819e-01,\n",
      "        -1.6368e-01, -2.3468e-01, -6.4051e-02, -2.9595e-01,  2.0103e-01,\n",
      "        -4.3604e-02,  2.7556e-01, -6.2394e-01,  1.8265e-02,  2.6806e-02,\n",
      "        -1.5929e-01,  5.5372e-02,  1.7636e-01, -7.5742e-01,  1.4246e-01,\n",
      "        -1.6240e-01, -2.0915e-01,  2.8566e-01,  9.1468e-04, -4.1689e-02,\n",
      "        -2.4866e-01,  3.2035e-01, -6.9200e-02, -2.9886e-01, -6.6667e-02,\n",
      "         3.3859e-01,  2.0783e-01,  2.3650e-01, -3.8248e-03, -1.3219e-01,\n",
      "         1.6688e-02, -3.5714e-01, -1.5753e-01, -4.6339e-01, -3.0746e-01,\n",
      "        -4.4238e-02,  5.6619e-02, -2.6851e-01, -9.3381e-02,  1.1793e-02,\n",
      "        -1.3354e-02,  2.9319e-01, -4.4492e-02,  2.9961e-01, -1.5128e-01,\n",
      "         3.0550e-02,  4.8964e-02,  5.2998e-02,  4.4729e-01, -1.4730e-01,\n",
      "         2.8471e-01, -1.9124e-01,  3.4266e-02, -1.8405e-01, -2.4061e-02,\n",
      "        -3.1047e-02, -3.0628e-01, -8.3974e-02,  2.5423e-01,  1.9282e-01,\n",
      "        -4.2800e-02,  8.8853e-03,  1.4841e-01,  8.1178e-03,  5.4606e-02,\n",
      "        -4.4349e-01, -3.0162e-02,  4.6679e-02, -2.8721e-02, -1.1497e-01,\n",
      "         4.8490e-02, -4.1281e-02, -3.3999e-01, -4.5232e-01, -3.3023e-02,\n",
      "        -3.3290e-01, -9.1205e-02, -4.2562e-01,  1.2532e-01, -1.0650e-01,\n",
      "         7.3496e-02,  1.0196e-01, -8.4448e-02,  1.4246e-02, -1.6886e-01,\n",
      "        -7.4314e-02, -4.3411e-02,  6.9964e-02, -5.7339e-02, -2.2700e-01,\n",
      "         2.3027e-01,  1.1767e-01, -1.6232e-01,  2.7270e-01,  1.3817e-01,\n",
      "         9.5856e-02,  8.2953e-02,  2.5498e-01, -9.4751e-02,  9.9884e-02,\n",
      "        -1.2031e-01, -6.3759e-02,  5.6658e-01, -1.2205e-02, -1.0259e-01,\n",
      "         5.6108e-02,  6.1712e-01,  3.6118e-01, -4.7317e-02, -2.1198e-01,\n",
      "        -9.6048e-02,  6.2820e-02, -7.2529e-02,  1.1924e-01,  2.8466e-01,\n",
      "        -6.0365e-02, -1.7486e-01,  9.3264e-02, -3.2120e-01,  1.4457e-01,\n",
      "        -2.2518e-01, -1.4116e-01,  4.4731e-01, -9.6071e-02, -7.1021e-02,\n",
      "         2.1554e-01,  1.0847e-01,  1.1707e-01,  1.3191e-01,  2.7328e-01,\n",
      "         7.5819e-02,  3.3459e-01,  4.1437e-02,  2.4265e-01,  2.4376e-02,\n",
      "         5.2507e-03,  7.9024e-02,  2.0705e-01,  2.3399e-01, -3.0844e-01,\n",
      "        -7.6023e-02, -5.4772e-02,  5.9231e-02,  2.6296e-01, -1.4467e-01,\n",
      "         9.4575e-02,  5.2501e-02, -3.0224e-01, -5.6170e-01, -4.2559e-02,\n",
      "        -1.0107e-01, -5.8436e-03,  3.3809e-01, -2.0906e-02, -3.7792e-02,\n",
      "        -2.0028e-01,  2.9423e-01, -1.4708e-01,  1.4800e-01, -3.2503e-01,\n",
      "         1.5685e-01,  6.0858e-02, -5.0707e-01,  1.6118e-01, -1.9749e-01,\n",
      "        -2.7310e-01,  5.5266e-02,  1.8293e-01,  1.6824e-01, -2.7947e-01,\n",
      "        -1.8066e-01,  5.8291e-02,  2.7160e-01, -5.7896e-01, -1.6343e-01,\n",
      "         1.1194e-01, -2.8512e-01, -6.5213e-02,  2.3469e-01, -3.2741e-01,\n",
      "        -3.9069e-03,  2.7601e-01,  4.6419e-02, -6.7972e-02,  8.0981e-02,\n",
      "         2.8426e-01,  4.0253e-01,  2.7276e-01,  1.3397e-01,  2.8927e-01,\n",
      "         2.7668e-01,  8.7999e-02, -1.9724e-01, -1.8956e-01,  1.0066e-01,\n",
      "        -6.4071e-03,  8.8225e-02,  2.7667e-01,  1.6001e-02, -3.2878e-01,\n",
      "         6.1580e-03, -2.0287e-02, -1.7341e-02, -2.1896e-01, -1.8559e-01,\n",
      "        -4.7220e-01,  2.1571e-02, -5.8641e-02,  2.3919e-01, -3.6413e-01,\n",
      "         2.6313e-01, -1.9494e-01,  3.5231e-01, -4.7352e-02, -2.7152e-02,\n",
      "        -7.1446e-02,  5.1474e-01,  1.7651e-01, -8.5469e-02,  5.3254e-02,\n",
      "         1.3941e-01, -1.7277e-01, -9.5280e-02,  3.0412e-01,  1.2122e-01,\n",
      "        -1.8753e-01, -8.3772e-02, -3.0678e-02, -2.6082e-01, -3.0324e-01,\n",
      "        -4.6616e-01, -1.2972e-01])\n",
      "Embedding of the sixth token:          tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Embedding of the twenty-fifth token:   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Embedding of the fortieth token:       tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Embedding of the fiftieth token:       tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "sequence = TrainTweetsDataset[0][0]\n",
    "sequence_embeddings = pretrained_embeddings_layer(sequence)\n",
    "\n",
    "print('Embedding of the first token:          {}'.format(sequence_embeddings[0]))\n",
    "print('Embedding of the second token:         {}'.format(sequence_embeddings[1]))\n",
    "print('Embedding of the third token:          {}'.format(sequence_embeddings[2]))\n",
    "print('Embedding of the fourth token:         {}'.format(sequence_embeddings[3]))\n",
    "print('Embedding of the fifth token:          {}'.format(sequence_embeddings[4]))\n",
    "print('Embedding of the sixth token:          {}'.format(sequence_embeddings[5]))\n",
    "print('Embedding of the twenty-fifth token:   {}'.format(sequence_embeddings[24]))\n",
    "print('Embedding of the fortieth token:       {}'.format(sequence_embeddings[39]))\n",
    "print('Embedding of the fiftieth token:       {}'.format(sequence_embeddings[48]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LSTM Neural Network with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:50.418593600Z",
     "start_time": "2023-12-21T15:34:50.412589100Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 128, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = skipgram_model,\n",
    "                                  hidden_size = 64, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 1, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6853 | Accuracy = 63.93% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 39.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6916\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6931 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 76.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6894\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6922 | Accuracy = 52.46% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 48.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6875\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6947 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 75.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6857\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7058 | Accuracy = 47.54% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 47.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6845\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6983 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 74.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6838\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6919 | Accuracy = 54.10% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 48.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6834\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7017 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 78.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6643 | Accuracy = 63.93% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 47.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6831\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7004 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 80.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6835\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6685 | Accuracy = 62.30% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 47.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6831\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7005 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 80.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6618 | Accuracy = 63.93% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 48.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6831\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7018 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 78.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6835\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6757 | Accuracy = 59.02% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:01<00:00, 48.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6829\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6992 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 76.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6815\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = skipgram_model, \n",
    "                                                      hidden_size = 64, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_Attention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6673 | Accuracy = 65.57% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6870\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6777 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 79.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6764\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5784 | Accuracy = 62.30% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6514\n",
      "Training Accuracy = 57.13%\n",
      "Training F1-Score = 0.43%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5697 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 85.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6082\n",
      "Test Accuracy = 59.03%\n",
      "Test F1-Score = 8.86%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5131 | Accuracy = 78.69% | F1-Score = 60.61% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5243\n",
      "Training Accuracy = 71.63%\n",
      "Training F1-Score = 53.75%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3850 | Accuracy = 84.13% | F1-Score = 80.77% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 82.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5217\n",
      "Test Accuracy = 72.66%\n",
      "Test F1-Score = 55.97%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4704 | Accuracy = 78.69% | F1-Score = 76.36% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4501\n",
      "Training Accuracy = 79.55%\n",
      "Training F1-Score = 71.96%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3446 | Accuracy = 88.89% | F1-Score = 87.27% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 82.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4926\n",
      "Test Accuracy = 76.62%\n",
      "Test F1-Score = 65.77%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3825 | Accuracy = 81.97% | F1-Score = 71.79% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4204\n",
      "Training Accuracy = 81.22%\n",
      "Training F1-Score = 74.86%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3501 | Accuracy = 88.89% | F1-Score = 87.27% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 88.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5035\n",
      "Test Accuracy = 75.76%\n",
      "Test F1-Score = 63.02%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4371 | Accuracy = 81.97% | F1-Score = 73.17% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3984\n",
      "Training Accuracy = 82.37%\n",
      "Training F1-Score = 76.64%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3450 | Accuracy = 88.89% | F1-Score = 87.27% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 84.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4931\n",
      "Test Accuracy = 76.77%\n",
      "Test F1-Score = 65.58%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4116 | Accuracy = 81.97% | F1-Score = 78.43% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3783\n",
      "Training Accuracy = 83.34%\n",
      "Training F1-Score = 78.09%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3467 | Accuracy = 90.48% | F1-Score = 89.29% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 88.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4946\n",
      "Test Accuracy = 77.35%\n",
      "Test F1-Score = 66.70%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3236 | Accuracy = 85.25% | F1-Score = 76.92% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3569\n",
      "Training Accuracy = 84.22%\n",
      "Training F1-Score = 79.34%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3654 | Accuracy = 90.48% | F1-Score = 89.29% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 88.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5237\n",
      "Test Accuracy = 76.16%\n",
      "Test F1-Score = 63.75%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3137 | Accuracy = 88.52% | F1-Score = 84.44% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3363\n",
      "Training Accuracy = 85.37%\n",
      "Training F1-Score = 81.05%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3753 | Accuracy = 90.48% | F1-Score = 89.29% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 87.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5366\n",
      "Test Accuracy = 76.10%\n",
      "Test F1-Score = 63.69%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2085 | Accuracy = 91.80% | F1-Score = 90.20% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3126\n",
      "Training Accuracy = 86.25%\n",
      "Training F1-Score = 82.30%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3880 | Accuracy = 88.89% | F1-Score = 87.72% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 86.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5473\n",
      "Test Accuracy = 76.40%\n",
      "Test F1-Score = 64.52%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3319 | Accuracy = 86.89% | F1-Score = 80.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2898\n",
      "Training Accuracy = 87.39%\n",
      "Training F1-Score = 83.84%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3936 | Accuracy = 88.89% | F1-Score = 87.72% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 84.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5597\n",
      "Test Accuracy = 76.65%\n",
      "Test F1-Score = 65.40%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2262 | Accuracy = 95.08% | F1-Score = 92.68% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2613\n",
      "Training Accuracy = 88.53%\n",
      "Training F1-Score = 85.50%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4229 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 81.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5961\n",
      "Test Accuracy = 76.13%\n",
      "Test F1-Score = 64.48%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2497 | Accuracy = 91.80% | F1-Score = 87.80% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2352\n",
      "Training Accuracy = 90.08%\n",
      "Training F1-Score = 87.56%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4288 | Accuracy = 85.71% | F1-Score = 84.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 83.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6194\n",
      "Test Accuracy = 76.37%\n",
      "Test F1-Score = 65.38%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1969 | Accuracy = 93.44% | F1-Score = 87.50% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2059\n",
      "Training Accuracy = 91.63%\n",
      "Training F1-Score = 89.63%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4120 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 80.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6330\n",
      "Test Accuracy = 77.29%\n",
      "Test F1-Score = 67.99%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1946 | Accuracy = 90.16% | F1-Score = 86.96% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1834\n",
      "Training Accuracy = 92.60%\n",
      "Training F1-Score = 90.94%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4362 | Accuracy = 85.71% | F1-Score = 84.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 86.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6831\n",
      "Test Accuracy = 76.89%\n",
      "Test F1-Score = 66.81%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1074 | Accuracy = 95.08% | F1-Score = 92.68% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1600\n",
      "Training Accuracy = 93.77%\n",
      "Training F1-Score = 92.44%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4255 | Accuracy = 85.71% | F1-Score = 84.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 81.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7076\n",
      "Test Accuracy = 76.65%\n",
      "Test F1-Score = 67.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM_Attention, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "\n",
    "customPreTrainedLSTM_MultiheadAttention = CustomLSTM_MultiHeadAttention(word2vec_model = skipgram_model,\n",
    "                                                                        hidden_size = 1024, \n",
    "                                                                        output_size = 1, \n",
    "                                                                        dropout = 0.1,\n",
    "                                                                        num_layers = 1, \n",
    "                                                                        bidirectional = True,\n",
    "                                                                        freeze_embeddings = True,\n",
    "                                                                        num_heads = 16).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_MultiheadAttention.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5430 | Accuracy = 73.77% | F1-Score = 69.23% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6806\n",
      "Training Accuracy = 73.81%\n",
      "Training F1-Score = 67.88%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4502 | Accuracy = 82.54% | F1-Score = 83.08% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6062\n",
      "Test Accuracy = 73.00%\n",
      "Test F1-Score = 70.17%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5971 | Accuracy = 78.69% | F1-Score = 69.77% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4918\n",
      "Training Accuracy = 79.08%\n",
      "Training F1-Score = 73.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4347 | Accuracy = 80.95% | F1-Score = 79.31% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5471\n",
      "Test Accuracy = 75.79%\n",
      "Test F1-Score = 69.59%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7244 | Accuracy = 65.57% | F1-Score = 51.16% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4611\n",
      "Training Accuracy = 80.07%\n",
      "Training F1-Score = 74.65%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6896 | Accuracy = 76.19% | F1-Score = 68.09% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.9887\n",
      "Test Accuracy = 67.97%\n",
      "Test F1-Score = 41.59%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4442 | Accuracy = 83.61% | F1-Score = 77.27% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4223\n",
      "Training Accuracy = 81.69%\n",
      "Training F1-Score = 76.59%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5707 | Accuracy = 76.19% | F1-Score = 78.26% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6778\n",
      "Test Accuracy = 71.01%\n",
      "Test F1-Score = 70.01%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3106 | Accuracy = 90.16% | F1-Score = 86.36% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3773\n",
      "Training Accuracy = 83.62%\n",
      "Training F1-Score = 78.98%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3773 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5410\n",
      "Test Accuracy = 76.62%\n",
      "Test F1-Score = 69.05%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5707 | Accuracy = 72.13% | F1-Score = 70.18% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3567\n",
      "Training Accuracy = 84.36%\n",
      "Training F1-Score = 79.76%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3735 | Accuracy = 87.30% | F1-Score = 86.67% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5663\n",
      "Test Accuracy = 75.54%\n",
      "Test F1-Score = 71.74%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4005 | Accuracy = 85.25% | F1-Score = 83.64% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3284\n",
      "Training Accuracy = 86.33%\n",
      "Training F1-Score = 82.61%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3234 | Accuracy = 92.06% | F1-Score = 91.53% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5283\n",
      "Test Accuracy = 76.43%\n",
      "Test F1-Score = 71.12%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2410 | Accuracy = 93.44% | F1-Score = 90.48% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2975\n",
      "Training Accuracy = 87.56%\n",
      "Training F1-Score = 84.24%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3618 | Accuracy = 87.30% | F1-Score = 85.19% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5363\n",
      "Test Accuracy = 76.59%\n",
      "Test F1-Score = 66.43%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4415 | Accuracy = 81.97% | F1-Score = 78.43% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2801\n",
      "Training Accuracy = 87.88%\n",
      "Training F1-Score = 84.71%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3843 | Accuracy = 88.89% | F1-Score = 88.52% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5959\n",
      "Test Accuracy = 73.34%\n",
      "Test F1-Score = 69.43%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3002 | Accuracy = 81.97% | F1-Score = 78.43% | Batch ID = 60 : 100%|██████████| 60/60 [00:20<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2630\n",
      "Training Accuracy = 89.37%\n",
      "Training F1-Score = 86.71%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3629 | Accuracy = 84.13% | F1-Score = 80.77% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5388\n",
      "Test Accuracy = 75.91%\n",
      "Test F1-Score = 65.19%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2693 | Accuracy = 86.89% | F1-Score = 85.71% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2424\n",
      "Training Accuracy = 90.32%\n",
      "Training F1-Score = 87.98%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4242 | Accuracy = 87.30% | F1-Score = 87.50% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5739\n",
      "Test Accuracy = 75.27%\n",
      "Test F1-Score = 71.04%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2492 | Accuracy = 90.16% | F1-Score = 88.89% | Batch ID = 60 : 100%|██████████| 60/60 [00:20<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2325\n",
      "Training Accuracy = 90.71%\n",
      "Training F1-Score = 88.52%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3658 | Accuracy = 85.71% | F1-Score = 83.02% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5546\n",
      "Test Accuracy = 76.89%\n",
      "Test F1-Score = 66.81%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2020 | Accuracy = 93.44% | F1-Score = 92.31% | Batch ID = 60 : 100%|██████████| 60/60 [00:20<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2122\n",
      "Training Accuracy = 91.57%\n",
      "Training F1-Score = 89.59%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4331 | Accuracy = 87.30% | F1-Score = 87.50% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6075\n",
      "Test Accuracy = 74.16%\n",
      "Test F1-Score = 71.18%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3414 | Accuracy = 86.89% | F1-Score = 87.10% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1955\n",
      "Training Accuracy = 92.74%\n",
      "Training F1-Score = 91.07%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4181 | Accuracy = 84.13% | F1-Score = 81.48% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6484\n",
      "Test Accuracy = 74.47%\n",
      "Test F1-Score = 59.97%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2540 | Accuracy = 88.52% | F1-Score = 86.79% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1948\n",
      "Training Accuracy = 92.68%\n",
      "Training F1-Score = 91.09%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3523 | Accuracy = 87.30% | F1-Score = 85.71% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5383\n",
      "Test Accuracy = 77.44%\n",
      "Test F1-Score = 67.69%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2501 | Accuracy = 91.80% | F1-Score = 88.89% | Batch ID = 60 : 100%|██████████| 60/60 [00:21<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1737\n",
      "Training Accuracy = 93.87%\n",
      "Training F1-Score = 92.58%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3687 | Accuracy = 87.30% | F1-Score = 87.10% | Batch ID = 26 : 100%|██████████| 26/26 [00:04<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5755\n",
      "Test Accuracy = 76.06%\n",
      "Test F1-Score = 71.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_MultiheadAttention, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 32, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = cbow_model,\n",
    "                                  hidden_size = 256, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 3, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6825 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6866\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6992 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7362 | Accuracy = 37.70% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6843\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7003 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 42.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6908 | Accuracy = 54.10% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6990 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 40.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6981 | Accuracy = 50.82% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6979 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 41.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6839\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6955 | Accuracy = 52.46% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6837\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6997 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 40.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6823 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7009 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 42.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6866 | Accuracy = 55.74% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6834\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6975 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6840\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6826 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:03<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6985 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:00<00:00, 41.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6837\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = cbow_model, \n",
    "                                                      hidden_size = 512, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(customPreTrainedLSTM_Attention.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6526 | Accuracy = 51.72% | F1-Score = 0.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6709\n",
      "Training Accuracy = 57.15%\n",
      "Training F1-Score = 0.55%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6085 | Accuracy = 32.26% | F1-Score = 8.70% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 73.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6354\n",
      "Test Accuracy = 58.54%\n",
      "Test F1-Score = 6.75%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3897 | Accuracy = 89.66% | F1-Score = 82.35% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5281\n",
      "Training Accuracy = 71.21%\n",
      "Training F1-Score = 53.48%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3595 | Accuracy = 80.65% | F1-Score = 84.21% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 73.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4843\n",
      "Test Accuracy = 75.85%\n",
      "Test F1-Score = 64.76%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3956 | Accuracy = 75.86% | F1-Score = 69.57% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4647\n",
      "Training Accuracy = 77.55%\n",
      "Training F1-Score = 69.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3565 | Accuracy = 80.65% | F1-Score = 84.21% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 72.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4744\n",
      "Test Accuracy = 76.89%\n",
      "Test F1-Score = 66.78%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4816 | Accuracy = 75.86% | F1-Score = 58.82% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4517\n",
      "Training Accuracy = 78.30%\n",
      "Training F1-Score = 70.43%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3636 | Accuracy = 80.65% | F1-Score = 84.21% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4717\n",
      "Test Accuracy = 76.92%\n",
      "Test F1-Score = 66.58%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4523 | Accuracy = 72.41% | F1-Score = 63.64% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4424\n",
      "Training Accuracy = 78.75%\n",
      "Training F1-Score = 71.03%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3546 | Accuracy = 80.65% | F1-Score = 85.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 71.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4669\n",
      "Test Accuracy = 77.23%\n",
      "Test F1-Score = 67.71%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4181 | Accuracy = 86.21% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4344\n",
      "Training Accuracy = 79.54%\n",
      "Training F1-Score = 72.68%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3607 | Accuracy = 80.65% | F1-Score = 85.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 71.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4671\n",
      "Test Accuracy = 77.05%\n",
      "Test F1-Score = 66.99%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4042 | Accuracy = 79.31% | F1-Score = 66.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4266\n",
      "Training Accuracy = 79.92%\n",
      "Training F1-Score = 73.09%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3511 | Accuracy = 80.65% | F1-Score = 85.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 72.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4629\n",
      "Test Accuracy = 77.97%\n",
      "Test F1-Score = 69.34%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3657 | Accuracy = 79.31% | F1-Score = 66.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4192\n",
      "Training Accuracy = 80.48%\n",
      "Training F1-Score = 74.12%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3562 | Accuracy = 80.65% | F1-Score = 85.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 73.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4639\n",
      "Test Accuracy = 78.12%\n",
      "Test F1-Score = 69.62%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2081 | Accuracy = 96.55% | F1-Score = 93.33% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 23.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4119\n",
      "Training Accuracy = 80.99%\n",
      "Training F1-Score = 74.86%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3466 | Accuracy = 80.65% | F1-Score = 85.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 74.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4611\n",
      "Test Accuracy = 78.58%\n",
      "Test F1-Score = 70.96%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4598 | Accuracy = 79.31% | F1-Score = 76.92% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4038\n",
      "Training Accuracy = 81.19%\n",
      "Training F1-Score = 75.34%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3460 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 72.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4608\n",
      "Test Accuracy = 78.33%\n",
      "Test F1-Score = 70.60%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3098 | Accuracy = 89.66% | F1-Score = 72.73% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3962\n",
      "Training Accuracy = 81.89%\n",
      "Training F1-Score = 76.60%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3582 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4651\n",
      "Test Accuracy = 78.55%\n",
      "Test F1-Score = 70.46%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3294 | Accuracy = 82.76% | F1-Score = 78.26% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3881\n",
      "Training Accuracy = 82.56%\n",
      "Training F1-Score = 77.48%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3580 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 69.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4657\n",
      "Test Accuracy = 78.39%\n",
      "Test F1-Score = 70.24%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6655 | Accuracy = 75.86% | F1-Score = 63.16% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3800\n",
      "Training Accuracy = 83.12%\n",
      "Training F1-Score = 78.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3481 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 68.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4674\n",
      "Test Accuracy = 78.88%\n",
      "Test F1-Score = 71.68%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2849 | Accuracy = 86.21% | F1-Score = 83.33% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3715\n",
      "Training Accuracy = 83.50%\n",
      "Training F1-Score = 78.99%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3642 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 71.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4726\n",
      "Test Accuracy = 78.33%\n",
      "Test F1-Score = 70.36%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2378 | Accuracy = 86.21% | F1-Score = 86.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 22.88it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3627\n",
      "Training Accuracy = 84.43%\n",
      "Training F1-Score = 80.36%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3625 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 74.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4699\n",
      "Test Accuracy = 78.76%\n",
      "Test F1-Score = 71.79%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3020 | Accuracy = 89.66% | F1-Score = 85.71% | Batch ID = 238 : 100%|██████████| 238/238 [00:10<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3531\n",
      "Training Accuracy = 84.83%\n",
      "Training F1-Score = 80.94%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3612 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:01<00:00, 73.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4789\n",
      "Test Accuracy = 78.64%\n",
      "Test F1-Score = 72.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM_Attention, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GRU Neural Network with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 128, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = skipgram_model, \n",
    "                                hidden_size = 256, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 3, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6691 | Accuracy = 60.66% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:39<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6842\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7072 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:05<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6841\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7125 | Accuracy = 47.54% | F1-Score = 0.00% | Batch ID = 60 : 100%|██████████| 60/60 [00:38<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6837\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7051 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 26 : 100%|██████████| 26/26 [00:05<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6839\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6834 | Accuracy = 57.03% | F1-Score = 0.00% | Batch ID = 2 :   3%|▎         | 2/60 [00:01<00:56,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomPreTrainedGRU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTestDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 311\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     all_targets\u001b[38;5;241m.\u001b[39mextend(target\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    308\u001b[0m     all_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    310\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[0;32m--> 311\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_batch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% | F1-Score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score_batch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% | Batch ID = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m    315\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = skipgram_model, \n",
    "                                                    hidden_size = 256, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 3, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5972 | Accuracy = 66.14% | F1-Score = 39.62% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6553\n",
      "Training Accuracy = 58.26%\n",
      "Training F1-Score = 5.59%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5561 | Accuracy = 67.54% | F1-Score = 56.34% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 65.22%\n",
      "Test F1-Score = 33.97%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 55.03% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 58.30%\n",
      "Training F1-Score = 5.81%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 54.50% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 52.91% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 59.26% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 47.62% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 60.85% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 54.50% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU_Attention, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 128, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = cbow_model, \n",
    "                                hidden_size = 256, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 3, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 57.67% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 59.79% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 56.61% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 63.49% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 60.32% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 61.90% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 59.26% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 55.56% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = cbow_model, \n",
    "                                                    hidden_size = 256, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 3, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 58.20% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 58.73% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 52.91% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 52.38% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 55.56% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 62.96% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 59.79% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 56.61% | F1-Score = 0.00% | Batch ID = 30 : 100%|██████████| 30/30 [00:06<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = nan\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 48.17% | F1-Score = 0.00% | Batch ID = 13 : 100%|██████████| 13/13 [00:01<00:00, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = nan\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU_Attention, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT tokenizer and model for sequence classification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data\n",
    "train_encodings = tokenizer(tweets_train['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(tweets_train['target'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for training data\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a model savel with extentions .bin\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "# Adjust DistilBERT for binary classification\n",
    "model.classifier = torch.nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data\n",
    "test_encodings = tokenizer(tweets_test['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# Convert labels to PyTorch tensors\n",
    "test_labels = torch.tensor(tweets_test['target'].tolist())\n",
    "# Create a DataLoader for training data\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "DistilBERT = model.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(DistilBERT.parameters(), lr = 1e-5, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2964 | Accuracy = 89.66% | F1-Score = 89.66% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.86it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4925\n",
      "Training Accuracy = 77.59%\n",
      "Training F1-Score = 70.15%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3072 | Accuracy = 90.32% | F1-Score = 89.66% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 32.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4437\n",
      "Test Accuracy = 80.45%\n",
      "Test F1-Score = 76.95%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4128 | Accuracy = 86.21% | F1-Score = 83.33% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3879\n",
      "Training Accuracy = 83.86%\n",
      "Training F1-Score = 79.87%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3541 | Accuracy = 87.10% | F1-Score = 83.33% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 32.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4110\n",
      "Test Accuracy = 82.26%\n",
      "Test F1-Score = 77.32%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2588 | Accuracy = 89.66% | F1-Score = 89.66% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.90it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3409\n",
      "Training Accuracy = 86.89%\n",
      "Training F1-Score = 83.76%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4910 | Accuracy = 77.42% | F1-Score = 69.57% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 32.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4258\n",
      "Test Accuracy = 81.98%\n",
      "Test F1-Score = 77.52%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4039 | Accuracy = 86.21% | F1-Score = 86.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2964\n",
      "Training Accuracy = 88.91%\n",
      "Training F1-Score = 86.26%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5968 | Accuracy = 74.19% | F1-Score = 77.78% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 32.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4529\n",
      "Test Accuracy = 80.82%\n",
      "Test F1-Score = 77.37%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4699 | Accuracy = 79.31% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.69it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2557\n",
      "Training Accuracy = 90.75%\n",
      "Training F1-Score = 88.69%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4525 | Accuracy = 83.87% | F1-Score = 76.19% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4692\n",
      "Test Accuracy = 82.07%\n",
      "Test F1-Score = 77.85%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1161 | Accuracy = 96.55% | F1-Score = 95.24% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2166\n",
      "Training Accuracy = 92.18%\n",
      "Training F1-Score = 90.48%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5097 | Accuracy = 80.65% | F1-Score = 76.92% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 31.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4992\n",
      "Test Accuracy = 81.55%\n",
      "Test F1-Score = 77.28%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2562 | Accuracy = 89.66% | F1-Score = 84.21% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1879\n",
      "Training Accuracy = 93.38%\n",
      "Training F1-Score = 91.99%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5145 | Accuracy = 77.42% | F1-Score = 72.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5088\n",
      "Test Accuracy = 81.73%\n",
      "Test F1-Score = 77.56%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2789 | Accuracy = 89.66% | F1-Score = 85.71% | Batch ID = 238 : 100%|██████████| 238/238 [00:24<00:00,  9.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1608\n",
      "Training Accuracy = 94.50%\n",
      "Training F1-Score = 93.38%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5422 | Accuracy = 83.87% | F1-Score = 76.19% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 31.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5399\n",
      "Test Accuracy = 81.46%\n",
      "Test F1-Score = 77.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(DistilBERT, train_loader, test_loader, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data\n",
    "train_encodings = tokenizer(tweets_train['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(tweets_train['target'].tolist())\n",
    "\n",
    "# Create a DataLoader for training data\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Tokenize test data\n",
    "test_encodings = tokenizer(tweets_test['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# Convert labels to PyTorch tensors\n",
    "test_labels = torch.tensor(tweets_test['target'].tolist())\n",
    "# Create a DataLoader for training data\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model for sequence classification\n",
    "bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# Adjust DistilBERT for binary classification\n",
    "# bert.classifier = torch.nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "bert = bert.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr = 1e-5, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5043 | Accuracy = 79.31% | F1-Score = 84.21% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4867\n",
      "Training Accuracy = 78.23%\n",
      "Training F1-Score = 72.31%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5482 | Accuracy = 70.97% | F1-Score = 66.67% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4430\n",
      "Test Accuracy = 80.57%\n",
      "Test F1-Score = 77.88%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5764 | Accuracy = 79.31% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:49<00:00,  4.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3802\n",
      "Training Accuracy = 84.55%\n",
      "Training F1-Score = 80.87%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4920 | Accuracy = 74.19% | F1-Score = 69.23% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4049\n",
      "Test Accuracy = 82.38%\n",
      "Test F1-Score = 77.77%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2739 | Accuracy = 89.66% | F1-Score = 88.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3259\n",
      "Training Accuracy = 87.07%\n",
      "Training F1-Score = 84.04%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2121 | Accuracy = 93.55% | F1-Score = 92.31% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4416\n",
      "Test Accuracy = 81.61%\n",
      "Test F1-Score = 77.65%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1834 | Accuracy = 93.10% | F1-Score = 90.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.66it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2632\n",
      "Training Accuracy = 90.15%\n",
      "Training F1-Score = 87.92%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5813 | Accuracy = 80.65% | F1-Score = 76.92% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4622\n",
      "Test Accuracy = 82.16%\n",
      "Test F1-Score = 77.63%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2690 | Accuracy = 89.66% | F1-Score = 86.96% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2158\n",
      "Training Accuracy = 92.04%\n",
      "Training F1-Score = 90.36%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4078 | Accuracy = 87.10% | F1-Score = 81.82% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5273\n",
      "Test Accuracy = 81.06%\n",
      "Test F1-Score = 77.61%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0735 | Accuracy = 96.55% | F1-Score = 96.77% | Batch ID = 238 : 100%|██████████| 238/238 [01:57<00:00,  2.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1674\n",
      "Training Accuracy = 94.01%\n",
      "Training F1-Score = 92.81%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3738 | Accuracy = 90.32% | F1-Score = 85.71% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5672\n",
      "Test Accuracy = 80.66%\n",
      "Test F1-Score = 76.83%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2331 | Accuracy = 87.50% | F1-Score = 85.71% | Batch ID = 52 :  22%|██▏       | 52/238 [00:10<00:39,  4.73it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_BERT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 425\u001b[0m, in \u001b[0;36mtrain_BERT\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m target \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    424\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 425\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    427\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1552\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1552\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1566\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1007\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:236\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    234\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 236\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m(position_ids)\n\u001b[1;32m    237\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(bert, train_loader, test_loader, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd015f111414629b4cda56f598f3e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587ffbef91e04477a8314692f005afbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8be0e050b344bdaaa5d8fdf3584c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data\n",
    "train_encodings = tokenizer(tweets_train['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(tweets_train['target'].tolist())\n",
    "\n",
    "# Create a DataLoader for training data\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Tokenize test data\n",
    "test_encodings = tokenizer(tweets_test['clean_text'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# Convert labels to PyTorch tensors\n",
    "test_labels = torch.tensor(tweets_test['target'].tolist())\n",
    "# Create a DataLoader for training data\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the roBERTa model for sequence classification\n",
    "roberta = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "# Adjust roBERTa for binary classification\n",
    "roberta.classifier.out_proj = torch.nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "roberta = roberta.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(roberta.parameters(), lr = 1e-5, weight_decay = 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6346 | Accuracy = 68.97% | F1-Score = 72.73% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4918\n",
      "Training Accuracy = 77.49%\n",
      "Training F1-Score = 69.92%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5058 | Accuracy = 74.19% | F1-Score = 71.43% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4403\n",
      "Test Accuracy = 80.82%\n",
      "Test F1-Score = 77.47%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3730 | Accuracy = 89.66% | F1-Score = 89.66% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3921\n",
      "Training Accuracy = 83.66%\n",
      "Training F1-Score = 79.55%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6448 | Accuracy = 67.74% | F1-Score = 68.75% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4325\n",
      "Test Accuracy = 81.61%\n",
      "Test F1-Score = 78.65%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3100 | Accuracy = 89.66% | F1-Score = 84.21% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3537\n",
      "Training Accuracy = 85.64%\n",
      "Training F1-Score = 82.14%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5206 | Accuracy = 74.19% | F1-Score = 69.23% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4390\n",
      "Test Accuracy = 81.52%\n",
      "Test F1-Score = 75.24%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2468 | Accuracy = 82.76% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3166\n",
      "Training Accuracy = 87.43%\n",
      "Training F1-Score = 84.40%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3820 | Accuracy = 83.87% | F1-Score = 80.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4498\n",
      "Test Accuracy = 82.22%\n",
      "Test F1-Score = 78.03%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2115 | Accuracy = 96.55% | F1-Score = 94.74% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2835\n",
      "Training Accuracy = 88.93%\n",
      "Training F1-Score = 86.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2999 | Accuracy = 83.87% | F1-Score = 80.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4659\n",
      "Test Accuracy = 81.98%\n",
      "Test F1-Score = 78.01%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1199 | Accuracy = 96.55% | F1-Score = 95.24% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2582\n",
      "Training Accuracy = 89.64%\n",
      "Training F1-Score = 87.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5744 | Accuracy = 74.19% | F1-Score = 66.67% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4664\n",
      "Test Accuracy = 81.58%\n",
      "Test F1-Score = 76.12%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3179 | Accuracy = 89.66% | F1-Score = 88.89% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2279\n",
      "Training Accuracy = 91.20%\n",
      "Training F1-Score = 89.36%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5297 | Accuracy = 83.87% | F1-Score = 84.85% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5581\n",
      "Test Accuracy = 81.34%\n",
      "Test F1-Score = 77.57%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1053 | Accuracy = 100.00% | F1-Score = 100.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2049\n",
      "Training Accuracy = 92.30%\n",
      "Training F1-Score = 90.78%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5515 | Accuracy = 77.42% | F1-Score = 78.79% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5387\n",
      "Test Accuracy = 80.78%\n",
      "Test F1-Score = 77.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(roberta, train_loader, test_loader, optimizer, criterion, epochs = 8, device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
