{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Custom Functions & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T23:10:35.149706Z",
     "start_time": "2023-12-29T23:10:30.436532800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import math\n",
    "import spacy \n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification, \\\n",
    "                         RobertaTokenizer, RobertaForSequenceClassification, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, \\\n",
    "                         PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Ignore RuntimeWarning and UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing & Extract features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the given text by removing URLs, non-alphabetic characters, making it lowercase,\n",
    "    removing one-character words, and replacing double spaces.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub('http\\S*', ' ', text)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "    # Remove one-character word\n",
    "    text = re.sub(\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(\"^[a-zA-Z]\\s+\", '', text)\n",
    "    # Replace double space with one space\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    # Tokenize, lemmatize, remove stop words\n",
    "    doc = nlp(text)\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "class BytePairEncoding():\n",
    "    \"\"\"\n",
    "    Byte Pair Encoding (BPE) tokenizer class.\n",
    "\n",
    "    Args:\n",
    "    - corpus_df (pd.DataFrame): DataFrame containing the corpus.\n",
    "    - vocab_size (int): Vocabulary size.\n",
    "    - min_frequency (int): Minimum frequency.\n",
    "    - maxlen (int): Maximum length of tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus_df, vocab_size, min_frequency, maxlen):\n",
    "        self.corpus = corpus_df.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on the corpus.\n",
    "        \"\"\"\n",
    "        # Initialize a tokenizer\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        # Initialize a pre-tokenizer\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        # Train the tokenizer on the corpus\n",
    "        trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, special_tokens=[\"[PAD]\"])\n",
    "        self.tokenizer.train_from_iterator(iterator=self.corpus, trainer=trainer)\n",
    "        # Create a fast tokenizer for integration with Transformers\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=self.tokenizer, pad_token='[PAD]', truncation=True, padding=True)\n",
    "        self.tokenizer = fast_tokenizer\n",
    "\n",
    "    def tokenize(self, corpus_df):\n",
    "        \"\"\"\n",
    "        Tokenize the given corpus.\n",
    "\n",
    "        Args:\n",
    "        - corpus_df (pd.DataFrame): DataFrame containing the corpus.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Tokenized corpus, input IDs, attention masks.\n",
    "        \"\"\"\n",
    "        corpus = corpus_df.tolist()\n",
    "        input_ids = []\n",
    "        attn_masks_corpus = []\n",
    "        tokenized_corpus = []\n",
    "        for tweet in corpus:\n",
    "            # Tokenize the tweet\n",
    "            tokenized_tweet = self.tokenizer.encode_plus(tweet, max_length=self.maxlen, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            # Retrieve the input sequence\n",
    "            input_ids.append(tokenized_tweet['input_ids'])\n",
    "            # Retrieve the attention mask\n",
    "            attn_masks_corpus.append(tokenized_tweet['attention_mask'])\n",
    "            # Retrieve the tokenized tweet\n",
    "            tokenized_corpus.append(self.tokenizer.convert_ids_to_tokens(tokenized_tweet['input_ids'].squeeze()))\n",
    "\n",
    "        return tokenized_corpus, input_ids, attn_masks_corpus\n",
    "\n",
    "\n",
    "def tokenize_text(tokenizer, df, text_column):\n",
    "    \"\"\"\n",
    "    Tokenize the text in the given DataFrame using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - tokenizer: Tokenizer object.\n",
    "    - df (pd.DataFrame): DataFrame containing the text.\n",
    "    - text_column (str): Column containing the text to be tokenized.\n",
    "    \"\"\"\n",
    "    # Extract the text from the specified column\n",
    "    texts = df[text_column].tolist()\n",
    "    # Tokenize the text in the DataFrame using the pre-trained tokenizer and remove \"Ġ\" character\n",
    "    df[text_column + '_tokenized'] = [[token.replace(\"Ġ\", \"\") for token in tokenizer.encode(text).tokens] for text in texts]\n",
    "\n",
    "\n",
    "def get_tfidf_matrix(df, vectorizer):\n",
    "    \"\"\"\n",
    "    Convert the TF-IDF matrix to a dense NumPy array and then to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (sparse matrix): TF-IDF matrix.\n",
    "    - vectorizer: TF-IDF vectorizer.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dense DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert the TF-IDF matrix to a dense NumPy array\n",
    "    matrix = df.todense()\n",
    "    # Convert the dense matrix to a DataFrame\n",
    "    matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def string2embedding_idx(tokens, model):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding embedding indices.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list): List of tokens.\n",
    "    - model (Word2Vec): Gensim Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of embedding indices.\n",
    "    \"\"\"\n",
    "    # Convert tokens to their embedding indices\n",
    "    embedding_indices = [model.wv.key_to_index[token] for token in tokens]\n",
    "\n",
    "    return embedding_indices\n",
    "\n",
    "\n",
    "def TSNE_10ClosestWords(model, word, size):\n",
    "    \"\"\"\n",
    "    Visualize 10 closest words to the given word using t-SNE.\n",
    "\n",
    "    Args:\n",
    "    - model (Word2Vec): Gensim Word2Vec model.\n",
    "    - word (str): Target word.\n",
    "    - size (int): Dimensionality of word vectors.\n",
    "    \"\"\"\n",
    "    # Initialize arrays for word vectors and labels\n",
    "    arr = np.empty((0, size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    # Get 10 closest words to the target word\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "    # Apply t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=10)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    # Get x and y coordinates\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # Scatter plot with annotations\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "    plt.xlim(x_coords.min() + 0.00005, x_coords.max() + 0.00005)\n",
    "    plt.ylim(y_coords.min() + 0.00005, y_coords.max() + 0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for handling tweet data.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing tweet data.\n",
    "    - word2vec_model (str): Word2Vec model type ('skipgram' or 'cbow').\n",
    "    \"\"\"\n",
    "    def __init__(self, df, word2vec_model):\n",
    "        self.df = df\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx, -1 if self.word2vec_model == 'skipgram' else -2]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        # Convert sequence to a 1D tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Convert label to a 1D tensor (scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return sequence_tensor, label_tensor\n",
    "\n",
    "\n",
    "class TweetsDatasetEncoderTransformer(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for handling tweet data with attention masks.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing tweet data.\n",
    "    - word2vec_model (str): Word2Vec model type ('skipgram' or 'cbow').\n",
    "    - attn_masks (list): List of attention masks.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, word2vec_model, attn_masks):\n",
    "        self.df = df\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.attn_masks = attn_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx, -1 if self.word2vec_model == 'skipgram' else -2]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        attn_mask = self.attn_masks[idx]\n",
    "\n",
    "        # Convert sequence to a 1D tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Convert label to a 1D tensor (scalar)\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        # Convert attention mask to a 1D tensor (scalar)\n",
    "        attn_mask_tensor = attn_mask.flatten().bool()\n",
    "\n",
    "        return sequence_tensor, attn_mask_tensor, label_tensor\n",
    "\n",
    "\n",
    "def TokenizeBERT(tokenizer, df, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Tokenize text data using BERT tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - tokenizer: BERT tokenizer.\n",
    "    - df (pd.DataFrame): DataFrame containing text data.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tokenized dataset and DataLoader.\n",
    "    \"\"\"\n",
    "    # Tokenize training data\n",
    "    encodings = tokenizer(df['clean_text'].tolist(), add_special_tokens=True, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert labels to PyTorch tensors\n",
    "    labels = torch.tensor(df['target'].tolist())\n",
    "\n",
    "    # Create a DataLoader for training data\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def TokenizeBERTweet(tokenizer, df, batch_size=32, shuffle=True, tokenizer_normalizeTweet=None):\n",
    "    \"\"\"\n",
    "    Tokenize tweet data using BERTweet tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - tokenizer: BERTweet tokenizer.\n",
    "    - df (pd.DataFrame): DataFrame containing tweet data.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - shuffle (bool): Whether to shuffle the data.\n",
    "    - tokenizer_normalizeTweet: Optional normalizer for tweet text.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tokenized dataset and DataLoader.\n",
    "    \"\"\"\n",
    "    # Normalize the text\n",
    "    if tokenizer_normalizeTweet is not None:\n",
    "        normalized_tweets = df['text'].apply(lambda x: tokenizer_normalizeTweet.normalizeTweet(x))\n",
    "    else:\n",
    "        normalized_tweets = df['text'].apply(lambda x: tokenizer.normalizeTweet(x))\n",
    "\n",
    "    # Tokenize training data\n",
    "    encodings = tokenizer(normalized_tweets.tolist(), add_special_tokens=True, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert labels to PyTorch tensors\n",
    "    labels = torch.tensor(df['target'].tolist())\n",
    "\n",
    "    # Create a DataLoader for training data\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device for PyTorch based on availability.\n",
    "\n",
    "    Returns:\n",
    "    - torch.device: The selected device.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Use GPU if available\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # Use MPS (Metal Performance Shaders) if available (GPU of Apple Silicon's MacBooks)\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        # Use CPU as a fallback\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Print the selected device\n",
    "    print('Device:', device)\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN model to be used with the TF-IDF features\n",
    "class CustomFCNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the CustomFCNN model.\n",
    "\n",
    "        Args:\n",
    "        - input_size (int): Number of input features.\n",
    "        - hidden_size (int): Number of hidden units in the hidden layers.\n",
    "        - dropout_rate (float): Dropout rate for regularization (default is 0.1).\n",
    "        \"\"\"\n",
    "        super(CustomFCNN, self).__init__()\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.outlayer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Activation function and normalization\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Model predictions (logits).\n",
    "        \"\"\"\n",
    "        # Pass the input through the fc layers, activation function, and normalization\n",
    "        x = self.bn(self.relu(self.fc1(x)))\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass the input through the fc layers, activation function, and normalization\n",
    "        x = self.bn(self.relu(self.fc2(x)))\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass the input through the fc layers, activation function, and normalization\n",
    "        x = self.bn(self.relu(self.fc3(x)))\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass the input through the output layer\n",
    "        logits = self.outlayer(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (LSTM & GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model with pre-trained Word2Vec embeddings\n",
    "class CustomLSTM(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers=1, bidirectional=False, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the CustomLSTM model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Gensim Word2Vec model.\n",
    "        - hidden_size (int): Number of hidden units in the LSTM layer.\n",
    "        - output_size (int): Number of output units.\n",
    "        - num_layers (int): Number of LSTM layers (default is 1).\n",
    "        - bidirectional (bool): Whether the LSTM is bidirectional (default is False).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings during training (default is True).\n",
    "        \"\"\"\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        # Get the embedding dimension\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        # Initialize the embedding layer with our custom pre-trained embeddings\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings=torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        # Initialize the LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # Initialize the fully-connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract the word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, _ = self.lstm(x)\n",
    "        # Pass the output of the last time step through the fully-connected layer\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings\n",
    "class CustomGRU(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers=1, bidirectional=False, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the CustomGRU model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Gensim Word2Vec model.\n",
    "        - hidden_size (int): Number of hidden units in the GRU layer.\n",
    "        - output_size (int): Number of output units.\n",
    "        - num_layers (int): Number of GRU layers (default is 1).\n",
    "        - bidirectional (bool): Whether the GRU is bidirectional (default is False).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings during training (default is True).\n",
    "        \"\"\"\n",
    "        super(CustomGRU, self).__init__()\n",
    "        # Get the embedding dimension\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        # Initialize the embedding layer with our custom pre-trained embeddings\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings=torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        # Initialize the GRU layer\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # Initialize the fully-connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract the word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the GRU layer\n",
    "        output, _ = self.gru(x)\n",
    "        # Pass the output of the last time step through the fully-connected layer\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Attention module\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Initialize the attention layer\n",
    "        self.attn = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, output):\n",
    "        # Apply the attention layer\n",
    "        attention_scores = self.attn(output)\n",
    "\n",
    "        return torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomLSTM_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers=1, bidirectional=False, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the CustomLSTM_Attention model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Gensim Word2Vec model.\n",
    "        - hidden_size (int): Number of hidden units in the LSTM layer.\n",
    "        - output_size (int): Number of output units.\n",
    "        - num_layers (int): Number of LSTM layers (default is 1).\n",
    "        - bidirectional (bool): Whether the LSTM is bidirectional (default is False).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings during training (default is True).\n",
    "        \"\"\"\n",
    "        super(CustomLSTM_Attention, self).__init__()\n",
    "        # Get the embedding dimension\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        # Initialize the embedding layer with our custom pre-trained embeddings\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        # Initialize the LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # Initialize the attention layer\n",
    "        self.attention = Attention(hidden_size * (2 if bidirectional else 1))\n",
    "        # Initialize the fully-connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract the word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, _ = self.lstm(x)\n",
    "        # Apply the attention mechanism\n",
    "        attn_weights = self.attention(output)\n",
    "        # Apply the attention weights to the output of the LSTM layer\n",
    "        output = torch.sum(attn_weights * output, dim=1)\n",
    "        # Pass the output of the last time step through the fully-connected layer\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomGRU_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers=1, bidirectional=False, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the CustomGRU_Attention model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Gensim Word2Vec model.\n",
    "        - hidden_size (int): Number of hidden units in the GRU layer.\n",
    "        - output_size (int): Number of output units.\n",
    "        - num_layers (int): Number of GRU layers (default is 1).\n",
    "        - bidirectional (bool): Whether the GRU is bidirectional (default is False).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings during training (default is True).\n",
    "        \"\"\"\n",
    "        super(CustomGRU_Attention, self).__init__()\n",
    "        # Get the embedding dimension\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        # Initialize the embedding layer with our custom pre-trained embeddings\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        # Initialize the GRU layer\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # Initialize the attention layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        # Initialize the fully-connected layer\n",
    "        self.attention = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract the word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the GRU layer\n",
    "        output, _ = self.gru(x)\n",
    "        # Apply the attention mechanism\n",
    "        attention_weights = torch.nn.functional.softmax(self.attention(output), dim=1)\n",
    "        # Apply the attention weights to the output of the GRU layer\n",
    "        output = torch.sum(attention_weights * output, dim=1)\n",
    "        # Pass the output of the last time step through the fully-connected layer\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and multi-head attention mechanism\n",
    "class CustomLSTM_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, dropout=0.1, num_layers=1, bidirectional=False, freeze_embeddings=True, num_heads=8):\n",
    "        \"\"\"\n",
    "        Initialize the CustomLSTM_MultiHeadAttention model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Gensim Word2Vec model.\n",
    "        - hidden_size (int): Number of hidden units in the LSTM layer.\n",
    "        - output_size (int): Number of output units.\n",
    "        - dropout (float): Dropout rate for regularization (default is 0.1).\n",
    "        - num_layers (int): Number of LSTM layers (default is 1).\n",
    "        - bidirectional (bool): Whether the LSTM is bidirectional (default is False).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings during training (default is True).\n",
    "        - num_heads (int): Number of attention heads in the multi-head attention mechanism (default is 8).\n",
    "        \"\"\"\n",
    "        super(CustomLSTM_MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.sequence_size = 50\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attention = torch.nn.MultiheadAttention(embed_dim=hidden_size * (2 if bidirectional else 1), num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Fully-connected layers for classification head\n",
    "        self.fc1 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1) * self.sequence_size, hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc2 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size * (2 if bidirectional else 1))\n",
    "        self.classification_head = torch.nn.Sequential(self.fc1, self.relu, self.bn, self.dropout, self.fc2)\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization\n",
    "        for layer in self.classification_head:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the multi-head attention\n",
    "        torch.nn.init.kaiming_uniform_(self.multihead_attention.in_proj_weight, nonlinearity='relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the LSTM\n",
    "        for layer in self.lstm._all_weights:\n",
    "            for param_name in layer:\n",
    "                if 'weight' in param_name:\n",
    "                    torch.nn.init.kaiming_uniform_(getattr(self.lstm, param_name), nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract the word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        hx, cx = self.lstm(x)\n",
    "        # Pass the output of the LSTM layer through the multi-head attention mechanism\n",
    "        attn_output, attn_weights = self.multihead_attention(hx, hx, hx)\n",
    "        # Flatten the output of the multi-head attention mechanism\n",
    "        flattened_output = attn_output.reshape(attn_output.size(0), -1)\n",
    "        # Pass the flattened output through the classification head\n",
    "        logits = self.classification_head(flattened_output)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTweet for Text Classification (without using AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTweetForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, bertweet_model, hidden_size, output_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the BERTweetForSequenceClassification model.\n",
    "\n",
    "        Args:\n",
    "        - bertweet_model: BERTweet model.\n",
    "        - hidden_size (int): Number of hidden units in the output of BERTweet.\n",
    "        - output_size (int): Number of output units.\n",
    "        - dropout_rate (float): Dropout rate for regularization (default is 0.1).\n",
    "        \"\"\"\n",
    "        super(BERTweetForSequenceClassification, self).__init__()\n",
    "\n",
    "        # BERTweet model as the base model\n",
    "        self.bertweet_model = bertweet_model\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Batch Normalization layer\n",
    "        self.BatchNorm = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # Fully-connected layer for classification output\n",
    "        self.dense = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through the BERTweet model\n",
    "        berteet_output = self.bertweet_model(input_ids, attention_mask, output_attentions=True)\n",
    "        \n",
    "        # Apply batch normalization to the output of the BERTweet model\n",
    "        output = self.BatchNorm(berteet_output.pooler_output)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Apply the fully-connected layer for classification output\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        # Return the logits and the BERTweet output (including attention information)\n",
    "        return logits, berteet_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderTransformer model.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Word2Vec model for token embeddings.\n",
    "        - d_model (int): Dimension of model embeddings.\n",
    "        - num_heads (int): Number of attention heads in the transformer.\n",
    "        - num_layers (int): Number of transformer layers.\n",
    "        - d_ff (int): Dimension of the feedforward layer.\n",
    "        - max_seq_length (int): Maximum sequence length.\n",
    "        - dropout (float): Dropout rate for regularization (default is 0.1).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings (default is True).\n",
    "        \"\"\"\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "\n",
    "        # Word embeddings layer\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        \n",
    "        # Positional embeddings layer\n",
    "        self.position_embeddings = torch.nn.Embedding(max_seq_length, d_model)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.LayerNorm = torch.nn.LayerNorm(d_model, eps=1e-8, elementwise_affine=True)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.BatchNorm = torch.nn.BatchNorm1d(d_model * 2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers, norm=self.LayerNorm)\n",
    "\n",
    "        # Dense layer\n",
    "        self.dense = torch.nn.Linear(d_model * max_seq_length, d_model * 2)\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = torch.nn.Linear(d_model * 2, 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.Tanh = torch.nn.Tanh()\n",
    "        self.LeakyReLU = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, tweet, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderTransformer model.\n",
    "\n",
    "        Args:\n",
    "        - tweet (torch.Tensor): Input tensor representing the tweet.\n",
    "        - attention_mask (torch.Tensor): Attention mask tensor for the transformer.\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): Output logits of the model.\n",
    "        \"\"\"\n",
    "        # Get the embeddings of the tokens\n",
    "        embeddings = self.embeddings(tweet)\n",
    "        \n",
    "        # Get the positional embeddings\n",
    "        position_ids = torch.arange(tweet.size(1)).unsqueeze(0).to(tweet.device)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Add the positional embeddings to the token embeddings\n",
    "        embeddings += position_embeddings\n",
    "        \n",
    "        # Apply layer normalization and dropout\n",
    "        embeddings = self.dropout(self.LayerNorm(embeddings))\n",
    "        \n",
    "        # Permute the embeddings to have the shape (seq_length, batch_size, embedding_dim)\n",
    "        embeddings = embeddings.permute(1, 0, 2)\n",
    "\n",
    "        # Apply the transformer encoder\n",
    "        encoder_output = self.encoder(embeddings, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Permute the output to have the shape (batch_size, seq_length, embedding_dim)\n",
    "        encoder_output = encoder_output.permute(1, 0, 2)\n",
    "\n",
    "        # Flatten output but keep the batch dimension\n",
    "        flatten_output = encoder_output.reshape(encoder_output.size(0), -1)\n",
    "\n",
    "        # Apply a dense layer followed by a non-linear activation function\n",
    "        output = self.BatchNorm(self.ReLU(self.dense(flatten_output)))\n",
    "        \n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Apply the classifier layer\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Transformer implemented from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimension of the model.\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_k = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_v = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_o = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "        - Q (torch.Tensor): Query tensor.\n",
    "        - K (torch.Tensor): Key tensor.\n",
    "        - V (torch.Tensor): Value tensor.\n",
    "        - mask (torch.Tensor): Mask tensor for attention.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            mask = (mask != 0).unsqueeze(1).unsqueeze(2)\n",
    "            # Apply mask to attention scores so padding terms have score of -1e9 (attention weight almost 0 after softmax)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        # Get attention output by multiplying attention probabilities with values (V)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForwardNN(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Feedforward Neural Network module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimension of the model.\n",
    "        - d_ff (int): Dimension of the feedforward layer.\n",
    "        \"\"\"\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Feedforward Neural Network.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        \"\"\"\n",
    "        Positional Encoding module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimension of the model.\n",
    "        - max_seq_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Positional Encoding.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to word embeddings\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Encoder Layer module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimension of the model.\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        - d_ff (int): Dimension of the feedforward layer.\n",
    "        - dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardNN(d_model, d_ff)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "        - mask (torch.Tensor): Mask tensor.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        attn_output = self.self_attn(Q=x, K=x, V=x, mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, layer_norm=None):\n",
    "        \"\"\"\n",
    "        Encoder module.\n",
    "\n",
    "        Args:\n",
    "        - encoder_layer (nn.Module): Encoder layer.\n",
    "        - num_layers (int): Number of encoder layers.\n",
    "        - layer_norm (nn.Module): Layer normalization module.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = layer_norm\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "        - mask (torch.Tensor): Mask tensor.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        encoder_layers = torch.nn.ModuleList([self.encoder_layer for _ in range(self.num_layers)])\n",
    "\n",
    "        for encoder_layer in encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "            if self.layer_norm is not None:\n",
    "                x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderTransformer_scratch(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1, freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Encoder Transformer (from scratch) module.\n",
    "\n",
    "        Args:\n",
    "        - word2vec_model: Word2Vec model for token embeddings.\n",
    "        - d_model (int): Dimension of model embeddings.\n",
    "        - num_heads (int): Number of attention heads in the transformer.\n",
    "        - num_layers (int): Number of transformer layers.\n",
    "        - d_ff (int): Dimension of the feedforward layer.\n",
    "        - max_seq_length (int): Maximum sequence length.\n",
    "        - dropout (float): Dropout rate for regularization (default is 0.1).\n",
    "        - freeze_embeddings (bool): Whether to freeze word embeddings (default is True).\n",
    "        \"\"\"\n",
    "        super(EncoderTransformer_scratch, self).__init__()\n",
    "        self.encoder_embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.LayerNorm = torch.nn.LayerNorm(d_model, eps=1e-8, elementwise_affine=True)\n",
    "        self.encoder = Encoder(self.encoder_layer, num_layers, self.LayerNorm)\n",
    "\n",
    "        self.LayerNorm_embedd = torch.nn.LayerNorm(d_model, eps=1e-8, elementwise_affine=True)\n",
    "\n",
    "        # Output layer for binary classification\n",
    "        self.dense = torch.nn.Linear(d_model * max_seq_length, d_model * 2)\n",
    "        self.classifier = torch.nn.Linear(d_model * 2, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.BatchNorm = torch.nn.BatchNorm1d(d_model * 2)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, tweets, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder Transformer (from scratch) model.\n",
    "\n",
    "        Args:\n",
    "        - tweets (torch.Tensor): Input tensor representing the tweets.\n",
    "        - attention_mask (torch.Tensor): Attention mask tensor for the transformer.\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): Output logits of the model.\n",
    "        \"\"\"\n",
    "        # Extract word embeddings from tweet\n",
    "        wrd_embeddings = self.encoder_embedding(tweets)\n",
    "        # Add positional encoding to word embeddings\n",
    "        embeddings = self.positional_encoding(wrd_embeddings)\n",
    "        # Apply dropout to embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        # Apply layer normalization to embeddings\n",
    "        embeddings = self.LayerNorm_embedd(embeddings)\n",
    "    \n",
    "        # Apply encoder to embeddings\n",
    "        enc_output = self.encoder(embeddings, attention_mask)\n",
    "        \n",
    "        # Flatten output\n",
    "        flatten_output = enc_output.reshape(enc_output.size(0), -1)\n",
    "        # Apply a dense layer followed by a non-linear activation function and batch normalization\n",
    "        output = self.BatchNorm(self.ReLU(self.dense(flatten_output)))\n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        # Apply the classifier layer\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains our custom FCNN and Recurrent Neural Networks.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_f1_lst (list): List containing training f1-score values for each epoch.\n",
    "    - test_loss_hist (list): List containing test loss values for each epoch.\n",
    "    - test_f1_lst (list): List containing test f1-score values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_fn', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predicted = (output > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in pbar:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (output > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_BERT(model, train_loader, test_loader, optimizer, scheduler, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Fine-tunes pre-trained encoder transformers based on BERT, except BERTweet.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_f1_lst (list): List containing training f1-score values for each epoch.\n",
    "    - test_loss_hist (list): List containing test loss values for each epoch.\n",
    "    - test_f1_lst (list): List containing test f1-score values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(input_ids, attention_mask=attn_mask, labels=target)\n",
    "            logits = output.logits.squeeze(-1)\n",
    "\n",
    "            loss = output.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].to(device)\n",
    "                model.zero_grad()\n",
    "                output = model(input_ids, attention_mask=attn_mask, labels=target)\n",
    "                logits = output.logits.squeeze(-1)\n",
    "\n",
    "                loss = output.loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_BERTweet(model, train_loader, test_loader, optimizer, scheduler, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Fine-tunes a BERTweet model, either base or large.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - scheduler (torch.optim.lr_scheduler): The learning rate scheduler used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_f1_lst (list): List containing training f1-score values for each epoch.\n",
    "    - test_loss_hist (list): List containing test loss values for each epoch.\n",
    "    - test_f1_lst (list): List containing test f1-score values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_func', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].float().unsqueeze(1).to(device)\n",
    "            model.zero_grad()\n",
    "            logits, bertweet_output = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            predicted = (logits > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].float().unsqueeze(1).to(device)\n",
    "                model.zero_grad()\n",
    "                logits, bertweet_output = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "                loss = loss_fn(logits, target)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (logits > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_EncoderTransformer(model, train_loader, test_loader, optimizer, scheduler, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains our custom Encoder Transformers.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - scheduler (torch.optim.lr_scheduler): The learning rate scheduler used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_f1_lst (list): List containing training f1-score values for each epoch.\n",
    "    - test_loss_hist (list): List containing test loss values for each epoch.\n",
    "    - test_f1_lst (list): List containing test f1-score values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_func', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].float().unsqueeze(1).to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            predicted = (logits > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].float().unsqueeze(1).to(device)\n",
    "                model.zero_grad()\n",
    "                logits = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "                loss = loss_fn(logits, target)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (logits > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on the test set\n",
    "def predict(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Make predictions on the test set using the provided model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): PyTorch model.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "    - device (torch.device): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - predictions (list): List of predicted labels.\n",
    "    - labels (list): List of true labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].float().flatten().to(device)\n",
    "            model.zero_grad()\n",
    "            logits, bert_output = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "            # Thresholding to convert logits to binary predictions\n",
    "            predicted = (logits > 0.0).float().flatten()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            predictions.extend(predicted.tolist())\n",
    "            labels.extend(target.tolist())\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# Function to compute and display confusion matrix\n",
    "def ComputeConfusionMatrix(labels, preds):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for the predicted and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "    - labels (list): List of true labels.\n",
    "    - preds (list): List of predicted labels.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    confusion_table = tabulate(cm,\n",
    "                               headers=['Predicted ' + cls for cls in ['Not a Disaster', 'Disaster']],\n",
    "                               showindex=['Actual ' + cls for cls in ['Not a Disaster', 'Disaster']],\n",
    "                               tablefmt=\"fancy_grid\",\n",
    "                               stralign=\"center\",\n",
    "                               numalign=\"center\"\n",
    "                               )\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_table)\n",
    "\n",
    "\n",
    "# Function to compute and display classification metrics\n",
    "def ComputeClassificationMetrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Computes the classification metrics for the predicted and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "    - labels (list): List of true labels.\n",
    "    - preds (list): List of predicted labels.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(labels, preds) * 100\n",
    "    precision = precision_score(labels, preds, average='binary') * 100\n",
    "    recall = recall_score(labels, preds, average='binary') * 100\n",
    "    f1 = f1_score(labels, preds, average='binary') * 100\n",
    "\n",
    "    results_table = [\n",
    "        [\"Accuracy\", f\"{accuracy:.2f}%\"],\n",
    "        [\"Weighted Precision\", f\"{precision:.2f}%\"],\n",
    "        [\"Weighted Recall\", f\"{recall:.2f}%\"],\n",
    "        [\"Weighted F1 Score\", f\"{f1:.2f}%\"],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(results_table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\", stralign=\"center\", numalign=\"center\"))\n",
    "\n",
    "\n",
    "# Function to compute and visualize attention maps\n",
    "def ComputeAttentionMaps(dataset, model, tokenizer, input_id, multiheadlayer_no, head_no):\n",
    "    \"\"\"\n",
    "    Computes and visualizes attention maps for a specific input instance.\n",
    "\n",
    "    Args:\n",
    "    - dataset (torch.utils.data.Dataset): Dataset containing input instances.\n",
    "    - model (torch.nn.Module): Transformer model.\n",
    "    - tokenizer: Tokenizer for input sequences.\n",
    "    - input_id (int): Index of the input instance to visualize.\n",
    "    - multiheadlayer_no (int): Index of the multi-head layer.\n",
    "    - head_no (int): Index of the attention head.\n",
    "    \"\"\"\n",
    "    device = set_device()\n",
    "\n",
    "    # Get input ids and attention masks from the test dataset\n",
    "    inputs = torch.stack([data[0] for data in dataset]).to(device)\n",
    "    attn_masks = torch.stack([data[1] for data in dataset]).to(device)\n",
    "\n",
    "    # Get logits and attention weights for the specified input instance\n",
    "    logits, bert_output = model(inputs[input_id].unsqueeze(0), attention_mask=attn_masks[input_id].unsqueeze(0))\n",
    "\n",
    "    attn_weights = bert_output.attentions\n",
    "    attn_weights = torch.stack(attn_weights).squeeze(1)\n",
    "    attn_weights = attn_weights[multiheadlayer_no, head_no, :, :].cpu().detach().numpy()\n",
    "\n",
    "    len_seq = sum(attn_masks[input_id]).item()\n",
    "    attention_layer_head = attn_weights[1:len_seq-1, 1:len_seq-1]\n",
    "\n",
    "    sequence = inputs[input_id][1:len_seq-1]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sequence.cpu().numpy())\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(attention_layer_head, cmap=\"binary\", xticklabels=tokens, yticklabels=tokens, annot=False, fmt=\".2f\", cbar=True)\n",
    "    plt.yticks(rotation=0)\n",
    "    ax.xaxis.tick_top()  # x axis on top\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:48:51.252105700Z",
     "start_time": "2023-12-29T16:48:49.728914500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                 Just happened a terrible car crash       1\n",
       "1  Heard about #earthquake is different cities, s...       1\n",
       "2  there is a forest fire at spot pond, geese are...       1\n",
       "3           Apocalypse lighting. #Spokane #wildfires       1\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_train = pd.read_csv('../data/tweets_data/train.csv')[['text', 'target']].reset_index(drop=True)\n",
    "tweets_test = pd.read_csv('../data/tweets_data/test.csv')[['id', 'text']]\n",
    "tweets_labels = pd.read_csv('../data/tweets_data/test_labels.csv', encoding='latin-1')[['choose_one', 'text']]\n",
    "\n",
    "tweets_labels['target'] = (tweets_labels['choose_one']=='Relevant').astype(int)\n",
    "tweets_labels['id'] = tweets_labels.index\n",
    "\n",
    "tweets_test = pd.merge(left = tweets_test, right = tweets_labels, on='id', how = 'left')[['id', 'text_x', 'target']]\n",
    "tweets_test.rename(columns={'text_x': 'text'}, inplace=True)\n",
    "tweets_test = tweets_test[['text', 'target']]\n",
    "\n",
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing & Extract features from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text using key-words and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.782176300Z",
     "start_time": "2023-12-21T15:27:58.466800200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_train['clean_text'] = tweets_train['text'].apply(preprocess)\n",
    "tweets_test['clean_text'] = tweets_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.798553700Z",
     "start_time": "2023-12-21T15:29:01.785391100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \n",
       "0               deed reason earthquake allah forgive  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3    people receive wildfire evacuation order cal...  \n",
       "4  got send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \n",
       "0                     happen terrible car crash  \n",
       "1      hear earthquake different city stay safe  \n",
       "2  forest fire spot pond goose flee street save  \n",
       "3          apocalypse lighting spokane wildfire  \n",
       "4            typhoon soudelor kill china taiwan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word tokenization with BERT Tokenizer for Byte-Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an object for Byte Pair Encoding\n",
    "BPETokenizer = BytePairEncoding(corpus_df = tweets_train['clean_text'], vocab_size = 20000, min_frequency = 1, maxlen = 50)\n",
    "# Train the BPE tokenizer on the training data\n",
    "BPETokenizer.train()\n",
    "\n",
    "# Tokenize the training data using the pre-trained tokenizer\n",
    "tokenized_train_corpus, input_ids_train, attn_masks_train = BPETokenizer.tokenize(corpus_df = tweets_train['clean_text'])\n",
    "# Tokenize the test data using the pre-trained tokenizer\n",
    "tokenized_test_corpus, input_ids_test, attn_masks_test = BPETokenizer.tokenize(corpus_df = tweets_test['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the tokenized tweets\n",
    "tweets_train['clean_text_tokenized'] = tokenized_train_corpus\n",
    "tweets_test['clean_text_tokenized'] = tokenized_test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...  \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...  \n",
       "2  [resident, ask, shelter, place, notify, office...  \n",
       "3  [people, receive, wildfire, evacuation, order,...  \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...  \n",
       "1  [hear, earthquake, different, city, stay, safe...  \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...  \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...  \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text into Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.476189500Z",
     "start_time": "2023-12-21T15:33:21.430017200Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Filter out [PAD] tokens\n",
    "filtered_tokens = tweets_train['clean_text_tokenized'].apply(lambda tokens: [token for token in tokens if token != '[PAD]'])\n",
    "X_train = vectorizer.fit_transform(filtered_tokens.apply(lambda tokens: ' '.join(tokens)))\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_train['TFIDF'] = X_train.toarray().tolist()\n",
    "\n",
    "# Filter out [PAD] tokens\n",
    "filtered_tokens = tweets_test['clean_text_tokenized'].apply(lambda tokens: [token for token in tokens if token != '[PAD]'])\n",
    "X_test = vectorizer.fit_transform(filtered_tokens.apply(lambda tokens: ' '.join(tokens)))\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_test['TFIDF'] = X_test.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.661826500Z",
     "start_time": "2023-12-21T15:33:23.626279500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...   \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [people, receive, wildfire, evacuation, order,...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...   \n",
       "1  [hear, earthquake, different, city, stay, safe...   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...   \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (CBOW and Skip-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97446400, 97446400)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create object for the Continuous Bag of Words model\n",
    "cbow_model = Word2Vec(sentences = None, vector_size = 512, window = 10, min_count = 1, workers = 4, sg = 0) \n",
    "# Set the vocabulary and initialize vectors\n",
    "cbow_model.wv.index_to_key = list(BPETokenizer.tokenizer.get_vocab().keys())\n",
    "cbow_model.build_vocab([list(BPETokenizer.tokenizer.get_vocab().keys())])\n",
    "cbow_model.train(corpus_iterable = tweets_train['clean_text_tokenized'].tolist(), total_examples = len(tweets_train['clean_text_tokenized']), epochs = 256)\n",
    "\n",
    "# Create object for the Skip-Gram model\n",
    "skipgram_model = Word2Vec(sentences = None, vector_size = 512, window = 5, min_count = 1, workers = 4, sg = 1)\n",
    "# Set the vocabulary and initialize vectors\n",
    "skipgram_model.wv.index_to_key = list(BPETokenizer.tokenizer.get_vocab().keys())\n",
    "skipgram_model.build_vocab([list(BPETokenizer.tokenizer.get_vocab().keys())])\n",
    "skipgram_model.train(corpus_iterable = tweets_train['clean_text_tokenized'].tolist(), total_examples = len(tweets_train['clean_text_tokenized']), epochs = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the string2embedding_idx function to create a new column with the indices of the embeddings\n",
    "tweets_train['CBOW_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_train['SkipGram_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))\n",
    "tweets_test['CBOW_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_test['SkipGram_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:36.442786700Z",
     "start_time": "2023-12-21T15:34:36.381065300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[17602, 10719, 19839, 12622, 6644, 1692, 1692,...</td>\n",
       "      <td>[17602, 10719, 19839, 12622, 6644, 1692, 1692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[2963, 18922, 17193, 1710, 3447, 4630, 4839, 1...</td>\n",
       "      <td>[2963, 18922, 17193, 1710, 3447, 4630, 4839, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[11892, 4895, 2565, 14191, 16962, 12905, 478, ...</td>\n",
       "      <td>[11892, 4895, 2565, 14191, 16962, 12905, 478, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[17862, 3705, 1735, 478, 11879, 19559, 1692, 1...</td>\n",
       "      <td>[17862, 3705, 1735, 478, 11879, 19559, 1692, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[13345, 13823, 6316, 16250, 13806, 9137, 1735,...</td>\n",
       "      <td>[13345, 13823, 6316, 16250, 13806, 9137, 1735,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...   \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [people, receive, wildfire, evacuation, order,...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [17602, 10719, 19839, 12622, 6644, 1692, 1692,...   \n",
       "1  [2963, 18922, 17193, 1710, 3447, 4630, 4839, 1...   \n",
       "2  [11892, 4895, 2565, 14191, 16962, 12905, 478, ...   \n",
       "3  [17862, 3705, 1735, 478, 11879, 19559, 1692, 1...   \n",
       "4  [13345, 13823, 6316, 16250, 13806, 9137, 1735,...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [17602, 10719, 19839, 12622, 6644, 1692, 1692,...  \n",
       "1  [2963, 18922, 17193, 1710, 3447, 4630, 4839, 1...  \n",
       "2  [11892, 4895, 2565, 14191, 16962, 12905, 478, ...  \n",
       "3  [17862, 3705, 1735, 478, 11879, 19559, 1692, 1...  \n",
       "4  [13345, 13823, 6316, 16250, 13806, 9137, 1735,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[6324, 13317, 17161, 9642, 1692, 1692, 1692, 1...</td>\n",
       "      <td>[6324, 13317, 17161, 9642, 1692, 1692, 1692, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8315, 19839, 18765, 18285, 9238, 15253, 1692,...</td>\n",
       "      <td>[8315, 19839, 18765, 18285, 9238, 15253, 1692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[2963, 18922, 635, 9054, 798, 13853, 19011, 12...</td>\n",
       "      <td>[2963, 18922, 635, 9054, 798, 13853, 19011, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[9912, 15894, 954, 1735, 1692, 1692, 1692, 169...</td>\n",
       "      <td>[9912, 15894, 954, 1735, 1692, 1692, 1692, 169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[10650, 494, 7889, 15421, 1739, 1692, 1692, 16...</td>\n",
       "      <td>[10650, 494, 7889, 15421, 1739, 1692, 1692, 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...   \n",
       "1  [hear, earthquake, different, city, stay, safe...   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...   \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [6324, 13317, 17161, 9642, 1692, 1692, 1692, 1...   \n",
       "1  [8315, 19839, 18765, 18285, 9238, 15253, 1692,...   \n",
       "2  [2963, 18922, 635, 9054, 798, 13853, 19011, 12...   \n",
       "3  [9912, 15894, 954, 1735, 1692, 1692, 1692, 169...   \n",
       "4  [10650, 494, 7889, 15421, 1739, 1692, 1692, 16...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [6324, 13317, 17161, 9642, 1692, 1692, 1692, 1...  \n",
       "1  [8315, 19839, 18765, 18285, 9238, 15253, 1692,...  \n",
       "2  [2963, 18922, 635, 9054, 798, 13853, 19011, 12...  \n",
       "3  [9912, 15894, 954, 1735, 1692, 1692, 1692, 169...  \n",
       "4  [10650, 494, 7889, 15421, 1739, 1692, 1692, 16...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:38.609225400Z",
     "start_time": "2023-12-21T15:34:37.441115300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'earthquake', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:39.868591200Z",
     "start_time": "2023-12-21T15:34:39.426708900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'earthquake', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:40.997210400Z",
     "start_time": "2023-12-21T15:34:40.562299400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'disaster', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:42.087371100Z",
     "start_time": "2023-12-21T15:34:41.642630900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'disaster', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-Connected Neural Network with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF column to a torch tensor\n",
    "X_train = torch.FloatTensor(tweets_train['TFIDF'].tolist())\n",
    "# Convert the target column to a torch tensor\n",
    "y_train = torch.FloatTensor(tweets_train['target'].tolist()).unsqueeze(1)\n",
    "\n",
    "# Convert the TF-IDF column to a torch tensor\n",
    "X_test = torch.FloatTensor(tweets_test['TFIDF'].tolist())\n",
    "# Convert the target column to a torch tensor\n",
    "y_test = torch.FloatTensor(tweets_test['target'].tolist()).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object\n",
    "TrainDatasetTFIDF = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# Create a torch DataLoader object\n",
    "TrainDataLoaderTFIDF = torch.utils.data.DataLoader(TrainDatasetTFIDF, batch_size = 1024)\n",
    "\n",
    "# Create a torch Dataset object\n",
    "TestDatasetTFIDF = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "# Create a torch DataLoader object\n",
    "TestDataLoaderTFIDF = torch.utils.data.DataLoader(TestDatasetTFIDF, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customFCNN = CustomFCNN(input_size = X_train.shape[1], \n",
    "                        hidden_size = 2048, \n",
    "                        dropout_rate = 0.5).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customFCNN.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7323 | Accuracy = 53.93% | F1-Score = 55.14% | Batch ID = 8 : 100%|██████████| 8/8 [00:00<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.7479\n",
      "Training Accuracy = 51.77%\n",
      "Training F1-Score = 48.51%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (1024x6719 and 14687x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomFCNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoaderTFIDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTestDataLoaderTFIDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     95\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     99\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mCustomFCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mForward pass of the model.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m- torch.Tensor: Model predictions (logits).\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Pass the input through the fc layers, activation function, and normalization\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply dropout\u001b[39;00m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (1024x6719 and 14687x2048)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customFCNN, \n",
    "                                                       train_loader = TrainDataLoaderTFIDF, \n",
    "                                                       test_loader = TestDataLoaderTFIDF, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM Neural Networks with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SkipGram embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:50.418593600Z",
     "start_time": "2023-12-21T15:34:50.412589100Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = skipgram_model,\n",
    "                                  hidden_size = 64, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 1, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6720 | Accuracy = 60.66% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6842\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7039 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6835\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6641 | Accuracy = 68.85% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6847\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6956 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 101.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6846\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7085 | Accuracy = 44.26% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6957 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6846\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6823 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6843\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7012 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 102.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7235 | Accuracy = 44.26% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6838\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7025 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 102.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6724 | Accuracy = 62.30% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6840\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6978 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7118 | Accuracy = 47.54% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6839\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7017 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 101.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6825 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6834\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6989 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 103.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6834\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = skipgram_model, \n",
    "                                                      hidden_size = 64, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_Attention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6415 | Accuracy = 65.57% | F1-Score = 36.36% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6707\n",
      "Training Accuracy = 57.77%\n",
      "Training F1-Score = 12.28%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6186 | Accuracy = 74.60% | F1-Score = 68.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 94.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6484\n",
      "Test Accuracy = 66.53%\n",
      "Test F1-Score = 45.99%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4540 | Accuracy = 77.05% | F1-Score = 63.16% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5621\n",
      "Training Accuracy = 73.97%\n",
      "Training F1-Score = 64.98%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3753 | Accuracy = 87.30% | F1-Score = 85.71% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5217\n",
      "Test Accuracy = 75.76%\n",
      "Test F1-Score = 66.86%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3810 | Accuracy = 83.61% | F1-Score = 77.27% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4572\n",
      "Training Accuracy = 79.60%\n",
      "Training F1-Score = 74.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3290 | Accuracy = 88.89% | F1-Score = 87.72% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4786\n",
      "Test Accuracy = 78.42%\n",
      "Test F1-Score = 70.37%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4183 | Accuracy = 80.33% | F1-Score = 71.43% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4173\n",
      "Training Accuracy = 81.22%\n",
      "Training F1-Score = 76.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3229 | Accuracy = 88.89% | F1-Score = 87.72% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4681\n",
      "Test Accuracy = 78.95%\n",
      "Test F1-Score = 70.78%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3676 | Accuracy = 86.89% | F1-Score = 85.19% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3897\n",
      "Training Accuracy = 83.00%\n",
      "Training F1-Score = 78.98%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3169 | Accuracy = 88.89% | F1-Score = 88.14% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4555\n",
      "Test Accuracy = 79.07%\n",
      "Test F1-Score = 74.47%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3159 | Accuracy = 86.89% | F1-Score = 84.62% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3627\n",
      "Training Accuracy = 84.09%\n",
      "Training F1-Score = 80.58%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3233 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 100.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4559\n",
      "Test Accuracy = 79.25%\n",
      "Test F1-Score = 73.29%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2938 | Accuracy = 88.52% | F1-Score = 86.79% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3341\n",
      "Training Accuracy = 85.96%\n",
      "Training F1-Score = 82.89%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3359 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4635\n",
      "Test Accuracy = 79.04%\n",
      "Test F1-Score = 73.18%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2427 | Accuracy = 90.16% | F1-Score = 82.35% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3011\n",
      "Training Accuracy = 87.71%\n",
      "Training F1-Score = 85.15%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3676 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4942\n",
      "Test Accuracy = 78.49%\n",
      "Test F1-Score = 71.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_Attention, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "\n",
    "customPreTrainedLSTM_MultiheadAttention = CustomLSTM_MultiHeadAttention(word2vec_model = skipgram_model,\n",
    "                                                                        hidden_size = 64, \n",
    "                                                                        output_size = 1, \n",
    "                                                                        dropout = 0.3,\n",
    "                                                                        num_layers = 1, \n",
    "                                                                        bidirectional = True,\n",
    "                                                                        freeze_embeddings = False,\n",
    "                                                                        num_heads = 8).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_MultiheadAttention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6149 | Accuracy = 68.85% | F1-Score = 61.22% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6246\n",
      "Training Accuracy = 66.96%\n",
      "Training F1-Score = 62.22%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7029 | Accuracy = 63.49% | F1-Score = 41.03% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8672\n",
      "Test Accuracy = 63.44%\n",
      "Test F1-Score = 26.49%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5737 | Accuracy = 73.77% | F1-Score = 71.43% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5122\n",
      "Training Accuracy = 76.33%\n",
      "Training F1-Score = 72.05%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3330 | Accuracy = 88.89% | F1-Score = 87.27% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5044\n",
      "Test Accuracy = 76.65%\n",
      "Test F1-Score = 66.37%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4045 | Accuracy = 85.25% | F1-Score = 81.63% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4626\n",
      "Training Accuracy = 79.35%\n",
      "Training F1-Score = 75.32%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3615 | Accuracy = 84.13% | F1-Score = 84.38% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4867\n",
      "Test Accuracy = 76.89%\n",
      "Test F1-Score = 74.05%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4158 | Accuracy = 81.97% | F1-Score = 78.43% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4210\n",
      "Training Accuracy = 81.32%\n",
      "Training F1-Score = 77.28%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3331 | Accuracy = 84.13% | F1-Score = 83.87% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4645\n",
      "Test Accuracy = 78.27%\n",
      "Test F1-Score = 74.15%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3853 | Accuracy = 85.25% | F1-Score = 82.35% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3805\n",
      "Training Accuracy = 83.46%\n",
      "Training F1-Score = 79.85%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6926 | Accuracy = 63.49% | F1-Score = 71.60% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.9655\n",
      "Test Accuracy = 51.55%\n",
      "Test F1-Score = 63.39%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3032 | Accuracy = 85.25% | F1-Score = 84.21% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3345\n",
      "Training Accuracy = 86.00%\n",
      "Training F1-Score = 83.15%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4451 | Accuracy = 82.54% | F1-Score = 84.06% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6045\n",
      "Test Accuracy = 70.89%\n",
      "Test F1-Score = 71.64%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1269 | Accuracy = 96.72% | F1-Score = 96.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2730\n",
      "Training Accuracy = 88.86%\n",
      "Training F1-Score = 86.72%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5011 | Accuracy = 80.95% | F1-Score = 82.86% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 85.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6368\n",
      "Test Accuracy = 70.82%\n",
      "Test F1-Score = 70.99%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2268 | Accuracy = 88.52% | F1-Score = 86.27% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2248\n",
      "Training Accuracy = 91.33%\n",
      "Training F1-Score = 89.76%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4087 | Accuracy = 90.48% | F1-Score = 89.29% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6556\n",
      "Test Accuracy = 77.75%\n",
      "Test F1-Score = 68.71%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1791 | Accuracy = 95.08% | F1-Score = 93.88% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1788\n",
      "Training Accuracy = 93.58%\n",
      "Training F1-Score = 92.41%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5318 | Accuracy = 80.95% | F1-Score = 82.35% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7262\n",
      "Test Accuracy = 73.06%\n",
      "Test F1-Score = 72.35%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2847 | Accuracy = 86.89% | F1-Score = 82.61% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1438\n",
      "Training Accuracy = 94.97%\n",
      "Training F1-Score = 94.08%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5090 | Accuracy = 80.95% | F1-Score = 81.25% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6731\n",
      "Test Accuracy = 74.84%\n",
      "Test F1-Score = 71.44%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0929 | Accuracy = 95.08% | F1-Score = 94.34% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1193\n",
      "Training Accuracy = 95.78%\n",
      "Training F1-Score = 95.09%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.8476 | Accuracy = 84.13% | F1-Score = 80.77% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 86.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.2917\n",
      "Test Accuracy = 72.26%\n",
      "Test F1-Score = 54.55%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2640 | Accuracy = 88.52% | F1-Score = 87.72% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.23it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1070\n",
      "Training Accuracy = 96.20%\n",
      "Training F1-Score = 95.55%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5682 | Accuracy = 82.54% | F1-Score = 82.54% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7138\n",
      "Test Accuracy = 76.77%\n",
      "Test F1-Score = 71.24%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1847 | Accuracy = 95.08% | F1-Score = 94.74% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0909\n",
      "Training Accuracy = 97.11%\n",
      "Training F1-Score = 96.61%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6824 | Accuracy = 80.95% | F1-Score = 81.25% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8885\n",
      "Test Accuracy = 72.20%\n",
      "Test F1-Score = 71.01%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1179 | Accuracy = 96.72% | F1-Score = 95.65% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0878\n",
      "Training Accuracy = 96.97%\n",
      "Training F1-Score = 96.46%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6021 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 86.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8728\n",
      "Test Accuracy = 76.56%\n",
      "Test F1-Score = 67.76%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0283 | Accuracy = 100.00% | F1-Score = 100.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0807\n",
      "Training Accuracy = 97.43%\n",
      "Training F1-Score = 96.98%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5593 | Accuracy = 85.71% | F1-Score = 85.25% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8447\n",
      "Test Accuracy = 76.52%\n",
      "Test F1-Score = 70.29%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0462 | Accuracy = 98.36% | F1-Score = 97.87% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0765\n",
      "Training Accuracy = 97.18%\n",
      "Training F1-Score = 96.70%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6099 | Accuracy = 82.54% | F1-Score = 82.54% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 86.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8627\n",
      "Test Accuracy = 72.42%\n",
      "Test F1-Score = 70.53%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_MultiheadAttention, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CBOW embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = cbow_model,\n",
    "                                  hidden_size = 64, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 1, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6822 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6833\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7032 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6834\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6831 | Accuracy = 57.38% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6839\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6976 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6837\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6726 | Accuracy = 62.30% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6842\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6974 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6837\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6652 | Accuracy = 63.93% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6838\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7000 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7033 | Accuracy = 50.82% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7037 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6834\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6994 | Accuracy = 50.82% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6839\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6999 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 96.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7199 | Accuracy = 44.26% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6841\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7008 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 100.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6994 | Accuracy = 49.18% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6959 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 102.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6844\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM, \n",
    "                                                       train_loader = TrainDataLoader_CBOW, \n",
    "                                                       test_loader = TestDataLoader_CBOW, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = cbow_model, \n",
    "                                                      hidden_size = 64, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(customPreTrainedLSTM_Attention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6054 | Accuracy = 75.41% | F1-Score = 54.55% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6590\n",
      "Training Accuracy = 60.15%\n",
      "Training F1-Score = 15.58%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5571 | Accuracy = 80.95% | F1-Score = 76.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 88.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6130\n",
      "Test Accuracy = 72.54%\n",
      "Test F1-Score = 56.80%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5145 | Accuracy = 73.77% | F1-Score = 61.90% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5268\n",
      "Training Accuracy = 77.43%\n",
      "Training F1-Score = 69.93%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3611 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5043\n",
      "Test Accuracy = 77.38%\n",
      "Test F1-Score = 71.75%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5660 | Accuracy = 70.49% | F1-Score = 70.97% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4585\n",
      "Training Accuracy = 79.97%\n",
      "Training F1-Score = 75.23%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3298 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 95.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4754\n",
      "Test Accuracy = 78.64%\n",
      "Test F1-Score = 73.28%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3986 | Accuracy = 81.97% | F1-Score = 74.42% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4308\n",
      "Training Accuracy = 81.15%\n",
      "Training F1-Score = 76.77%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3263 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 96.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4616\n",
      "Test Accuracy = 79.16%\n",
      "Test F1-Score = 74.22%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5685 | Accuracy = 72.13% | F1-Score = 65.31% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4135\n",
      "Training Accuracy = 82.19%\n",
      "Training F1-Score = 78.27%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3257 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4550\n",
      "Test Accuracy = 79.77%\n",
      "Test F1-Score = 74.42%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5272 | Accuracy = 78.69% | F1-Score = 75.47% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3975\n",
      "Training Accuracy = 82.85%\n",
      "Training F1-Score = 79.01%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3263 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4486\n",
      "Test Accuracy = 80.05%\n",
      "Test F1-Score = 74.82%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4757 | Accuracy = 80.33% | F1-Score = 73.91% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3814\n",
      "Training Accuracy = 83.59%\n",
      "Training F1-Score = 80.01%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3309 | Accuracy = 88.89% | F1-Score = 88.14% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4444\n",
      "Test Accuracy = 80.05%\n",
      "Test F1-Score = 75.16%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4634 | Accuracy = 77.05% | F1-Score = 75.86% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3657\n",
      "Training Accuracy = 84.43%\n",
      "Training F1-Score = 80.95%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3304 | Accuracy = 88.89% | F1-Score = 88.14% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4418\n",
      "Test Accuracy = 80.17%\n",
      "Test F1-Score = 75.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_Attention, \n",
    "                                                       train_loader = TrainDataLoader_CBOW, \n",
    "                                                       test_loader = TestDataLoader_CBOW, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "\n",
    "customPreTrainedLSTM_MultiheadAttention = CustomLSTM_MultiHeadAttention(word2vec_model = cbow_model,\n",
    "                                                                        hidden_size = 64, \n",
    "                                                                        output_size = 1, \n",
    "                                                                        dropout = 0.2,\n",
    "                                                                        num_layers = 1, \n",
    "                                                                        bidirectional = True,\n",
    "                                                                        freeze_embeddings = False,\n",
    "                                                                        num_heads = 8).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_MultiheadAttention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4239 | Accuracy = 81.97% | F1-Score = 77.55% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5627\n",
      "Training Accuracy = 72.42%\n",
      "Training F1-Score = 68.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 1.2338 | Accuracy = 50.79% | F1-Score = 66.67% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.5029\n",
      "Test Accuracy = 44.62%\n",
      "Test F1-Score = 60.64%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4427 | Accuracy = 75.41% | F1-Score = 66.67% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4917\n",
      "Training Accuracy = 77.41%\n",
      "Training F1-Score = 72.92%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4907 | Accuracy = 79.37% | F1-Score = 73.47% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 89.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6941\n",
      "Test Accuracy = 71.71%\n",
      "Test F1-Score = 52.59%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4230 | Accuracy = 80.33% | F1-Score = 77.78% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4414\n",
      "Training Accuracy = 80.64%\n",
      "Training F1-Score = 76.48%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3217 | Accuracy = 90.48% | F1-Score = 89.29% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 81.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4844\n",
      "Test Accuracy = 77.47%\n",
      "Test F1-Score = 69.00%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3269 | Accuracy = 86.89% | F1-Score = 82.61% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4119\n",
      "Training Accuracy = 81.91%\n",
      "Training F1-Score = 77.87%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3008 | Accuracy = 88.89% | F1-Score = 88.52% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4666\n",
      "Test Accuracy = 78.00%\n",
      "Test F1-Score = 73.87%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2941 | Accuracy = 91.80% | F1-Score = 89.36% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3738\n",
      "Training Accuracy = 83.61%\n",
      "Training F1-Score = 80.09%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5153 | Accuracy = 71.43% | F1-Score = 75.68% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7415\n",
      "Test Accuracy = 62.98%\n",
      "Test F1-Score = 68.16%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3787 | Accuracy = 81.97% | F1-Score = 78.43% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3234\n",
      "Training Accuracy = 86.13%\n",
      "Training F1-Score = 83.28%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3535 | Accuracy = 85.71% | F1-Score = 85.71% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 84.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5324\n",
      "Test Accuracy = 75.33%\n",
      "Test F1-Score = 73.09%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2615 | Accuracy = 91.80% | F1-Score = 90.57% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2733\n",
      "Training Accuracy = 88.80%\n",
      "Training F1-Score = 86.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4514 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 86.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6582\n",
      "Test Accuracy = 76.77%\n",
      "Test F1-Score = 66.52%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2663 | Accuracy = 88.52% | F1-Score = 87.72% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2235\n",
      "Training Accuracy = 91.16%\n",
      "Training F1-Score = 89.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4642 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 85.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6227\n",
      "Test Accuracy = 77.38%\n",
      "Test F1-Score = 70.46%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1271 | Accuracy = 95.08% | F1-Score = 93.62% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.32it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1762\n",
      "Training Accuracy = 93.56%\n",
      "Training F1-Score = 92.40%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4344 | Accuracy = 84.13% | F1-Score = 83.87% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 85.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6592\n",
      "Test Accuracy = 74.35%\n",
      "Test F1-Score = 70.25%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1365 | Accuracy = 93.44% | F1-Score = 91.30% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1483\n",
      "Training Accuracy = 94.79%\n",
      "Training F1-Score = 93.90%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 1.0231 | Accuracy = 80.95% | F1-Score = 76.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 86.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.4916\n",
      "Test Accuracy = 70.73%\n",
      "Test F1-Score = 49.28%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1253 | Accuracy = 95.08% | F1-Score = 94.34% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1282\n",
      "Training Accuracy = 95.60%\n",
      "Training F1-Score = 94.85%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.9901 | Accuracy = 68.25% | F1-Score = 74.36% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 72.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.2781\n",
      "Test Accuracy = 59.36%\n",
      "Test F1-Score = 65.43%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0968 | Accuracy = 95.08% | F1-Score = 94.12% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1189\n",
      "Training Accuracy = 95.84%\n",
      "Training F1-Score = 95.13%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5950 | Accuracy = 85.71% | F1-Score = 86.57% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 91.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.9149\n",
      "Test Accuracy = 68.56%\n",
      "Test F1-Score = 69.13%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0778 | Accuracy = 98.36% | F1-Score = 98.18% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1058\n",
      "Training Accuracy = 96.48%\n",
      "Training F1-Score = 95.88%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6034 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8655\n",
      "Test Accuracy = 76.31%\n",
      "Test F1-Score = 67.75%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1000 | Accuracy = 95.08% | F1-Score = 95.38% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0952\n",
      "Training Accuracy = 97.01%\n",
      "Training F1-Score = 96.51%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4853 | Accuracy = 85.71% | F1-Score = 85.71% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 91.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.8357\n",
      "Test Accuracy = 71.77%\n",
      "Test F1-Score = 70.60%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1496 | Accuracy = 96.72% | F1-Score = 96.30% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.12it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0873\n",
      "Training Accuracy = 97.08%\n",
      "Training F1-Score = 96.59%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4881 | Accuracy = 87.30% | F1-Score = 86.67% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 79.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7957\n",
      "Test Accuracy = 75.48%\n",
      "Test F1-Score = 71.57%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1726 | Accuracy = 93.44% | F1-Score = 90.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 25.06it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0835\n",
      "Training Accuracy = 97.35%\n",
      "Training F1-Score = 96.90%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6835 | Accuracy = 80.95% | F1-Score = 81.82% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.0553\n",
      "Test Accuracy = 68.00%\n",
      "Test F1-Score = 69.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_MultiheadAttention, \n",
    "                                                       train_loader = TrainDataLoader_CBOW, \n",
    "                                                       test_loader = TestDataLoader_CBOW, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GRU Neural Networks with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SkipGram Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 32, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = skipgram_model, \n",
    "                                hidden_size = 64, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 1, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 68.75% | F1-Score = 0.00% | Batch ID = 23 :  10%|▉         | 23/238 [00:05<00:49,  4.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomPreTrainedGRU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTestDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedGRU, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = skipgram_model, \n",
    "                                                    hidden_size = 64, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 1, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 56.25% | F1-Score = 0.00% | Batch ID = 13 :   5%|▌         | 13/238 [00:02<00:51,  4.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomPreTrainedGRU_Attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTestDataLoader_SkipGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 156\u001b[0m, in \u001b[0;36mCustomGRU_Attention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Pass the embeddings through the GRU layer\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Apply the attention mechanism\u001b[39;00m\n\u001b[1;32m    158\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(output), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1102\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1106\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedGRU_Attention, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CBOW Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 32, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = cbow_model, \n",
    "                                hidden_size = 64, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 1, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 50.00% | F1-Score = 0.00% | Batch ID = 17 :   7%|▋         | 17/238 [00:03<00:51,  4.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomPreTrainedGRU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoader_CBOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTestDataLoader_CBOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mCustomGRU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Pass the embeddings through the GRU layer\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Pass the output of the last time step through the fully-connected layer\u001b[39;00m\n\u001b[1;32m     66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Use the last time step's output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1102\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1106\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedGRU, \n",
    "                                                       train_loader = TrainDataLoader_CBOW, \n",
    "                                                       test_loader = TestDataLoader_CBOW, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = cbow_model, \n",
    "                                                    hidden_size = 64, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 1, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = nan | Accuracy = 65.62% | F1-Score = 0.00% | Batch ID = 10 :   4%|▍         | 10/238 [00:02<00:54,  4.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, train_f1s, test_losses, test_f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomPreTrainedGRU_Attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTrainDataLoader_CBOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTestDataLoader_CBOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedGRU_Attention, \n",
    "                                                       train_loader = TrainDataLoader_CBOW, \n",
    "                                                       test_loader = TestDataLoader_CBOW, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 8, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Encoder Transformers with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object for the training data\n",
    "TrainDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_train, 'skipgram', attn_masks_train)\n",
    "# Create a torch Dataset object for the test data\n",
    "TestDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_test, 'skipgram', attn_masks_test)\n",
    "\n",
    "# Create a torch DataLoader object for the training data\n",
    "TrainDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TrainDatasetEncoder, batch_size = 32, shuffle = True)\n",
    "# Create a torch DataLoader object for the test data\n",
    "TestDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TestDatasetEncoder, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "TransformerEncoder = EncoderTransformer(word2vec_model = skipgram_model,\n",
    "                                        d_model = 512,\n",
    "                                        num_heads = 16,\n",
    "                                        num_layers = 8,\n",
    "                                        d_ff = 2048,\n",
    "                                        max_seq_length = 50,\n",
    "                                        dropout = 0.1,\n",
    "                                        freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(TransformerEncoder.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoaderEncoder) * 8), \n",
    "                                            num_training_steps = len(TrainDataLoaderEncoder) * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6723 | Accuracy = 62.07% | F1-Score = 52.17% | Batch ID = 238 : 100%|██████████| 238/238 [00:29<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6700\n",
      "Training Accuracy = 60.06%\n",
      "Training F1-Score = 55.66%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4996 | Accuracy = 70.97% | F1-Score = 83.02% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 26.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.0354\n",
      "Test Accuracy = 43.37%\n",
      "Test F1-Score = 60.19%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6927 | Accuracy = 68.97% | F1-Score = 70.97% | Batch ID = 238 : 100%|██████████| 238/238 [00:29<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5960\n",
      "Training Accuracy = 71.93%\n",
      "Training F1-Score = 67.70%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4759 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6972\n",
      "Test Accuracy = 72.30%\n",
      "Test F1-Score = 53.88%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4800 | Accuracy = 79.31% | F1-Score = 72.73% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5460\n",
      "Training Accuracy = 74.40%\n",
      "Training F1-Score = 69.80%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3244 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5110\n",
      "Test Accuracy = 75.33%\n",
      "Test F1-Score = 68.22%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4166 | Accuracy = 72.41% | F1-Score = 69.23% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4937\n",
      "Training Accuracy = 77.09%\n",
      "Training F1-Score = 72.38%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3454 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5032\n",
      "Test Accuracy = 76.37%\n",
      "Test F1-Score = 66.67%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4119 | Accuracy = 79.31% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4712\n",
      "Training Accuracy = 78.38%\n",
      "Training F1-Score = 73.89%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3795 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6178\n",
      "Test Accuracy = 69.38%\n",
      "Test F1-Score = 64.59%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4819 | Accuracy = 79.31% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4644\n",
      "Training Accuracy = 78.52%\n",
      "Training F1-Score = 73.56%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5201 | Accuracy = 77.42% | F1-Score = 81.08% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6659\n",
      "Test Accuracy = 71.47%\n",
      "Test F1-Score = 51.33%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3653 | Accuracy = 79.31% | F1-Score = 70.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4467\n",
      "Training Accuracy = 79.50%\n",
      "Training F1-Score = 74.79%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3657 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5038\n",
      "Test Accuracy = 76.92%\n",
      "Test F1-Score = 66.25%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5422 | Accuracy = 79.31% | F1-Score = 76.92% | Batch ID = 238 : 100%|██████████| 238/238 [00:28<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4405\n",
      "Training Accuracy = 80.02%\n",
      "Training F1-Score = 75.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3604 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:03<00:00, 27.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4975\n",
      "Test Accuracy = 76.86%\n",
      "Test F1-Score = 66.04%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_EncoderTransformer(model = TransformerEncoder, \n",
    "                                                                          train_loader = TrainDataLoaderEncoder, \n",
    "                                                                          test_loader = TestDataLoaderEncoder, \n",
    "                                                                          optimizer = optimizer, \n",
    "                                                                          scheduler = scheduler,\n",
    "                                                                          loss_func = criterion, \n",
    "                                                                          epochs = 8, \n",
    "                                                                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object for the training data\n",
    "TrainDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_train, 'cbow', attn_masks_train)\n",
    "# Create a torch Dataset object for the test data\n",
    "TestDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_test, 'cbow', attn_masks_test)\n",
    "\n",
    "# Create a torch DataLoader object for the training data\n",
    "TrainDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TrainDatasetEncoder, batch_size = 32, shuffle = True)\n",
    "# Create a torch DataLoader object for the test data\n",
    "TestDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TestDatasetEncoder, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "TransformerEncoder = EncoderTransformer(word2vec_model = cbow_model,\n",
    "                                        d_model = 512,\n",
    "                                        num_heads = 16,\n",
    "                                        num_layers = 16,\n",
    "                                        d_ff = 2048,\n",
    "                                        max_seq_length = 50,\n",
    "                                        dropout = 0.1,\n",
    "                                        freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(TransformerEncoder.parameters(), lr = 5e-5, weight_decay = 0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoaderEncoder) * 8), \n",
    "                                            num_training_steps = len(TrainDataLoaderEncoder) * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.8696 | Accuracy = 48.28% | F1-Score = 34.78% | Batch ID = 238 : 100%|██████████| 238/238 [00:57<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.7284\n",
      "Training Accuracy = 52.48%\n",
      "Training F1-Score = 47.61%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 9.9656 | Accuracy = 29.03% | F1-Score = 0.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 6.0503\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6217 | Accuracy = 65.52% | F1-Score = 54.55% | Batch ID = 238 : 100%|██████████| 238/238 [00:57<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6242\n",
      "Training Accuracy = 66.66%\n",
      "Training F1-Score = 60.93%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4517 | Accuracy = 74.19% | F1-Score = 84.62% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.9205\n",
      "Test Accuracy = 48.82%\n",
      "Test F1-Score = 62.18%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4241 | Accuracy = 75.86% | F1-Score = 66.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:57<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5402\n",
      "Training Accuracy = 74.81%\n",
      "Training F1-Score = 69.99%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2993 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4971\n",
      "Test Accuracy = 76.89%\n",
      "Test F1-Score = 70.20%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5104 | Accuracy = 72.41% | F1-Score = 71.43% | Batch ID = 238 : 100%|██████████| 238/238 [00:57<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5153\n",
      "Training Accuracy = 76.82%\n",
      "Training F1-Score = 71.89%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4134 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5370\n",
      "Test Accuracy = 75.30%\n",
      "Test F1-Score = 62.65%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4642 | Accuracy = 79.31% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:56<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4903\n",
      "Training Accuracy = 77.55%\n",
      "Training F1-Score = 72.34%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3156 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4826\n",
      "Test Accuracy = 77.63%\n",
      "Test F1-Score = 69.88%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5154 | Accuracy = 72.41% | F1-Score = 55.56% | Batch ID = 238 : 100%|██████████| 238/238 [00:57<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4714\n",
      "Training Accuracy = 78.35%\n",
      "Training F1-Score = 73.32%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3153 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4777\n",
      "Test Accuracy = 78.03%\n",
      "Test F1-Score = 71.89%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4619 | Accuracy = 75.86% | F1-Score = 74.07% | Batch ID = 238 : 100%|██████████| 238/238 [00:56<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4499\n",
      "Training Accuracy = 79.52%\n",
      "Training F1-Score = 74.74%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2981 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4716\n",
      "Test Accuracy = 77.72%\n",
      "Test F1-Score = 71.72%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5862 | Accuracy = 72.41% | F1-Score = 69.23% | Batch ID = 238 : 100%|██████████| 238/238 [00:56<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4392\n",
      "Training Accuracy = 80.11%\n",
      "Training F1-Score = 75.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2976 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4679\n",
      "Test Accuracy = 78.24%\n",
      "Test F1-Score = 72.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_EncoderTransformer(model = TransformerEncoder, \n",
    "                                                                          train_loader = TrainDataLoaderEncoder, \n",
    "                                                                          test_loader = TestDataLoaderEncoder, \n",
    "                                                                          optimizer = optimizer, \n",
    "                                                                          scheduler = scheduler,\n",
    "                                                                          loss_func = criterion, \n",
    "                                                                          epochs = 8, \n",
    "                                                                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Encoder Transformers **implemented from scratch** with Custom Pre-Trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object for the training data\n",
    "TrainDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_train, 'skipgram', attn_masks_train)\n",
    "# Create a torch Dataset object for the test data\n",
    "TestDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_test, 'skipgram', attn_masks_test)\n",
    "\n",
    "# Create a torch DataLoader object for the training data\n",
    "TrainDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TrainDatasetEncoder, batch_size = 32, shuffle = True)\n",
    "# Create a torch DataLoader object for the test data\n",
    "TestDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TestDatasetEncoder, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "TransformerEncoder_scratch = EncoderTransformer_scratch(word2vec_model = skipgram_model,\n",
    "                                                        d_model = 512,\n",
    "                                                        num_heads = 16,\n",
    "                                                        num_layers = 16,\n",
    "                                                        d_ff = 2048,\n",
    "                                                        max_seq_length = 50,\n",
    "                                                        dropout = 0.1,\n",
    "                                                        freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(TransformerEncoder_scratch.parameters(), lr = 5e-5, weight_decay = 0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoaderEncoder) * 8), \n",
    "                                            num_training_steps = len(TrainDataLoaderEncoder) * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4404 | Accuracy = 79.31% | F1-Score = 70.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5797\n",
      "Training Accuracy = 69.97%\n",
      "Training F1-Score = 65.63%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3647 | Accuracy = 83.87% | F1-Score = 87.18% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5492\n",
      "Test Accuracy = 71.71%\n",
      "Test F1-Score = 53.45%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7526 | Accuracy = 65.52% | F1-Score = 58.33% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5051\n",
      "Training Accuracy = 76.34%\n",
      "Training F1-Score = 71.72%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2868 | Accuracy = 83.87% | F1-Score = 88.89% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5349\n",
      "Test Accuracy = 72.54%\n",
      "Test F1-Score = 71.89%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5295 | Accuracy = 79.31% | F1-Score = 75.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4676\n",
      "Training Accuracy = 78.33%\n",
      "Training F1-Score = 73.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3672 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5079\n",
      "Test Accuracy = 76.19%\n",
      "Test F1-Score = 64.08%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3825 | Accuracy = 86.21% | F1-Score = 86.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4486\n",
      "Training Accuracy = 79.80%\n",
      "Training F1-Score = 74.89%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3791 | Accuracy = 80.65% | F1-Score = 86.36% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7059\n",
      "Test Accuracy = 70.03%\n",
      "Test F1-Score = 70.22%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4151 | Accuracy = 82.76% | F1-Score = 78.26% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4390\n",
      "Training Accuracy = 80.27%\n",
      "Training F1-Score = 75.47%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4275 | Accuracy = 83.87% | F1-Score = 87.18% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5389\n",
      "Test Accuracy = 73.77%\n",
      "Test F1-Score = 57.41%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4297 | Accuracy = 82.76% | F1-Score = 73.68% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4269\n",
      "Training Accuracy = 81.40%\n",
      "Training F1-Score = 76.77%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3244 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4735\n",
      "Test Accuracy = 77.63%\n",
      "Test F1-Score = 68.43%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6245 | Accuracy = 68.97% | F1-Score = 68.97% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4128\n",
      "Training Accuracy = 82.08%\n",
      "Training F1-Score = 77.85%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2981 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4584\n",
      "Test Accuracy = 78.85%\n",
      "Test F1-Score = 72.73%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4589 | Accuracy = 75.86% | F1-Score = 72.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3971\n",
      "Training Accuracy = 82.44%\n",
      "Training F1-Score = 78.21%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3373 | Accuracy = 87.10% | F1-Score = 90.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4705\n",
      "Test Accuracy = 78.09%\n",
      "Test F1-Score = 68.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_EncoderTransformer(model = TransformerEncoder_scratch, \n",
    "                                                                          train_loader = TrainDataLoaderEncoder, \n",
    "                                                                          test_loader = TestDataLoaderEncoder, \n",
    "                                                                          optimizer = optimizer, \n",
    "                                                                          scheduler = scheduler,\n",
    "                                                                          loss_func = criterion, \n",
    "                                                                          epochs = 8, \n",
    "                                                                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object for the training data\n",
    "TrainDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_train, 'cbow', attn_masks_train)\n",
    "# Create a torch Dataset object for the test data\n",
    "TestDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_test, 'cbow', attn_masks_test)\n",
    "\n",
    "# Create a torch DataLoader object for the training data\n",
    "TrainDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TrainDatasetEncoder, batch_size = 32, shuffle = True)\n",
    "# Create a torch DataLoader object for the test data\n",
    "TestDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TestDatasetEncoder, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "TransformerEncoder_scratch = EncoderTransformer_scratch(word2vec_model = cbow_model,\n",
    "                                                        d_model = 512,\n",
    "                                                        num_heads = 16,\n",
    "                                                        num_layers = 16,\n",
    "                                                        d_ff = 2048,\n",
    "                                                        max_seq_length = 50,\n",
    "                                                        dropout = 0.1,\n",
    "                                                        freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(TransformerEncoder_scratch.parameters(), lr = 5e-5, weight_decay = 0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoaderEncoder) * 8), \n",
    "                                            num_training_steps = len(TrainDataLoaderEncoder) * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3563 | Accuracy = 86.21% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5481\n",
      "Training Accuracy = 73.40%\n",
      "Training F1-Score = 68.84%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6492 | Accuracy = 70.97% | F1-Score = 74.29% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7271\n",
      "Test Accuracy = 70.98%\n",
      "Test F1-Score = 50.44%\n",
      "\n",
      "Epoch 2/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3492 | Accuracy = 89.66% | F1-Score = 85.71% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4802\n",
      "Training Accuracy = 78.06%\n",
      "Training F1-Score = 72.38%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3216 | Accuracy = 77.42% | F1-Score = 84.44% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6176\n",
      "Test Accuracy = 72.30%\n",
      "Test F1-Score = 71.96%\n",
      "\n",
      "Epoch 3/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6000 | Accuracy = 75.86% | F1-Score = 58.82% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4640\n",
      "Training Accuracy = 78.89%\n",
      "Training F1-Score = 73.59%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4147 | Accuracy = 83.87% | F1-Score = 87.18% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5233\n",
      "Test Accuracy = 75.45%\n",
      "Test F1-Score = 62.38%\n",
      "\n",
      "Epoch 4/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3624 | Accuracy = 86.21% | F1-Score = 83.33% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4576\n",
      "Training Accuracy = 79.29%\n",
      "Training F1-Score = 74.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6003 | Accuracy = 80.65% | F1-Score = 84.21% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6954\n",
      "Test Accuracy = 73.15%\n",
      "Test F1-Score = 56.11%\n",
      "\n",
      "Epoch 5/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4698 | Accuracy = 72.41% | F1-Score = 66.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4410\n",
      "Training Accuracy = 80.17%\n",
      "Training F1-Score = 75.39%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3696 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4653\n",
      "Test Accuracy = 79.19%\n",
      "Test F1-Score = 72.02%\n",
      "\n",
      "Epoch 6/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3099 | Accuracy = 86.21% | F1-Score = 85.71% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4239\n",
      "Training Accuracy = 80.86%\n",
      "Training F1-Score = 76.17%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3756 | Accuracy = 83.87% | F1-Score = 88.37% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4664\n",
      "Test Accuracy = 78.64%\n",
      "Test F1-Score = 74.74%\n",
      "\n",
      "Epoch 7/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4223 | Accuracy = 86.21% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4090\n",
      "Training Accuracy = 81.77%\n",
      "Training F1-Score = 77.25%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3748 | Accuracy = 83.87% | F1-Score = 88.37% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4641\n",
      "Test Accuracy = 79.13%\n",
      "Test F1-Score = 73.53%\n",
      "\n",
      "Epoch 8/8\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4585 | Accuracy = 75.86% | F1-Score = 69.57% | Batch ID = 238 : 100%|██████████| 238/238 [00:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3932\n",
      "Training Accuracy = 82.94%\n",
      "Training F1-Score = 78.94%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3928 | Accuracy = 83.87% | F1-Score = 88.37% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 18.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4550\n",
      "Test Accuracy = 78.85%\n",
      "Test F1-Score = 73.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_EncoderTransformer(model = TransformerEncoder_scratch, \n",
    "                                                                          train_loader = TrainDataLoaderEncoder, \n",
    "                                                                          test_loader = TestDataLoaderEncoder, \n",
    "                                                                          optimizer = optimizer, \n",
    "                                                                          scheduler = scheduler,\n",
    "                                                                          loss_func = criterion, \n",
    "                                                                          epochs = 8, \n",
    "                                                                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "DistilBERTbase_name = \"distilbert-base-uncased\"\n",
    "DistilBERTbase_tokenizer = DistilBertTokenizer.from_pretrained(DistilBERTbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = DistilBERTbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = DistilBERTbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "DistilBERTbase = DistilBertForSequenceClassification.from_pretrained(DistilBERTbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "DistilBERTbase = DistilBERTbase.to(device)\n",
    "optimizer = torch.optim.AdamW(DistilBERTbase.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3902 | Accuracy = 86.21% | F1-Score = 81.82% | Batch ID = 238 : 100%|██████████| 238/238 [00:23<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4746\n",
      "Training Accuracy = 78.26%\n",
      "Training F1-Score = 71.19%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3257 | Accuracy = 83.87% | F1-Score = 88.37% | Batch ID = 102 : 100%|██████████| 102/102 [00:02<00:00, 34.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4060\n",
      "Test Accuracy = 82.53%\n",
      "Test F1-Score = 77.72%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3391 | Accuracy = 89.66% | F1-Score = 88.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:23<00:00, 10.23it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3237\n",
      "Training Accuracy = 87.04%\n",
      "Training F1-Score = 83.80%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2873 | Accuracy = 90.32% | F1-Score = 92.68% | Batch ID = 102 : 100%|██████████| 102/102 [00:02<00:00, 34.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4070\n",
      "Test Accuracy = 82.90%\n",
      "Test F1-Score = 78.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(model = DistilBERTbase, \n",
    "                                                            train_loader = TrainDataLoader, \n",
    "                                                            test_loader = TestDataLoader, \n",
    "                                                            optimizer = optimizer, \n",
    "                                                            scheduler = scheduler, \n",
    "                                                            epochs = 2, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT base (110M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "BERTbase_name = \"bert-base-uncased\"\n",
    "BERTbase_tokenizer = BertTokenizer.from_pretrained(BERTbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = BERTbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = BERTbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "BERTbase = BertForSequenceClassification.from_pretrained(BERTbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTbase = BERTbase.to(device)\n",
    "optimizer = torch.optim.AdamW(BERTbase.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4286 | Accuracy = 79.31% | F1-Score = 76.92% | Batch ID = 238 : 100%|██████████| 238/238 [00:49<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4677\n",
      "Training Accuracy = 78.37%\n",
      "Training F1-Score = 72.26%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3322 | Accuracy = 90.32% | F1-Score = 93.02% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4144\n",
      "Test Accuracy = 82.47%\n",
      "Test F1-Score = 78.58%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3629 | Accuracy = 86.21% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [00:49<00:00,  4.81it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3259\n",
      "Training Accuracy = 87.05%\n",
      "Training F1-Score = 83.85%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3521 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4087\n",
      "Test Accuracy = 82.68%\n",
      "Test F1-Score = 78.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(model = BERTbase, \n",
    "                                                            train_loader = TrainDataLoader, \n",
    "                                                            test_loader = TestDataLoader, \n",
    "                                                            optimizer = optimizer, \n",
    "                                                            scheduler = scheduler, \n",
    "                                                            epochs = 2, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT large (340M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "BERTlarge_name = \"bert-large-uncased\"\n",
    "BERTlarge_tokenizer = BertTokenizer.from_pretrained(BERTlarge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = BERTlarge_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = BERTlarge_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "BERTlarge = BertForSequenceClassification.from_pretrained(BERTlarge_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTlarge = BERTlarge.to(device)\n",
    "optimizer = torch.optim.AdamW(BERTlarge.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6820 | Accuracy = 55.17% | F1-Score = 38.10% | Batch ID = 238 : 100%|██████████| 238/238 [02:16<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6773\n",
      "Training Accuracy = 57.23%\n",
      "Training F1-Score = 38.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7361 | Accuracy = 29.03% | F1-Score = 0.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:14<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6844\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6513 | Accuracy = 68.97% | F1-Score = 40.00% | Batch ID = 238 : 100%|██████████| 238/238 [02:15<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6926\n",
      "Training Accuracy = 55.12%\n",
      "Training F1-Score = 25.02%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7774 | Accuracy = 29.03% | F1-Score = 0.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:14<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6836\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(model = BERTlarge, \n",
    "                                                            train_loader = TrainDataLoader, \n",
    "                                                            test_loader = TestDataLoader, \n",
    "                                                            optimizer = optimizer, \n",
    "                                                            scheduler = scheduler, \n",
    "                                                            epochs = 2, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "ROBERTAbase_name = \"roberta-base\"\n",
    "ROBERTAbase_tokenizer = RobertaTokenizer.from_pretrained(ROBERTAbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = ROBERTAbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = ROBERTAbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "ROBERTAbase = RobertaForSequenceClassification.from_pretrained(ROBERTAbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "ROBERTAbase = ROBERTAbase.to(device)\n",
    "optimizer = torch.optim.AdamW(ROBERTAbase.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5005 | Accuracy = 68.97% | F1-Score = 57.14% | Batch ID = 238 : 100%|██████████| 238/238 [00:51<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4979\n",
      "Training Accuracy = 76.42%\n",
      "Training F1-Score = 70.02%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3027 | Accuracy = 80.65% | F1-Score = 86.36% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4584\n",
      "Test Accuracy = 78.79%\n",
      "Test F1-Score = 76.99%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3246 | Accuracy = 86.21% | F1-Score = 86.67% | Batch ID = 238 : 100%|██████████| 238/238 [00:50<00:00,  4.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3696\n",
      "Training Accuracy = 84.88%\n",
      "Training F1-Score = 81.16%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3428 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:05<00:00, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4399\n",
      "Test Accuracy = 80.82%\n",
      "Test F1-Score = 77.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(model = ROBERTAbase, \n",
    "                                                            train_loader = TrainDataLoader, \n",
    "                                                            test_loader = TestDataLoader, \n",
    "                                                            optimizer = optimizer, \n",
    "                                                            scheduler = scheduler, \n",
    "                                                            epochs = 2, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roBERTa Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "ROBERTAlarge_name = \"roberta-large\"\n",
    "ROBERTAlarge_tokenizer = RobertaTokenizer.from_pretrained(ROBERTAlarge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = ROBERTAlarge_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = ROBERTAlarge_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "ROBERTAlarge = RobertaForSequenceClassification.from_pretrained(ROBERTAlarge_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "ROBERTAlarge = ROBERTAlarge.to(device)\n",
    "optimizer = torch.optim.AdamW(ROBERTAlarge.parameters(), lr = 5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4995 | Accuracy = 79.31% | F1-Score = 62.50% | Batch ID = 238 : 100%|██████████| 238/238 [02:16<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6575\n",
      "Training Accuracy = 61.25%\n",
      "Training F1-Score = 46.13%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6020 | Accuracy = 67.74% | F1-Score = 70.59% | Batch ID = 102 : 100%|██████████| 102/102 [00:15<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6239\n",
      "Test Accuracy = 66.99%\n",
      "Test F1-Score = 39.19%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6779 | Accuracy = 62.07% | F1-Score = 15.38% | Batch ID = 238 : 100%|██████████| 238/238 [02:16<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6864\n",
      "Training Accuracy = 56.34%\n",
      "Training F1-Score = 11.74%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7912 | Accuracy = 29.03% | F1-Score = 0.00% | Batch ID = 102 : 100%|██████████| 102/102 [00:15<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6844\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(model = ROBERTAlarge, \n",
    "                                                            train_loader = TrainDataLoader, \n",
    "                                                            test_loader = TestDataLoader, \n",
    "                                                            optimizer = optimizer, \n",
    "                                                            scheduler = scheduler, \n",
    "                                                            epochs = 2, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTweet base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "BERTweetbase_name = \"vinai/bertweet-base\"\n",
    "BERTweetbase_tokenizer = AutoTokenizer.from_pretrained(BERTweetbase_name, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERTweet(tokenizer = BERTweetbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERTweet(tokenizer = BERTweetbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the model and adjust if for text classification (binary)\n",
    "BERTweetbase = AutoModel.from_pretrained(BERTweetbase_name)\n",
    "BERTweetbase = BERTweetForSequenceClassification(bertweet_model = BERTweetbase, hidden_size = 768, output_size = 1, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTweetbase = BERTweetbase.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(BERTweetbase.parameters(), lr = 5e-5, weight_decay = 0.05)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5318 | Accuracy = 79.31% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [01:08<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4546\n",
      "Training Accuracy = 79.59%\n",
      "Training F1-Score = 75.58%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3595 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:07<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.3747\n",
      "Test Accuracy = 83.57%\n",
      "Test F1-Score = 78.81%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2718 | Accuracy = 89.66% | F1-Score = 84.21% | Batch ID = 238 : 100%|██████████| 238/238 [01:06<00:00,  3.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3165\n",
      "Training Accuracy = 87.92%\n",
      "Training F1-Score = 85.12%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4641 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4088\n",
      "Test Accuracy = 84.00%\n",
      "Test F1-Score = 80.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERTweet(model = BERTweetbase, \n",
    "                                                                train_loader = TrainDataLoader, \n",
    "                                                                test_loader = TestDataLoader, \n",
    "                                                                optimizer = optimizer, \n",
    "                                                                scheduler = scheduler,\n",
    "                                                                loss_func = criterion, \n",
    "                                                                epochs = 2, \n",
    "                                                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTweet Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model's tokenizer\n",
    "BERTweetlarge_name = \"vinai/bertweet-large\"\n",
    "BERTweetlarge_tokenizer = AutoTokenizer.from_pretrained(BERTweetlarge_name, use_fast = False)\n",
    "\n",
    "# Initialize base BERTweet tokenizer to normalize tweets (large version doesn't do it)\n",
    "BERTweetbase_name = \"vinai/bertweet-base\"\n",
    "BERTweetbase_tokenizer = AutoTokenizer.from_pretrained(BERTweetbase_name, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset and DataLoader object for the training data\n",
    "TrainDataset, TrainDataLoader = TokenizeBERTweet(tokenizer = BERTweetlarge_tokenizer, \n",
    "                                                 df = tweets_train, \n",
    "                                                 batch_size = 32, \n",
    "                                                 shuffle = True, \n",
    "                                                 tokenizer_normalizeTweet = BERTweetbase_tokenizer)\n",
    "\n",
    "# Create a torch Dataset and DataLoader object for the test data\n",
    "TestDataset, TestDataLoader = TokenizeBERTweet(tokenizer = BERTweetlarge_tokenizer, \n",
    "                                               df = tweets_test, \n",
    "                                               batch_size = 32, \n",
    "                                               shuffle = False, \n",
    "                                               tokenizer_normalizeTweet = BERTweetbase_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and adjust if for text classification (binary)\n",
    "BERTweetlarge = AutoModel.from_pretrained(BERTweetlarge_name)\n",
    "BERTweetlarge = BERTweetForSequenceClassification(bertweet_model = BERTweetlarge, hidden_size = 1024, output_size = 1, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTweetlarge = BERTweetlarge.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(BERTweetlarge.parameters(), lr = 5e-5, weight_decay = 0.05)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5674 | Accuracy = 72.41% | F1-Score = 63.64% | Batch ID = 238 : 100%|██████████| 238/238 [03:35<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5324\n",
      "Training Accuracy = 74.24%\n",
      "Training F1-Score = 69.02%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3802 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:24<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4348\n",
      "Test Accuracy = 82.62%\n",
      "Test F1-Score = 79.84%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5191 | Accuracy = 79.31% | F1-Score = 70.00% | Batch ID = 238 : 100%|██████████| 238/238 [03:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3744\n",
      "Training Accuracy = 84.68%\n",
      "Training F1-Score = 80.79%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4215 | Accuracy = 83.87% | F1-Score = 87.80% | Batch ID = 102 : 100%|██████████| 102/102 [00:24<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4070\n",
      "Test Accuracy = 84.00%\n",
      "Test F1-Score = 80.68%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_losses, train_f1s, test_losses, test_f1s = train_BERTweet(model = BERTweetlarge, \n",
    "                                                                train_loader = TrainDataLoader, \n",
    "                                                                test_loader = TestDataLoader, \n",
    "                                                                optimizer = optimizer, \n",
    "                                                                scheduler = scheduler, \n",
    "                                                                loss_func = criterion, \n",
    "                                                                epochs = 2, \n",
    "                                                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model obtained\n",
    "BERTweetbase_8115f1 = torch.load('/Users/luish/Desktop/BERTweetbase_0.8115f1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:09<00:00, 10.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_true, y_hat = predict(BERTweetbase_8115f1, TestDataLoader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "╒═══════════════════════╤════════════════════════════╤══════════════════════╕\n",
      "│                       │  Predicted Not a Disaster  │  Predicted Disaster  │\n",
      "╞═══════════════════════╪════════════════════════════╪══════════════════════╡\n",
      "│ Actual Not a Disaster │            1665            │         311          │\n",
      "├───────────────────────┼────────────────────────────┼──────────────────────┤\n",
      "│    Actual Disaster    │            196             │         1091         │\n",
      "╘═══════════════════════╧════════════════════════════╧══════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Compute the confusion matrix\n",
    "ComputeConfusionMatrix(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════╤═════════╕\n",
      "│       Metric       │  Value  │\n",
      "╞════════════════════╪═════════╡\n",
      "│      Accuracy      │ 84.46%  │\n",
      "├────────────────────┼─────────┤\n",
      "│ Weighted Precision │ 77.82%  │\n",
      "├────────────────────┼─────────┤\n",
      "│  Weighted Recall   │ 84.77%  │\n",
      "├────────────────────┼─────────┤\n",
      "│ Weighted F1 Score  │ 81.15%  │\n",
      "╘════════════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "# Compute the classification metrics\n",
    "ComputeClassificationMetrics(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability (BERTweet Large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAAKVCAYAAADhiZc6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdjklEQVR4nO3de1iUdf7/8dcIMiDo4CFRtoQURVHMA54PWJ5abdXWLTtZbmqW5mHNMtYOZiVt5mpa6G6ri5WaWet2WLa0A+apVBTNVDSztQNmmkpqjiCf3x/+nK8THgZGvWduno/ruq8Lbu65533fyMib13vu22GMMQIAAAAAwCYqWF0AAAAAAAAXE40uAAAAAMBWaHQBAAAAALZCowsAAAAAsBUaXQAAAACArdDoAgAAAABshUYXAAAAAGArNLoAAAAAAFuh0QUAAAAA2AqNLmATjRs3VkZGhtVllHt8H86NcwNYg589AOWRwxhjrC4CgP/+97//KTo6Wi6Xy+pSyjW+D+fGuQGswc8egPKIRBflksPh0L///W+ry7io4uLizvpLzJnH+vXXX8vhcCg3N9fn/WZmZio6OrrU9ZTluezgXN8HcG5QNtnZ2XI4HDp06JDVpQQtfvbK5tf//02cOFHNmjU772MGDRqkfv36XdK6LgdjjO655x5Vq1ZNDodD0dHRGjNmjNVlAaVCo2sTq1evVkhIiK6//nqrS8FltHfvXo0ePVoJCQkKDw9XTEyMOnbsqNmzZ+vYsWMltr/qqquUn5+vJk2aXNQ6zvYf+6V6rkBU2u9DeVLac7N48WK1b99ekrRq1SrVrVv3cpdsqX379mnYsGGqU6eOnE6natWqpZ49e2rNmjVWl4Ygw8+e/wYMGKAdO3ZYXYYl3nvvPWVmZurdd99Vfn6+duzYoSeffNLqsoBSCbW6AFwcc+fO1ciRI/WPf/xDe/bsUZ06dawuCZfYV199pQ4dOig6OlqTJ09WcnKyioqKtGPHDs2dO1exsbHq06eP12NCQkJUq1aty1Lf5XwuK5Xl+1BelOXcrFmzRh06dJAkrVy50vNxedG/f38VFhZq3rx5qlu3rn744Qd9+OGH+umnn6wuDUGEn72LIyIiQhEREVaXYYldu3apdu3anj9+XMiJEycUFhZ2iasCSskg6B05csRUrlzZbN++3QwYMMA88cQTnq99/PHHRpJ59913TdOmTY3T6TStW7c2mzdvtrDiCzt+/LgZOXKkueKKK4zT6TQdOnQwa9eu9Xx9y5YtplevXqZy5comKirKdOzY0Xz55ZfGGGPWrl1runXrZqpXr26qVKliOnfubHJycrz2L8ksWbLEGGPMtddea0aMGOH19f3795uwsDDz4YcfGmOMiYuLM5MmTTK33nqriYyMNLVr1zYzZszweszjjz9urrrqKhMWFmZq165tRo4c6fma2+02Dz74oImNjTWVKlUyrVu3Nh9//LFf56hnz57myiuvNEeOHDnr14uLi0sc6+7du40ks3HjRs92b731lklISDDh4eGmS5cuJjMz00gyBw8eNMYY889//tO4XC7z3nvvmYYNG5rIyEjTs2dP8/3333uOW5LX8vHHH5d4rtP/Fj/44APTsmVLExERYdq1a2e2b9/uVfeTTz5prrjiChMVFWUGDx5sxo8fb6655hq/ztWl5Ov3oTwqy7lp06aN59/rDTfcYGbNmnUpSwwoBw8eNJJMdnb2ObeRZDIyMsz1119vwsPDTXx8vHn99dcvY5WXxvle80+/dpx+TTp27Jjp1auXadOmjTlw4IDZv3+/ueWWW8xvfvMbExERYZo0aWIWLFjgtf/U1FQzcuRI8+CDD5qqVauamJgY8/jjj3ttc+jQITN06FBzxRVXmMqVK5trr73W5ObmXo7Dv+j42Tu3t99+27hcLnPy5EljjDEbN240ksy4ceM829xzzz3mlltu8fz/d9rjjz/u9f9RUVGR+dOf/mRcLpepVq2aefDBB82dd95p+vbte5mO5tK46667vP5Pj4uLM6mpqWb06NGebeLi4syTTz5p7rrrLlOlShVz5513GmOMWbVqlenUqZMJDw83V155pRk5cqTXv8MXX3zRJCQkGKfTaWrWrGn69+9/uQ8P5QiNrg3MmTPHpKSkGGOMeeedd0x8fLznP7HTvyA0atTILF261GzevNnccMMNJj4+3pw4ccLKss9r1KhRJjY21mRlZZkvvvjC3HXXXaZq1armwIED5ttvvzXVqlUzv//97826detMXl6emTt3rqdh+vDDD80rr7xitm7darZu3WoGDx5sYmJiTEFBgWf/ZzZ/8+fPN1WrVjXHjx/3fP3555/3Oo9xcXGmcuXKJj093eTl5ZkZM2aYkJAQs3TpUmOMMYsXLzZVqlQxWVlZ5n//+5/57LPPzN///nfP/m677TbTvn1788knn5gvv/zSTJkyxTidTrNjx44ynZ/9+/cbh8Nh0tPTL7jt+Rrd3bt3m4oVK5px48aZ7du3m4ULF5rf/OY3JRrdihUrmm7dupl169aZnJwc06hRI3PbbbcZY4z5+eefzc0332yuv/56k5+fb/Lz843b7T5no9umTRuTnZ1tvvjiC9OpUyfTvn17T62vvvqqCQ8PN3PnzjV5eXnmiSeeMFWqVAnYRrc034fypjTnZv78+cblchmXy2UcDoeJiooyLpfLVKhQwURGRhqXy2Xmz59/Gaq2VmFhoYmKijJjxozxej06kyRTvXp189JLL5m8vDzzyCOPmJCQELN169bLXO3Fdb7X/DMb3UOHDpmOHTuabt26eX55/vbbb82UKVPMxo0bza5duzyvz59++qln/6mpqaZKlSpm4sSJZseOHWbevHnG4XB4XsOLi4tNhw4dzO9+9zuzbt06s2PHDvPAAw+Y6tWrmwMHDlhyTsqKn73zO3TokKlQoYJZv369McaY6dOnmxo1aphWrVp5tmnQoIGZNWvWBRvdv/zlL8blcpk33njD8/tG5cqVg77RPXTokJk0aZK58sorTX5+vtm3b99ZG90qVaqYKVOmmJ07d5qdO3eazZs3m6ioKDNt2jSzY8cOs2rVKtO8eXMzaNAgY4wx69atMyEhIWbBggXm66+/Nhs2bDDPP/+8RUeJ8oBG1wbat29vpk+fbow59YtSjRo1zLJly4wx/9dcvPbaa57tDxw4YCIiIsyiRYssqfdCjhw5YipWrOj1n+uJEydMbGysefbZZ01aWpq5+uqrfW7Ui4qKTOXKlc0777zjWXdm83f8+HFTrVo1r/PRrFkzM3HiRM/ncXFx5vrrr/fa74ABA8xvf/tbY4wxU6dONQ0aNDhrTV9++aVxOBzmu+++81rftWtXk5aW5tMx/Nqnn35qJJl//etfnnXHjx83kZGRnuWhhx4qcay/bj7Hjx9vmjRp4rXvCRMmlGh0JXkSc2NO/UU2JibG8/ldd91V4j/28yW6p/3nP/8xkswvv/xijDmVKPw6Xe/QoUPANrql+T6UN6U5Nz///LPZvXu3eemll0zjxo3N7t27zVtvvWVq165tdu/ebXbv3m1+/vlnqw7lsnrjjTdM1apVTXh4uGnfvr1JS0szmzZt8nxdkrn33nu9HtOmTRtz3333Xe5SL5oLveaffu3Yvn27ueaaa8zvf/9743a7z7vPXr16mQceeMDzeWpqqunYsaPXNq1atTLjx483xpz6A2mVKlVK/IGhXr165m9/+5u/h3hZ8bN3YS1atDDPPfecMcaYfv36maefftqEhYWZgoICk5+fbySZbdu2XbDRrV27tnnmmWc8nxcWFporr7wy6BtdY4yZNm2aiYuL83x+tka3X79+Xo8ZOHCgueeee7zWrVixwlSoUMH88ssv5s033zRVqlTxCh6AS4mLUQW5vLw8rV27VrfccoskKTQ0VAMGDNDcuXO9tmvXrp3n42rVqikxMVHbtm27rLX6ateuXSosLPR6f1DFihXVunVrbdu2Tbm5uerUqZMqVqx41sfv27dP9957rxo0aCCXyyWXy6UjR45oz549Z93e6XTqjjvu8Jyz3Nxcbdq0SYMGDfLa7sxzePrz0+fwpptu0i+//KK6detq6NChWrJkiYqKiiRJGzZskDFGDRo0UFRUlGdZvny5du3aVaZzdJrD4fB8HBYWptzcXOXm5qpx48Zyu90XfHxeXp5atWrlta5169YltqtUqZLq1avn+bx27drat29fmWpu2rSp134kefaVl5dX4vnPVk+g8ff7YGe+nJuoqCjFx8drw4YN6tu3r+Lj4/X555+rV69eio+PV3x8vKKioqw6hMuqf//++v777/X222+rZ8+eys7OVosWLZSZmenZ5nyvRcHoQq/5p3Xr1k1169bV66+/7vVewJMnT+rpp59W06ZNVb16dUVFRWnp0qUlXvPPfO2RvF/HcnJydOTIEc/jTy+7d+/2+3XaKvzsnVuXLl2UnZ0tY4xWrFihvn37qkmTJlq5cqU+/vhjxcTEqGHDhufdx+HDh5Wfn+/18xgaGqqUlJRLXX7A+PWx5uTkKDMz0+tnqGfPniouLtbu3bvVvXt3xcXFqW7duho4cKDmz59f7i/YiEuLi1EFuTlz5qioqEi/+c1vPOuMMapYsaIOHjx43see+Z9gIDH//9bOv67PGCOHw3HBC0MMGjRIP/74o6ZPn664uDg5nU61a9dOJ06cOOdjhgwZombNmunbb7/V3Llz1bVrV8XFxV2w1tM1XnXVVcrLy9OyZcv0wQcfaPjw4ZoyZYqWL1+u4uJihYSEKCcnRyEhIV6PL+svEAkJCXI4HNq+fbtXLQkJCZLk88UzTp/TX6/7tV//UcHhcJx1O1+cua/Tz11cXFxi3fnqCRQX6/tgR76emz179igpKUmSdPz4cYWGhur555+X2+1WhQoV9Nprr+mOO+7Q7NmzL/9BWCQ8PFzdu3dX9+7d9dhjj2nIkCF6/PHHS/zx7UyB+nruiwu95p/Wu3dvvfnmm9q6dauSk5M966dOnapp06Zp+vTpSk5OVmRkpMaMGVPiNf9sr2OnX3uKi4tVu3ZtZWdnl6ivLLdXsxI/exfWpUsXzZkzR5s2bVKFChWUlJSk1NRULV++XAcPHlRqaqrVJQaFyMhIr8+Li4s1bNgwjRo1qsS2derUUVhYmDZs2KDs7GwtXbpUjz32mCZOnKh169YF3c8ZggOJbhArKirSyy+/rKlTp3r+Uns6jYyLi9P8+fM923766aeejw8ePKgdO3Zc8K+VVklISFBYWJhWrlzpWVdYWKj169erUaNGatq0qVasWKHCwsKzPn7FihUaNWqUevXqpcaNG8vpdGr//v3nfc7k5GSlpKTopZde0oIFC3T33XeX2ObMc3j68zPPYUREhPr06aMZM2YoOztba9as0eeff67mzZvr5MmT2rdvnxISEryWsl6VuHr16urevbteeOEFHT16tEz7kKSGDRtq3bp1XuvWr19f6v2EhYXp5MmTZa7jtMTERK1du9bvei6Xi/V9sCNfz01sbKxyc3P1/vvvKzQ0VLm5ufrss88knfpZzs3N1aRJky5X2QEpKSnJ6xxe6LUo2FzoNf+0Z555RnfddZe6du2qrVu3etafTuTuuOMOXXPNNapbt6527txZqhpatGihvXv3KjQ0tMTrdI0aNfw/yMuIn70L69y5s37++WdNnz5dqampcjgcSk1NVXZ2trKzs31qdF0ul2rXru3181hUVKScnJxLWXpAa9Gihb744osSP0Onf8alU6l3t27d9Oyzz2rz5s36+uuv9dFHH1lcOeyKRjeIvfvuuzp48KAGDx6sJk2aeC1/+MMfNGfOHM+2kyZN0ocffqgtW7Zo0KBBqlGjRsDe0DwyMlL33XefHnzwQb333nvaunWrhg4dqmPHjmnw4MG6//77VVBQoFtuuUXr16/Xzp079corrygvL0/SqV+aXnnlFW3btk2fffaZbr/9dp+StSFDhuiZZ57RyZMndeONN5b4+qpVq/Tss89qx44devHFF7V48WKNHj1a0qmbys+ZM0dbtmzRV199pVdeeUURERGKi4tTgwYNdPvtt+vOO+/Uv/71L+3evVvr1q3TX/7yF2VlZZX5PGVkZKioqEgpKSlatGiRtm3bpry8PL366qvavn17ifT4bIYNG6bt27dr/Pjx2rFjh15//XXPiGRpEqL4+Hht3rxZeXl52r9//zn/CHEhI0eO1Jw5czRv3jzt3LlTTz31lDZv3hzQaZWv34c777xTaWlpFld7eflybk43Ft98843atGmjhg0b6sCBA6pbt65at26thIQE1axZ0+pDuSwOHDig6667Tq+++qo2b96s3bt3a/HixXr22WfVt29fz3aLFy/W3LlztWPHDj3++ONau3at7r//fgsr98+FXvPP9Nxzz+n222/Xdddd50ksExIStGzZMq1evVrbtm3TsGHDtHfv3lLV0K1bN7Vr1079+vXT+++/r6+//lqrV6/WI488EtB/bDsXfvbOz+VyqVmzZnr11VfVpUsXSaea3w0bNmjHjh2edRcyevRoPfPMM1qyZIm2b9+u4cOH69ChQ5es7kA3fvx4rVmzRiNGjFBubq527typt99+WyNHjpR06vfWGTNmKDc3V//73//08ssvq7i4WImJiRZXDtuy6L3BuAhuuOEG06tXr7N+LScnx0gyU6dONZLMO++8Yxo3bmzCwsJMq1atAv6WCb/88osZOXKkqVGjxllvL7Rp0ybTo0cPU6lSJVO5cmXTqVMns2vXLmOMMRs2bDApKSnG6XSa+vXrm8WLF5u4uDgzbdo0z+N1xgWaTvv5559NpUqVzPDhw0vUExcXZ5544glz8803m0qVKpmYmBjPBcCMMWbJkiWmTZs2pkqVKiYyMtK0bdvW66JLJ06cMI899piJj483FStWNLVq1TI33nij37d5+v777839999vrr76alOxYkUTFRVlWrdubaZMmWKOHj1a4ljPd3shp9NpunTpYmbNmuV1gahfX4zj9PGe+fKxb98+0717dxMVFXXB2wudvsiVMf93W4fdu3d71k2aNMnUqFHDREVFmbvvvtuMGjXKtG3b1q/zdKn58n1ITU01d911l7WFWsCXc2OMMcOGDTOPPPKIMebUv4EhQ4ZYVbJljh8/bh5++GHTokUL43K5TKVKlUxiYqJ55JFHzLFjx4wxp36eX3zxRdO9e3fjdDpNXFycWbhwocWV++98r/lne+0YOXKkqV27tsnLyzMHDhwwffv2NVFRUaZmzZrmkUceKXGLl19fSMcYY/r27ev1M1lQUGBGjhxpYmNjTcWKFc1VV11lbr/9drNnz55LeOSXDj975/fAAw8YSWbLli2edddcc4254oorPHdcuNDFqAoLC83o0aNNlSpVTHR0tBk7dqwtbi9kjG8Xozrz96rT1q5d6/l9IDIy0jRt2tQ8/fTTxphTF6ZKTU01VatWNREREaZp06YBe2FU2IPDmAB+Axz8lp2drWuvvVYHDx7k/Q8X8M033yg+Pl7r1q1TixYtvL4WHx+vMWPGaMyYMdYUdxk9/fTTmj17tr755hurS5Ekde/eXbVq1dIrr7xidSmA5RwOh5YsWRKwEzkAAAQKLkaFcq+wsFD5+fl6+OGH1bZt2xJNrt1lZGSoVatWql69ulatWqUpU6ZYNgZ57NgxzZ49Wz179lRISIgWLlyoDz74QMuWLbOkHgAAAAQnGl2Ue6tWrdK1116rBg0a6I033rC6nMvu9Hthf/rpJ9WpU0cPPPCAZe8ldTgcysrK0lNPPSW3263ExES9+eab6tatmyX1AAAAIDgxugwAAAAAsBWuugwAAAAAsBUaXQAAAACArdDoAgAAAABshUbXxtxutyZOnCi32211KQGN8+Q7zpVvOE++41z5hvPkG86T7zhXvuE8+Y5zhUDDxahsrKCgQC6XS4cPH1aVKlWsLidgcZ58x7nyDefJd5wr33CefMN58h3nyjecJ99xrhBoSHQBAAAAALZCowsAAAAAsBUaXQAAAACArYRaXYCdFBQUWF2CF7fbrYcfflhutzugaqtYsaLVJXgpLi7Wn//8ZxUXF+uXX36xuhwvM2bMsLoEL0VFReratatmzpyp0NDAefm45ZZbrC7Bi9vt1ujRo7V3714dPHjQ6nI8Au1nTzp1rsaOHasDBw7oyJEjVpfj8d5771ldgpfCwkL16dNHCxcuDKjv4w8//GB1CV6Kiop03XXX6fnnnw+o1yhJuvXWW60uwcvp16n8/Hz99NNPVpfj0bt3b6tL8FJcXKwaNWqoTZs2qlAhcPKhefPmWV1CCSdOnNDQoUO1ZcsWhYWFWV2OR0pKitUllInD4bDsue1yCScuRnURBVIzGcgC6Ze0QBdojW6gCrRGN1Dxs+e7QGt0A1WgNbqBLNAa3UAVaI1uoArERjdQ0eiWnl3aw8D6cycAAAAAlHNWNrp2ETgzGAAAAAAAXAQ0ugAAAAAAW2F0GQAAAAACCKPL/iPRBQAAAADYCokuAAAAAAQQEl3/kegCAAAAAGyFRhcAAAAAYCuMLgMAAABAAGF02X8kugAAAAAAWyHRBQAAAIAAQqLrPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZodAEAAAAAtsLoMgAAAAAEEEaX/UeiCwAAAACwFRJdAAAAAAggJLr+I9EFAAAAANgKjS4AAAAAwFYYXQYAAACAAMLosv9IdAEAAAAAtkKiCwAAAAABhETXf+Um0V29erVCQkJ0/fXXW10KAAAAAOASKjeN7ty5czVy5EitXLlSe/bssbocAAAAADgrh8Nh2WIX5aLRPXr0qF5//XXdd999uuGGG5SZmen5WnZ2thwOh/7zn//ommuuUXh4uNq0aaPPP//cuoIBAAAAAGVWLhrdRYsWKTExUYmJibrjjjv0z3/+U8YYr20efPBBPffcc1q3bp1q1qypPn36qLCw0KKKAQAAAABlVS4a3Tlz5uiOO+6QJF1//fU6cuSIPvzwQ69tHn/8cXXv3l3JycmaN2+efvjhBy1ZssSKcgEAAACUY4wu+8/2jW5eXp7Wrl2rW265RZIUGhqqAQMGaO7cuV7btWvXzvNxtWrVlJiYqG3btp1zv263WwUFBV6L2+2+NAcBAAAAAPCZ7W8vNGfOHBUVFek3v/mNZ50xRhUrVtTBgwfP+9jz/UUjPT1dTzzxhNe6hx9+WGlpaf4VDAAAAKBcs1OyahVbN7pFRUV6+eWXNXXqVPXo0cPra/3799f8+fPVpEkTSdKnn36qOnXqSJIOHjyoHTt2qGHDhufcd1pamsaOHeu1jkQXAAAAAKxn60b33Xff1cGDBzV48GC5XC6vr/3hD3/QnDlzNG3aNEnSpEmTVL16dcXExGjChAmqUaOG+vXrd859O51OOZ1Or3UFBQUX/RgAAAAAAKVj6/fozpkzR926dSvR5EqnEt3c3Fxt2LBBkvTMM89o9OjRatmypfLz8/X2228rLCzscpcMAAAAoJzjYlT+s3Wi+84775zzay1atJAxRtnZ2ZKkjh07asuWLZepMgAAAADApWLrRhcAAAAAgo2dklWr2Hp0GQAAAABQ/pT7RLdLly4yxlhdBgAAAADgIin3jS4AAAAABBJGl/3H6DIAAAAAwFZIdAEAAAAggJDo+o9EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVGl0AAAAAgK0wugwAAAAAAYTRZf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqMLAAAAALAVRpcBAAAAIIAwuuw/El0AAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVkh0AQAAACCAkOj6j0QXAAAAAGArNLoAAAAAAFthdBkAAAAAAgijy/4j0QUAAAAA2AqJLgAAAAAEEBJd/5HoAgAAAABshUT3IgoN5XT6IiIiwuoSgkaTJk2sLiEoZGVlWV1CULjtttusLiFo9OvXz+oSgsKJEyesLiFo8DuCb+677z6rSwgKkZGRVpcABDxedQEAAAAggDC67D9GlwEAAAAAtkKiCwAAAAABhETXfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVGl0AAAAAgK0wugwAAAAAAYTRZf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqILAAAAAAGERNd/JLoAAAAAAFuh0QUAAAAA2AqjywAAAAAQQBhd9h+JLgAAAADAVkh0AQAAACCAkOj6j0QXAAAAAGArAdnoOhwO/fvf/7a6DAAAAABAEGJ0GQAAAAACCKPL/gvIRBcAAAAAgLIqU6Prdrs1atQo1axZU+Hh4erYsaPWrVvn+foXX3yh3r17q0qVKqpcubI6deqkXbt2SZLWrVun7t27q0aNGnK5XEpNTdWGDRvO+VzXXXed7r//fq91Bw4ckNPp1EcffSRJio+P15NPPqnbbrtNUVFRio2N1cyZM70eM3HiRNWpU0dOp1OxsbEaNWqU52snTpzQQw89pN/85jeKjIxUmzZtlJ2dXZZTAwAAAAB+cTgcli12UaZG96GHHtKbb76pefPmacOGDUpISFDPnj31008/6bvvvlPnzp0VHh6ujz76SDk5Obr77rtVVFQkSfr555911113acWKFfr0009Vv3599erVSz///PNZn2vIkCFasGCB3G63Z938+fMVGxura6+91rNuypQpatq0qTZs2KC0tDT96U9/0rJlyyRJb7zxhqZNm6a//e1v2rlzp/79738rOTnZ89g//vGPWrVqlV577TVt3rxZN910k66//nrt3LmzLKcHAAAAAGAhhzHGlOYBR48eVdWqVZWZmanbbrtNklRYWKj4+HiNGTNGBw8e1Guvvaa8vDxVrFjxgvs7efKkqlatqgULFuiGG244VZTDoSVLlqhfv35yu92KjY3VrFmzdPPNN0uSmjdvrn79+unxxx+XdCrRbdSokf773/969nvLLbeooKBAWVlZ+utf/6q//e1v2rJlS4madu3apfr16+vbb79VbGysZ323bt3UunVrTZ48+ax1u91ur+b79LE4nc4LHnN5V6lSJatLCBr/+c9/rC4hKOzZs8fqEoLC6ddsXNjJkyetLiEonDhxwuoSgkZoKJdF8cVrr71mdQlBoWvXrlaXEDQaNWpkdQllkpiYaNlz5+XlWfbcF1OpE91du3apsLBQHTp08KyrWLGiWrdurW3btik3N1edOnU6Z5O7b98+3XvvvWrQoIFcLpdcLpeOHDlyzl9UnU6n7rjjDs2dO1eSlJubq02bNmnQoEFe27Vr167E59u2bZMk3XTTTfrll19Ut25dDR06VEuWLPEkzBs2bJAxRg0aNFBUVJRnWb58uWfc+mzS09M99Z9ennvuufOfPAAAAADAJVfqPy+eDoB/Pb9tjJHD4VBERMR5Hz9o0CD9+OOPmj59uuLi4uR0OtWuXbvz/lV4yJAhatasmb799lvNnTtXXbt2VVxc3AVrPV3jVVddpby8PC1btkwffPCBhg8frilTpmj58uUqLi5WSEiIcnJyFBIS4vX4qKioc+47LS1NY8eO9VpHAgAAAAAA1it1o5uQkKCwsDCtXLnSa3R5/fr1GjNmjI4ePap58+apsLDwrKnuihUrlJGRoV69ekmSvvnmG+3fv/+8z5mcnKyUlBS99NJLWrBgQYkLTUnSp59+WuLzhg0bej6PiIhQnz591KdPH40YMUINGzbU559/rubNm+vkyZPat2+fOnXq5PN5cDqdJcaUjx075vPjAQAAAOBs7HRRKKuUutGNjIzUfffdpwcffFDVqlVTnTp19Oyzz+rYsWMaPHiwiouLNXPmTN1yyy1KS0uTy+XSp59+qtatWysxMVEJCQl65ZVXlJKSooKCAj344IMXTIGlU6nu/fffr0qVKunGG28s8fVVq1bp2WefVb9+/bRs2TItXrzY8/7GzMxMnTx5Um3atFGlSpX0yiuvKCIiQnFxcapevbpuv/123XnnnZo6daqaN2+u/fv366OPPlJycrKnIQcAAAAABIcyXXX5mWeeUf/+/TVw4EC1aNFCX375pd5//31VrVpV1atX10cffaQjR44oNTVVLVu21EsvveRJd+fOnauDBw+qefPmGjhwoOc2RRdy6623KjQ0VLfddpvCw8NLfP2BBx5QTk6OmjdvrieffFJTp05Vz549JUnR0dF66aWX1KFDBzVt2lQffvih3nnnHVWvXl2S9M9//lN33nmnHnjgASUmJqpPnz767LPPdNVVV5Xl9AAAAABAmXF7If+V+qrLVvnmm28UHx+vdevWqUWLFl5fO33F5zFjxlhT3P/H6LJvuOqy77jqsm+46rJvuOqy77jmgm+46rLvuOqyb7jqsm+46rLvgvWqy1bWffqCvsEu4F91CwsLlZ+fr4cfflht27Yt0eQCAAAAAHCmgG90V61apWuvvVYNGjTQG2+8YXU5AAAAAHBJ2WmE2Cpleo/u5dSlSxcZY5SXl6fk5OSzbvP1119bPrYMAAAAAOVRRkaGrr76aoWHh6tly5ZasWLFObf917/+pe7du+uKK65QlSpV1K5dO73//vsltnvzzTeVlJQkp9OppKQkLVmypFQ1BXyjCwAAAADlSTBdjGrRokUaM2aMJkyYoI0bN6pTp0767W9/e85rqHzyySfq3r27srKylJOTo2uvvVa/+93vtHHjRs82a9as0YABAzRw4EBt2rRJAwcO1M0336zPPvvM93MYLBejCgZcjMo3XIzKd1yMyjdcjMo3XIzKd1yMyjdcjMp3XIzKN1yMyjdcjMp3wXoxqsaNG1v23F988UWptm/Tpo1atGihWbNmedY1atRI/fr1U3p6uk/7aNy4sQYMGKDHHntMkjRgwAAVFBTov//9r2eb66+/XlWrVtXChQt92ieJLgAAAAAEECsTXbfbrYKCAq/F7Xaftc4TJ04oJydHPXr08Frfo0cPrV692qdjLS4u1s8//6xq1ap51q1Zs6bEPnv27OnzPiUaXQAAAADA/5eeni6Xy+W1nCuZ3b9/v06ePKmYmBiv9TExMdq7d69Pzzd16lQdPXpUN998s2fd3r17/dqnFARXXQYAAAAAXB5paWkaO3as1zqn03nex/z6vb3GGJ/e77tw4UJNnDhRb731lmrWrHlR9nkajS4AAAAABBArby/kdDov2NieVqNGDYWEhJRIWvft21cikf21RYsWafDgwVq8eLG6devm9bVatWqVaZ9nYnQZAAAAAFBqYWFhatmypZYtW+a1ftmyZWrfvv05H7dw4UINGjRICxYsUO/evUt8vV27diX2uXTp0vPu89dIdAEAAAAggFiZ6JbW2LFjNXDgQKWkpKhdu3b6+9//rj179ujee++VdGoU+rvvvtPLL78s6VSTe+edd+r5559X27ZtPcltRESEXC6XJGn06NHq3Lmz/vKXv6hv375666239MEHH2jlypU+10WiCwAAAAAokwEDBmj69OmaNGmSmjVrpk8++URZWVmKi4uTJOXn53vdCvJvf/ubioqKNGLECNWuXduzjB492rNN+/bt9dprr+mf//ynmjZtqszMTC1atEht2rTxuS7uo3sRcR9d33AfXd9xH13fcB9d33AfXd9xH13fcB9d33EfXd9wH13fcB9d3wXrfXSbNm1q2XNv3rzZsue+mHjVBQAAAIAAEkyjy4GK0WUAAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVmh0AQAAAAC2wugyAAAAAAQQRpf9R6ILAAAAALAVEl0AAAAACCAkuv4j0QUAAAAA2AqJLgAAAAAEEBJd/5HoAgAAAABshUYXAAAAAGArjC5fREOHDrW6hKCwYMECq0sIGkeOHLG6hKAQHh5udQlB4fDhw1aXEDQ++OADq0sICp06dbK6hKDBGKJvOnToYHUJQcEYY3UJuMR4zfAfiS4AAAAAwFZIdAEAAAAggJDo+o9EFwAAAABgKzS6AAAAAABbYXQZAAAAAAIIo8v+I9EFAAAAANgKiS4AAAAABBASXf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqMLAAAAALAVRpcBAAAAIIAwuuw/El0AAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVmh0AQAAAAC2wugyAAAAAAQQRpf9R6ILAAAAALAVEl0AAAAACCAkuv4j0QUAAAAA2AqJLgAAAAAEEBJd/5HoAgAAAABshUYXAAAAAGArfje6jRs3VkZGxsWoBQAAAADKPYfDYdliF36/RzcrK0vR0dEXoRQAAAAAAPznd6IbFxcnl8tVYr3D4dC///1vSdLXX38th8Oh3Nxcn/ebmZlZpga6LM8FAAAAAIGCRNd/ZWp09+7dq9GjRyshIUHh4eGKiYlRx44dNXv2bB07dqzE9ldddZXy8/PVpEkTvws+06BBg9SvX7/L8lwAAAAAgOBQ6tHlr776Sh06dFB0dLQmT56s5ORkFRUVaceOHZo7d65iY2PVp08fr8eEhISoVq1aF63o87mczwUAAAAACDylTnSHDx+u0NBQrV+/XjfffLMaNWqk5ORk9e/fX//5z3/0u9/9rsRjzjZO/Pbbb6t+/fqKiIjQtddeq3nz5snhcOjQoUNej33//ffVqFEjRUVF6frrr1d+fr4kaeLEiZo3b57eeustT8yenZ1d4rmys7PlcDj04YcfKiUlRZUqVVL79u2Vl5fn9TxPPfWUatasqcqVK2vIkCF6+OGH1axZs9KeHgAAAADwC6PL/itVo3vgwAEtXbpUI0aMUGRk5Fm38eXkfP311/rDH/6gfv36KTc3V8OGDdOECRNKbHfs2DE999xzeuWVV/TJJ59oz549GjdunCRp3Lhxuvnmmz3Nb35+vtq3b3/O55wwYYKmTp2q9evXKzQ0VHfffbfna/Pnz9fTTz+tv/zlL8rJyVGdOnU0a9as8x6D2+1WQUGB13Ly5MkLHjsAAAAA4NIqVaP75ZdfyhijxMREzzq3262oqCjPMn78+AvuZ/bs2UpMTNSUKVOUmJioW265RYMGDSqxXWFhoWbPnq2UlBS1aNFC999/vz788ENJUlRUlCIiIuR0OlWrVi3VqlVLYWFh53zOp59+WqmpqUpKStLDDz+s1atX6/jx45KkmTNnavDgwfrjH/+oBg0a6LHHHlNycvJ5jyE9PV0ul8tr+eKLLy547AAAAABwPiS6/ivTxajOPAFhYWHKzc1Vbm6uGjduLLfbfcHH5+XlqVWrVl7rWrduXWK7SpUqqV69ep7Pa9eurX379pWlZDVt2tRrP5I8+8rLyyvx/Ger50xpaWk6fPiw19K4ceMy1QYAAAAAuHhKdTGqhIQEORwObd++3bPO4XAoISFBkhQREeHTfowxJf5aYIwpsV3FihW9Pnc4HGfdzhdn7uv0cxcXF5dYd756zuR0OuV0Or3WhYSElKk2AAAAADjNTsmqVUqV6FavXl3du3fXCy+8oKNHj5b5SRs2bKh169Z5rVu/fn2p9xMWFnZR3hebmJiotWvX+l0PAAAAAMB6pR5dzsjIUFFRkVJSUrRo0SJt27ZNeXl5evXVV7V9+3afUs1hw4Zp+/btGj9+vHbs2KHXX39dmZmZkkr314v4+Hht3rxZeXl52r9/vwoLC0t7OJKkkSNHas6cOZo3b5527typp556Sps3b+YvKQAAAAAQhErd6NarV08bN25Ut27dlJaWpmuuuUYpKSmaOXOmxo0bpyeffPKC+7j66qv1xhtv6F//+peaNm2qWbNmea66/Otx4PMZOnSoEhMTlZKSoiuuuEKrVq0q7eFIkm6//XalpaVp3LhxatGihXbv3q1BgwYpPDy8TPsDAAAAgLLiYlT+c5iyvun1Inv66ac1e/ZsffPNN1aXIknq3r27atWqpVdeecXnx9x+++2XsCL7WLBggdUlBI0jR45YXUJQ4I9Svjl8+LDVJQSNDz74wOoSgkKnTp2sLiFonO/OEPg/e/bssbqEoFCaYKi8S0pKsrqEMunatatlz336LjfBrlQXo7qYMjIy1KpVK1WvXl2rVq3SlClTdP/991tSy7FjxzR79mz17NlTISEhWrhwoT744AMtW7bMknoAAAAAlF92SlatYlmje/q9sD/99JPq1KmjBx54QGlpaZbU4nA4lJWVpaeeekput1uJiYl688031a1bN0vqAQAAAACUnWWN7rRp0zRt2jSrnt5LREQEY2oAAAAAYBOWNboAAAAAgJIYXfZfqa+6DAAAAABAICPRBQAAAIAAQqLrPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZodAEAAAAAtsLoMgAAAAAEEEaX/UeiCwAAAACwFRJdAAAAAAggJLr+I9EFAAAAANgKjS4AAAAAwFYYXQYAAACAAMLosv9IdAEAAAAAtkKiCwAAAAABhETXfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6F9HkyZOtLiEo/Pjjj1aXEDT+/Oc/W11CUBgxYoTVJQSF8PBwq0sIGsnJyVaXEBR69+5tdQlB47333rO6hKDwxhtvWF1CUBg8eLDVJeASI9H1H4kuAAAAAMBWaHQBAAAAALbC6DIAAAAABBBGl/1HogsAAAAAsBUSXQAAAAAIICS6/iPRBQAAAADYCo0uAAAAAMBWGF0GAAAAgADC6LL/SHQBAAAAALZCogsAAAAAAYRE138kugAAAAAAWyHRBQAAAIAAQqLrPxJdAAAAAICt0OgCAAAAAGyF0WUAAAAACCCMLvuPRBcAAAAAYCskugAAAAAQQEh0/UeiCwAAAACwFRpdAAAAAICtMLoMAAAAAAGE0WX/kegCAAAAAGyl3De6jRs3VkZGhtVlAAAAAICkU4muVYtdlPvR5aysLEVHR1tdBgAAAADgIin3jW5cXJzVJQAAAACAh52SVauUy9HlvXv3avTo0UpISFB4eLhiYmLUsWNHzZ49W8eOHbO6PAAAAACAH8pdovvVV1+pQ4cOio6O1uTJk5WcnKyioiLt2LFDc+fOVWxsrPr06WN1mQAAAACAMip3je7w4cMVGhqq9evXKzIy0rM+OTlZ/fv3lzHGwuoAAAAAlHeMLvuvXI0uHzhwQEuXLtWIESO8mtwz8Y8KAAAAAIJbuWp0v/zySxljlJiY6FnndrsVFRXlWcaPH29hhQAAAADKO24v5L9yN7oseae2YWFhys3NlSTdfvvtcrvdPu3D7XaX2NbtdsvpdF60OgEAAAAApVeuEt2EhAQ5HA5t377ds87hcCghIUEJCQmKiIjweV/p6elyuVxeS0ZGxqUoGwAAAABQCuWq0a1evbq6d++uF154QUePHvVrX2lpaTp8+LDXMnz48ItUKQAAAIDyitFl/5WrRleSMjIyVFRUpJSUFC1atEjbtm1TXl6eXn31VW3fvl0hISGSpDvvvFNpaWnn3I/T6VSVKlW8FsaWAQAAAMB65e49uvXq1dPGjRs1efJkpaWl6dtvv5XT6VRSUpLGjRvnSWX37NmjChXK3d8BAAAAAFjMTsmqVcpdoytJtWvX1syZMzVz5sxzbpOdnX35CgIAAAAAXDTlstEFAAAAgEBFous/ZnMBAAAAALZCowsAAAAAsBVGlwEAAAAggDC67D8SXQAAAACArZDoAgAAAEAAIdH1H4kuAAAAAMBWaHQBAAAAAGWWkZGhq6++WuHh4WrZsqVWrFhxzm3z8/N12223KTExURUqVNCYMWNKbJOZmSmHw1FiOX78uM810egCAAAAQAA5W5N3uZbSWrRokcaMGaMJEyZo48aN6tSpk377299qz549Z93e7Xbriiuu0IQJE3TNNdecc79VqlRRfn6+1xIeHu5zXTS6AAAAAIAy+etf/6rBgwdryJAhatSokaZPn66rrrpKs2bNOuv28fHxev7553XnnXfK5XKdc78Oh0O1atXyWkqDRhcAAAAAAoiVia7b7VZBQYHX4na7z1rniRMnlJOTox49enit79Gjh1avXu3XOThy5Iji4uJ05ZVX6oYbbtDGjRtL9XgaXQAAAACAJCk9PV0ul8trSU9PP+u2+/fv18mTJxUTE+O1PiYmRnv37i1zDQ0bNlRmZqbefvttLVy4UOHh4erQoYN27tzp8z64vRAAAAAABBArby+UlpamsWPHeq1zOp3nfcyv6zXG+HUMbdu2Vdu2bT2fd+jQQS1atNDMmTM1Y8YMn/ZBowsAAAAAkHSqqb1QY3tajRo1FBISUiK93bdvX4mU1x8VKlRQq1atSpXoMroMAAAAACi1sLAwtWzZUsuWLfNav2zZMrVv3/6iPY8xRrm5uapdu7bPjyHRBQAAAIAAYuXocmmNHTtWAwcOVEpKitq1a6e///3v2rNnj+69915Jp0ahv/vuO7388suex+Tm5ko6dcGpH3/8Ubm5uQoLC1NSUpIk6YknnlDbtm1Vv359FRQUaMaMGcrNzdWLL77oc100ugAAAACAMhkwYIAOHDigSZMmKT8/X02aNFFWVpbi4uIkSfn5+SXuqdu8eXPPxzk5OVqwYIHi4uL09ddfS5IOHTqke+65R3v37pXL5VLz5s31ySefqHXr1j7XRaMLAAAAAAEkmBJdSRo+fLiGDx9+1q9lZmaWWGeMOe/+pk2bpmnTpvlVE+/RBQAAAADYCo0uAAAAAMBWGF0GAAAAgAASbKPLgYhEFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVEl0AAAAACCAkuv6j0b2IDhw4YHUJQaFPnz5WlxA06tSpY3UJQWH79u1WlxAUWrVqZXUJQaNmzZpWlxAUlixZYnUJQeP48eNWlxAU/vGPf1hdQlC4+uqrrS4haNStW9fqEmARRpcBAAAAALZCogsAAAAAAYTRZf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqMLAAAAALAVRpcBAAAAIIAwuuw/El0AAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVkh0AQAAACCAkOj6j0QXAAAAAGArNLoAAAAAAFthdBkAAAAAAgijy/4j0QUAAAAA2AqJLgAAAAAEEBJd/5HoAgAAAABshUYXAAAAAGArjC4DAAAAQABhdNl/JLoAAAAAAFsh0QUAAACAAEKi6z8SXQAAAACArdi+0W3cuLEyMjKsLgMAAAAAcJnYfnQ5KytL0dHRVpcBAAAAAD5hdNl/tm904+LirC4BAAAAAHAZ2XJ0ee/evRo9erQSEhIUHh6umJgYdezYUbNnz9axY8dKbL948WK1b99ekrRq1SrVrVv3cpcMAAAAAJJOJbpWLXZhu0T3q6++UocOHRQdHa3JkycrOTlZRUVF2rFjh+bOnavY2Fj16dPH6zFr1qxRhw4dJEkrV670fAwAAAAACD62a3SHDx+u0NBQrV+/XpGRkZ71ycnJ6t+/v4wxJR6zevVqPfzww5JONbq9e/e+bPUCAAAAwJnslKxaxVajywcOHNDSpUs1YsQIryb3TKf/0SxYsEDR0dGKjo7W2rVrNXDgQEVHRysrK0vjxo1TdHS0FixYcDnLBwAAAABcBLZqdL/88ksZY5SYmOhZ53a7FRUV5VnGjx8vSerTp49yc3P13HPPKSkpSZ9//rlefvllxcTEaMuWLcrNzS0x4nwmt9utgoICr+XEiROX/BgBAAAAAOdnq0b3tDOj/rCwMOXm5io3N1eNGzeW2+2WJEVFRSk+Pl4bNmxQ3759FR8fr88//1y9evVSfHy84uPjFRUVdc7nSE9Pl8vl8lr++c9/XvJjAwAAAGBvXIzKf7Z6j25CQoIcDoe2b9/uWedwOJSQkCBJioiIkCTt2bNHSUlJkqTjx48rNDRUzz//vNxutypUqKDXXntNd9xxh2bPnn3O50pLS9PYsWO91m3duvViHxIAAAAAoJRs1ehWr15d3bt31wsvvKCRI0ee8326sbGxys3N1Q8//KCuXbsqNzdXJ0+eVLNmzbRixQpVq1ZNVapUOe9zOZ1OOZ1Or3VhYWEX7VgAAAAAlE92SlatYrvR5YyMDBUVFSklJUWLFi3Stm3blJeXp1dffVXbt29XSEiIQkNDlZCQoG+++UZt2rRRw4YNdeDAAdWtW1etW7dWQkKCatasafWhAAAAAADKwFaJriTVq1dPGzdu1OTJk5WWlqZvv/1WTqdTSUlJGjdunIYPH+7ZNjs7W507d5YkLV++3PMxAAAAACB42a7RlaTatWtr5syZmjlz5nm3O/M9uI8++uilLgsAAAAALojRZf/ZbnQZAAAAAFC+2TLRBQAAAIBgRaLrPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZodAEAAAAAtsLoMgAAAAAEEEaX/UeiCwAAAACwFRJdAAAAAAggJLr+I9EFAAAAANgKjS4AAAAAwFYYXQYAAACAAMLosv9IdAEAAAAAtkKiCwAAAAABhETXfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVGl0AAAAAgK0wugwAAAAAAYTRZf+R6AIAAAAAbIVE9yIKCwuzuoSgcMMNN1hdQtCoWrWq1SUEhV9++cXqEoJClSpVrC4haBw8eNDqEoJCdHS01SUEjYoVK1pdQlD44osvrC4hKGzbts3qEnCJkej6j0QXAAAAAGArJLoAAAAAEEBIdP1HogsAAAAAsBUaXQAAAACArTC6DAAAAAABhNFl/5HoAgAAAABshUQXAAAAAAIIia7/SHQBAAAAALZCowsAAAAAsBVGlwEAAAAggDC67D8SXQAAAACArZDoAgAAAEAAIdH1H4kuAAAAAMBWSHQBAAAAIICQ6PqPRBcAAAAAYCs0ugAAAAAAW2F0GQAAAAACCKPL/iPRBQAAAADYCokuAAAAAAQQEl3/kegCAAAAAGyFRhcAAAAAYCuMLgMAAABAAGF02X8kugAAAAAAWyHRBQAAAIAAQqLrv3KR6O7bt0/Dhg1TnTp15HQ6VatWLfXs2VNr1qyxujQAAAAAwEVWLhLd/v37q7CwUPPmzVPdunX1ww8/6MMPP9RPP/1kdWkAAAAA4IVE13+2b3QPHTqklStXKjs7W6mpqZKkuLg4tW7d2rONw+FQRkaG3n77bWVnZ6tWrVp69tlnddNNN1lVNgAAAACgjGw/uhwVFaWoqCj9+9//ltvtPud2jz76qPr3769Nmzbpjjvu0K233qpt27ZdxkoBAAAAABeD7Rvd0NBQZWZmat68eYqOjlaHDh305z//WZs3b/ba7qabbtKQIUPUoEEDPfnkk0pJSdHMmTMtqhoAAABAeeVwOCxb7ML2ja506j2633//vd5++2317NlT2dnZatGihTIzMz3btGvXzusx7dq1O2+i63a7VVBQ4LWcOHHiUh0CAAAAAMBH5aLRlaTw8HB1795djz32mFavXq1Bgwbp8ccfP+9jzvcXjfT0dLlcLq/lH//4x8UuGwAAAEA5Q6Lrv3LT6P5aUlKSjh496vn8008/9fr6p59+qoYNG57z8WlpaTp8+LDXMmTIkEtWLwAAAADAN7a/6vKBAwd000036e6771bTpk1VuXJlrV+/Xs8++6z69u3r2W7x4sVKSUlRx44dNX/+fK1du1Zz5sw5536dTqecTqfXurCwsEt2HAAAAAAA39i+0Y2KilKbNm00bdo07dq1S4WFhbrqqqs0dOhQ/fnPf/Zs98QTT+i1117T8OHDVatWLc2fP19JSUkWVg4AAACgPLLTCLFVbN/oOp1OpaenKz09/bzbxcbGaunSpZepKgAAAADApWL7RhcAAAAAggmJrv/K7cWoAAAAAAD2RKIryRhjdQkAAAAAIIlE92Ig0QUAAAAA2AqNLgAAAADAVhhdBgAAAIAAwuiy/0h0AQAAAAC2QqILAAAAAAGERNd/JLoAAAAAAFuh0QUAAAAA2AqjywAAAAAQQBhd9h+JLgAAAADAVkh0AQAAACCAkOj6j0QXAAAAAFBmGRkZuvrqqxUeHq6WLVtqxYoV59w2Pz9ft912mxITE1WhQgWNGTPmrNu9+eabSkpKktPpVFJSkpYsWVKqmmh0AQAAAABlsmjRIo0ZM0YTJkzQxo0b1alTJ/32t7/Vnj17zrq92+3WFVdcoQkTJuiaa6456zZr1qzRgAEDNHDgQG3atEkDBw7UzTffrM8++8znuhzGGFOmI0IJW7ZssbqEoBAVFWV1CUGjatWqVpcQFH755RerSwgKVapUsbqEoHHw4EGrSwgKvJ77rmLFilaXEBSOHTtmdQlBYdu2bVaXEDQ6depkdQllkpGRYdlzDx8+vFTbt2nTRi1atNCsWbM86xo1aqR+/fopPT39vI/t0qWLmjVrpunTp3utHzBggAoKCvTf//7Xs+76669X1apVtXDhQp/qItEFAAAAAEg6lbgWFBR4LW63+6zbnjhxQjk5OerRo4fX+h49emj16tVlrmHNmjUl9tmzZ89S7ZNGFwAAAAACiMPhsGxJT0+Xy+XyWs6VzO7fv18nT55UTEyM1/qYmBjt3bu3zMe/d+9ev/fJVZcBAAAAAJKktLQ0jR071mud0+k872N+fZVoY4zfV472d580ugAAAAAQQKy8vZDT6bxgY3tajRo1FBISUiJp3bdvX4lEtjRq1arl9z4ZXQYAAAAAlFpYWJhatmypZcuWea1ftmyZ2rdvX+b9tmvXrsQ+ly5dWqp9kugCAAAAAMpk7NixGjhwoFJSUtSuXTv9/e9/1549e3TvvfdKOjUK/d133+nll1/2PCY3N1eSdOTIEf3444/Kzc1VWFiYkpKSJEmjR49W586d9Ze//EV9+/bVW2+9pQ8++EArV670uS4aXQAAAAAIIFaOLpfWgAEDdODAAU2aNEn5+flq0qSJsrKyFBcXJ0nKz88vcU/d5s2bez7OycnRggULFBcXp6+//lqS1L59e7322mt65JFH9Oijj6pevXpatGiR2rRp43Nd3Ef3IuI+ur7hvou+4z66vuE+ur7hPrq+4z66vuH13HfcR9c33EfXN9xH13fBeh/dv/3tb5Y997Bhwyx77ouJRBcAAAAAAkgwJbqBiotRAQAAAABshUT3Ivruu++sLiEopKamWl1C0Dh69KjVJQSF8PBwq0sICoWFhVaXEDROnjxpdQlBweVyWV1C0MjPz7e6hKBQo0YNq0sICqV5nyJQXtHoAgAAAEAAYXTZf4wuAwAAAABshUQXAAAAAAIIia7/SHQBAAAAALZCogsAAAAAAYRE138kugAAAAAAW6HRBQAAAADYCqPLAAAAABBAGF32H4kuAAAAAMBWSHQBAAAAIICQ6PqPRBcAAAAAYCs0ugAAAAAAW2F0GQAAAAACCKPL/iPRBQAAAADYCokuAAAAAAQQEl3/kegCAAAAAGyFRBcAAAAAAgiJrv9IdAEAAAAAtkKjCwAAAACwFUaXAQAAACCAMLrsPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFYCvtHNzs6Ww+HQoUOHrC4FAAAAABAEGF0GAAAAgADC6LL/Aj7RBQAAAACgNAKi0XW73Ro1apRq1qyp8PBwdezYUevWrTvrtr/88ot69+6ttm3b6qefftKBAwd066236sorr1SlSpWUnJyshQsXej2mS5cuGjVqlB566CFVq1ZNtWrV0sSJE722OXz4sO655x7VrFlTVapU0XXXXadNmzZdqkMGAAAAgLNyOByWLXYREI3uQw89pDfffFPz5s3Thg0blJCQoJ49e+qnn37y2u7w4cPq0aOHTpw4oQ8//FDVqlXT8ePH1bJlS7377rvasmWL7rnnHg0cOFCfffaZ12PnzZunyMhIffbZZ3r22Wc1adIkLVu2TJJkjFHv3r21d+9eZWVlKScnRy1atFDXrl1L1AAAAAAACGwOY4yxsoCjR4+qatWqyszM1G233SZJKiwsVHx8vMaMGaNWrVrp2muv1fbt2zVgwADVq1dPCxcuVFhY2Dn32bt3bzVq1EjPPfecpFOJ7smTJ7VixQrPNq1bt9Z1112nZ555Rh999JFuvPFG7du3T06n07NNQkKCHnroId1zzz0lnsPtdsvtdnutW758+XnrwimpqalWlxA0jh49anUJQSEkJMTqEoKCnf5Ke6kdPnzY6hKCQp06dawuIWjk5+dbXUJQqFGjhtUlBAWLf30PKsH6u/mvJ1Qvp1tvvdWy576YLE90d+3apcLCQnXo0MGzrmLFimrdurW2bdvmWdetWzfVrVtXr7/+utc/2JMnT+rpp59W06ZNVb16dUVFRWnp0qXas2eP1/M0bdrU6/PatWtr3759kqScnBwdOXLE8/jTy+7du7Vr166z1p2eni6Xy+W1LFq0yO/zAQAAAADwj+VXXT79F6lfJw3GGK91vXv31ptvvqmtW7cqOTnZs37q1KmaNm2apk+fruTkZEVGRmrMmDE6ceKE1/4qVqzo9bnD4VBxcbEkqbi4WLVr11Z2dnaJ+qKjo89ad1pamsaOHeu1bvny5ec/WAAAAADAJWd5o5uQkKCwsDCtXLnSa3R5/fr1GjNmjGe7Z555RlFRUeratauys7OVlJQkSVqxYoX69u2rO+64Q9KppnXnzp1q1KiRzzW0aNFCe/fuVWhoqOLj4316jNPp9BpzloJ3NAIAAABA4ODtRv6zfHQ5MjJS9913nx588EG999572rp1q4YOHapjx45p8ODBXts+99xzuv3223Xddddp+/btkk41ysuWLdPq1au1bds2DRs2THv37i1VDd26dVO7du3Ur18/vf/++/r666+1evVqPfLII1q/fv1FO1YAAAAAwKVneaIrnUpri4uLNXDgQP38889KSUnR+++/r6pVq5bYdtq0aTp58qSuu+46ZWdn69FHH9Xu3bvVs2dPVapUSffcc4/69etXqguJOBwOZWVlacKECbr77rv1448/qlatWurcubNiYmIu5qECAAAAwHmR6PrP8qsu28n7779vdQlBgasu+46rLvuGqy77hv80fcdVl33DVZd9x1WXfcNVl33Dr+++C9a3Flp5kdsBAwZY9twXk+WjywAAAAAAXEwBMboMAAAAADiFKSz/kegCAAAAAGyFRBcAAAAAAgiJrv9IdAEAAAAAtkKiCwAAAAABhETXfyS6AAAAAABbodEFAAAAANgKo8sAAAAAEEAYXfYfiS4AAAAAwFZIdAEAAAAggJDo+o9EFwAAAABgKzS6AAAAAABbYXQZAAAAAAIIo8v+I9EFAAAAANgKiS4AAAAABBASXf+R6AIAAAAAbIVGFwAAAABgK4wuAwAAAEAAYXTZfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVmh0AQAAAAC2wujyRRQbG2t1CUHh8ccft7qEoHHjjTdaXUJQcLvdVpcQFBITE60uIWg4nU6rSwgK3377rdUlBA1jjNUlBIUaNWpYXUJQmDx5stUlBI0RI0ZYXUKZMLrsPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZodAEAAAAAtsLoMgAAAAAEEEaX/UeiCwAAAACwFRJdAAAAAAggJLr+I9EFAAAAANgKiS4AAAAABBASXf+R6AIAAAAAbIVGFwAAAABgK4wuAwAAAEAAYXTZfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVEl0AAAAACCAkuv4j0QUAAAAA2IrtG93GjRsrIyPD6jIAAAAAAJeJ7UeXs7KyFB0dbXUZAAAAAOATRpf9Z/tGNy4uzuoSAAAAAACXkS1Hl/fu3avRo0crISFB4eHhiomJUceOHTV79mwdO3asxPaLFy9W+/btJUmrVq1S3bp1L3fJAAAAACDpVKJr1WIXtkt0v/rqK3Xo0EHR0dGaPHmykpOTVVRUpB07dmju3LmKjY1Vnz59vB6zZs0adejQQZK0cuVKz8cAAAAAgOBju0Z3+PDhCg0N1fr16xUZGelZn5ycrP79+8sYU+Ixq1ev1sMPPyzpVKPbu3fvy1YvAAAAAODistXo8oEDB7R06VKNGDHCq8k90+k4fsGCBYqOjlZ0dLTWrl2rgQMHKjo6WllZWRo3bpyio6O1YMGCy1k+AAAAADC6fBHYqtH98ssvZYxRYmKiZ53b7VZUVJRnGT9+vCSpT58+ys3N1XPPPaekpCR9/vnnevnllxUTE6MtW7YoNze3xIjzmdxutwoKCryWEydOXPJjBAAAAACcn60a3dPO/EtEWFiYcnNzlZubq8aNG8vtdkuSoqKiFB8frw0bNqhv376Kj4/X559/rl69eik+Pl7x8fGKioo653Okp6fL5XJ5Lf/4xz8u+bEBAAAAsDcSXf/Z6j26CQkJcjgc2r59u2edw+FQQkKCJCkiIkKStGfPHiUlJUmSjh8/rtDQUD3//PNyu92qUKGCXnvtNd1xxx2aPXv2OZ8rLS1NY8eO9Vq3c+fOi31IAAAAAIBSslWjW716dXXv3l0vvPCCRo4cec736cbGxio3N1c//PCDunbtqtzcXJ08eVLNmjXTihUrVK1aNVWpUuW8z+V0OuV0Or3WhYWFXbRjAQAAAFA+2SlZtYrtRpczMjJUVFSklJQULVq0SNu2bVNeXp5effVVbd++XSEhIQoNDVVCQoK++eYbtWnTRg0bNtSBAwdUt25dtW7dWgkJCapZs6bVhwIAAAAAKANbJbqSVK9ePW3cuFGTJ09WWlqavv32WzmdTiUlJWncuHEaPny4Z9vs7Gx17txZkrR8+XLPxwAAAACA4GW7RleSateurZkzZ2rmzJnn3e7M9+A++uijl7osAAAAALggRpf9Z7vRZQAAAABA+WbLRBcAAAAAghWJrv9IdAEAAAAAtkKjCwAAAACwFUaXAQAAACCAMLrsPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZIdAEAAAAggJDo+o9EFwAAAABgKzS6AAAAAABbYXQZAAAAAAIIo8v+I9EFAAAAANgKiS4AAAAABBASXf+R6AIAAAAAbIVGFwAAAABgK4wuAwAAAEAAYXTZfyS6AAAAAIAyy8jI0NVXX63w8HC1bNlSK1asOO/2y5cvV8uWLRUeHq66detq9uzZXl/PzMyUw+EosRw/ftznmmh0AQAAACCAnK3Ju1xLaS1atEhjxozRhAkTtHHjRnXq1Em//e1vtWfPnrNuv3v3bvXq1UudOnXSxo0b9ec//1mjRo3Sm2++6bVdlSpVlJ+f77WEh4f7XBejywAAAACAMvnrX/+qwYMHa8iQIZKk6dOn6/3339esWbOUnp5eYvvZs2erTp06mj59uiSpUaNGWr9+vZ577jn179/fs53D4VCtWrXKXBeJLgAAAAAEECsTXbfbrYKCAq/F7Xaftc4TJ04oJydHPXr08Frfo0cPrV69+qyPWbNmTYnte/bsqfXr16uwsNCz7siRI4qLi9OVV16pG264QRs3bizVOaTRBQAAAABIktLT0+VyubyWsyWzkrR//36dPHlSMTExXutjYmK0d+/esz5m7969Z92+qKhI+/fvlyQ1bNhQmZmZevvtt7Vw4UKFh4erQ4cO2rlzp8/HwegyAAAAAECSlJaWprFjx3qtczqd533Mr9/ba4w57/t9z7b9mevbtm2rtm3ber7eoUMHtWjRQjNnztSMGTMufBCi0QUAAACAgGLl7YWcTucFG9vTatSooZCQkBLp7b59+0qktqfVqlXrrNuHhoaqevXqZ31MhQoV1KpVq1IluowuAwAAAABKLSwsTC1bttSyZcu81i9btkzt27c/62PatWtXYvulS5cqJSVFFStWPOtjjDHKzc1V7dq1fa6NRPci4sbOvhk0aJDVJQSNCRMmWF1CUHjwwQetLiEo5ObmWl1C0IiNjbW6hKDw8ccfW11C0GjSpInVJQSF77//3uoSgsKXX35pdQm4xIKprxg7dqwGDhyolJQUtWvXTn//+9+1Z88e3XvvvZJOjUJ/9913evnllyVJ9957r1544QWNHTtWQ4cO1Zo1azRnzhwtXLjQs88nnnhCbdu2Vf369VVQUKAZM2YoNzdXL774os910egCAAAAAMpkwIABOnDggCZNmqT8/Hw1adJEWVlZiouLkyTl5+d73VP36quvVlZWlv70pz/pxRdfVGxsrGbMmOF1a6FDhw7pnnvu0d69e+VyudS8eXN98sknat26tc910egCAAAAAMps+PDhGj58+Fm/lpmZWWJdamqqNmzYcM79TZs2TdOmTfOrJhpdAAAAAAggwTS6HKi4GBUAAAAAwFZIdAEAAAAggJDo+o9EFwAAAABgKzS6AAAAAABbYXQZAAAAAAIIo8v+I9EFAAAAANgKiS4AAAAABBASXf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqMLAAAAALAVRpcBAAAAIIAwuuw/El0AAAAAgK2Q6AIAAABAACHR9R+JLgAAAADAVmh0AQAAAAC2wugyAAAAAAQQRpf9R6ILAAAAALAVEl0AAAAACCAkuv4j0QUAAAAA2Eq5aHQzMzMVHR3t+XzixIlq1qzZeR8zaNAg9evX75LWBQAAAAC/5nA4LFvsolw0ugMGDNCOHTusLgMAAAAAcBmUi/foRkREKCIiwuoyAAAAAACXQdAmuu+8846io6NVXFwsScrNzZXD4dCDDz7o2WbYsGG69dZbS4wu/9rJkyc1duxYRUdHq3r16nrooYdkjLnUhwAAAAAAJTC67L+gbXQ7d+6sn3/+WRs3bpQkLV++XDVq1NDy5cs922RnZys1NfWC+5o6darmzp2rOXPmaOXKlfrpp5+0ZMmSS1Y7AAAAAODSCdpG1+VyqVmzZsrOzpZ0qqn905/+pE2bNunnn3/W3r17tWPHDnXp0uWC+5o+fbrS0tLUv39/NWrUSLNnz5bL5bq0BwAAAAAAZ0Gi67+gbXQlqUuXLsrOzpYxRitWrFDfvn3VpEkTrVy5Uh9//LFiYmLUsGHD8+7j8OHDys/PV7t27TzrQkNDlZKSct7Hud1uFRQUeC0nTpy4KMcFAAAAACi7oG90V6xYoU2bNqlChQpKSkpSamqqli9f7vPYclmlp6fL5XJ5Lf/4xz8u2fMBAAAAAHwT1I3u6ffpTp8+XampqXI4HEpNTVV2drbPja7L5VLt2rX16aefetYVFRUpJyfnvI9LS0vT4cOHvZYhQ4b4fUwAAAAAyjdGl/0X1LcXOv0+3VdffVXPP/+8pFPN70033aTCwkKf3p8rSaNHj9Yzzzyj+vXrq1GjRvrrX/+qQ4cOnfcxTqdTTqfTa11YWFhZDgMAAAAAcBEFdaMrSddee602bNjgaWqrVq2qpKQkff/992rUqJFP+3jggQeUn5+vQYMGqUKFCrr77rt144036vDhw5ewcgAAAAAoyU7JqlUchhvGXjRbtmyxuoSgEBISYnUJQWPChAlWlxAUzrx/Ns6NP975LjY21uoSgsLHH39sdQlBo0mTJlaXEBTatm1rdQlB4csvv7S6hKBxzTXXWF1CmVj5PU5ISLDsuS+moE90AQAAAMBOSHT9F9QXowIAAAAA4NdodAEAAAAAtsLoMgAAAAAEEEaX/UeiCwAAAACwFRJdAAAAAAggJLr+I9EFAAAAANgKjS4AAAAAwFYYXQYAAACAAMLosv9IdAEAAAAAtkKiCwAAAAABhETXfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVGl0AAAAAgK0wugwAAAAAAYTRZf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqILAAAAAAGERNd/NLoX0XXXXWd1CUFh8+bNVpcQNOrXr291CUGhatWqVpcQFAoLC60uIWg0aNDA6hKCQkJCgtUlBI2tW7daXUJQOHjwoNUlBIXk5GSrSwACHqPLAAAAAABbIdEFAAAAgADC6LL/SHQBAAAAALZCogsAAAAAAYRE138kugAAAAAAW6HRBQAAAADYCqPLAAAAABBAGF32H4kuAAAAAMBWSHQBAAAAIICQ6PqPRBcAAAAAYCskugAAAAAQQEh0/UeiCwAAAACwFRpdAAAAAICtMLoMAAAAAAGE0WX/kegCAAAAAGyFRBcAAAAAAgiJrv9IdAEAAAAAtkKjCwAAAACwFUaXAQAAACCAMLrsPxJdAAAAAICtkOgCAAAAQAAh0fUfiS4AAAAAwFZodAEAAAAAtsLoMgAAAAAEEEaX/ReUia4xRvfcc4+qVasmh8Oh6OhojRkzxuqyAAAAAAABICgT3ffee0+ZmZnKzs5W3bp1VaFCBUVERFhdFgAAAAD4jUTXf0HZ6O7atUu1a9dW+/btfdr+xIkTCgsLu8RVAQAAAAACQdCNLg8aNEgjR47Unj175HA4FB8fry5duniNLsfHx+upp57SoEGD5HK5NHToUEnS6tWr1blzZ0VEROiqq67SqFGjdPToUc/jMjIyVL9+fYWHhysmJkZ/+MMfLvfhAQAAACjnHA6HZYtdBF2j+/zzz2vSpEm68sorlZ+fr3Xr1p11uylTpqhJkybKycnRo48+qs8//1w9e/bU73//e23evFmLFi3SypUrdf/990uS1q9fr1GjRmnSpEnKy8vTe++9p86dO1/OQwMAAAAAXARBN7rscrlUuXJlhYSEqFatWufc7rrrrtO4ceM8n99555267bbbPMlv/fr1NWPGDKWmpmrWrFnas2ePIiMjdcMNN6hy5cqKi4tT8+bNz7l/t9stt9vttc4YY6u/ggAAAABAMAq6RNdXKSkpXp/n5OQoMzNTUVFRnqVnz54qLi7W7t271b17d8XFxalu3boaOHCg5s+fr2PHjp1z/+np6XK5XF7L+bYHAAAAAF8wuuw/2za6kZGRXp8XFxdr2LBhys3N9SybNm3Szp07Va9ePVWuXFkbNmzQwoULVbt2bT322GO65pprdOjQobPuPy0tTYcPH/ZaKlWqdBmODAAAAABwPkE3ulxWLVq00BdffKGEhIRzbhMaGqpu3bqpW7duevzxxxUdHa2PPvpIv//970ts63Q65XQ6vdbZ6S8gAAAAAKxBX+G/ctPojh8/Xm3bttWIESM0dOhQRUZGatu2bVq2bJlmzpypd999V1999ZU6d+6sqlWrKisrS8XFxUpMTLS6dAAAAABAKZSbRrdp06Zavny5JkyYoE6dOskYo3r16mnAgAGSpOjoaP3rX//SxIkTdfz4cdWvX18LFy5U48aNLa4cAAAAAFAaDmOMsboIu6hZs6bVJQSFzZs3W11C0Jg2bZrVJQSFP/7xj1aXEBT27dtndQlBo3Xr1laXEBSKi4utLiFobN261eoSgsL57qiB/xMbG2t1CUGjQoXgvCSRlRe5tct1h4LzOw8AAAAAwDmUm9FlAAAAAAgGXIzKfyS6AAAAAABbIdEFAAAAgABCous/El0AAAAAgK3Q6AIAAAAAbIXRZQAAAAAIIIwu+49EFwAAAABgKyS6AAAAABBASHT9R6ILAAAAALAVGl0AAAAAgK0wugwAAAAAAYTRZf+R6AIAAAAAbIVEFwAAAAACCImu/0h0AQAAAAC2QqILAAAAAAGERNd/JLoAAAAAAFuh0QUAAAAA2AqjywAAAAAQQBhd9h+JLgAAAADAVkh0AQAAACCAkOj6j0QXAAAAAGArNLoAAAAAAFthdBkAAAAAAgijy/4j0QUAAAAA2IrDGGOsLgIAAAAAgIuFRBcAAAAAYCs0ugAAAAAAW6HRBQAAAADYCo0uAAAAAMBWaHQBAAAAALZCowsAAAAAsBUaXQAAAACArdDoAgAAAABshUYXAAAAAGAr/w/kG9ZVEb72RQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the attention map, for the third tweet in the test set, on the 11th head of the 1st multi-head layer\n",
    "ComputeAttentionMaps(dataset = TestDataset, model = BERTweetlarge, tokenizer = BERTweetlarge_tokenizer, input_id = 3, multiheadlayer_no = 0, head_no = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the attention map, for the third tweet in the test set, on the 5th head of the 3rd multi-head layer\n",
    "ComputeAttentionMaps(dataset = TestDataset, model = BERTweetlarge, tokenizer = BERTweetlarge_tokenizer, input_id = 3, multiheadlayer_no = 2, head_no = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
