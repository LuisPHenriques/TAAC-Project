{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Custom Functions & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n",
    "import spacy \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    return pd.read_csv(filename, sep=',', encoding='utf-8', index_col=0).iloc[:, -2:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub('http\\S*', ' ', text)\n",
    "    \n",
    "    # remove non-alphabetic\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove one character word\n",
    "    text = re.sub(\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(\"^[a-zA-Z]\\s+\", '', text)\n",
    "    \n",
    "    # replace double space to one space\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    \n",
    "    # tokenize, lemmatize, remove stop words\n",
    "    doc = nlp(text)\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "class BPE():\n",
    "    \"\"\"Byte-Pair Encoding: Subword-based tokenization algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train BPE tokenizer.\"\"\"\n",
    "\n",
    "        # compute the frequencies of each word in the corpus\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # compute the base vocabulary of all characters in the corpus\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.append(letter)\n",
    "        alphabet.sort()\n",
    "\n",
    "        # add the special token </w> at the beginning of the vocabulary\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # split each word into individual characters before training\n",
    "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
    "\n",
    "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "        while len(vocab) < self.vocab_size:\n",
    "\n",
    "            # compute the frequency of each pair\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "\n",
    "            # find the most frequent pair\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # merge the most frequent pair\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"Merge the given pair.\"\"\"\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "    \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
    "        \n",
    "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 Just happened a terrible car crash\n",
       "1  Heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           Apocalypse lighting. #Spokane #wildfires\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_train = read_csv('../data/tweets_data/train.csv')\n",
    "tweets_test = read_csv('../data/tweets_data/test.csv')[['text']]\n",
    "\n",
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text using key-words and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train['clean_text'] = tweets_train['text'].apply(preprocess)\n",
    "tweets_test['clean_text'] = tweets_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \n",
       "0               deed reason earthquake allah forgive  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3    people receive wildfire evacuation order cal...  \n",
       "4  got send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                 Just happened a terrible car crash   \n",
       "1  Heard about #earthquake is different cities, s...   \n",
       "2  there is a forest fire at spot pond, geese are...   \n",
       "3           Apocalypse lighting. #Spokane #wildfires   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                     clean_text  \n",
       "0                     happen terrible car crash  \n",
       "1      hear earthquake different city stay safe  \n",
       "2  forest fire spot pond goose flee street save  \n",
       "3          apocalypse lighting spokane wildfire  \n",
       "4            typhoon soudelor kill china taiwan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word tokenization with BERT Tokenizer for Byte-Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 'r'): 'er',\n",
       " ('r', 'e'): 're',\n",
       " ('i', 'n'): 'in',\n",
       " ('a', 'n'): 'an',\n",
       " ('o', 'n'): 'on',\n",
       " ('s', 't'): 'st',\n",
       " ('a', 't'): 'at',\n",
       " ('a', 'r'): 'ar',\n",
       " ('a', 'l'): 'al',\n",
       " ('o', 'r'): 'or',\n",
       " ('e', 'n'): 'en',\n",
       " ('l', 'e'): 'le',\n",
       " ('l', 'i'): 'li',\n",
       " ('d', 'e'): 'de',\n",
       " ('i', 'c'): 'ic',\n",
       " ('s', 'e'): 'se',\n",
       " ('a', 'm'): 'am',\n",
       " ('r', 'o'): 'ro',\n",
       " ('l', 'o'): 'lo',\n",
       " ('i', 'l'): 'il',\n",
       " ('a', 'c'): 'ac',\n",
       " ('i', 't'): 'it',\n",
       " ('s', 'h'): 'sh',\n",
       " ('u', 'n'): 'un',\n",
       " ('c', 'h'): 'ch',\n",
       " ('c', 'o'): 'co',\n",
       " ('u', 'r'): 'ur',\n",
       " ('v', 'e'): 've',\n",
       " ('t', 'h'): 'th',\n",
       " ('r', 'a'): 'ra',\n",
       " ('i', 'on'): 'ion',\n",
       " ('g', 'e'): 'ge',\n",
       " ('i', 's'): 'is',\n",
       " ('m', 'e'): 'me',\n",
       " ('in', 'g'): 'ing',\n",
       " ('a', 'y'): 'ay',\n",
       " ('k', 'e'): 'ke',\n",
       " ('n', 'e'): 'ne',\n",
       " ('a', 'd'): 'ad',\n",
       " ('r', 'i'): 'ri',\n",
       " ('h', 'o'): 'ho',\n",
       " ('o', 'd'): 'od',\n",
       " ('s', 's'): 'ss',\n",
       " ('at', 'e'): 'ate',\n",
       " ('en', 't'): 'ent',\n",
       " ('e', 'l'): 'el',\n",
       " ('a', 'p'): 'ap',\n",
       " ('f', 'i'): 'fi',\n",
       " ('l', 'y'): 'ly',\n",
       " ('t', 'er'): 'ter',\n",
       " ('o', 'm'): 'om',\n",
       " ('g', 'h'): 'gh',\n",
       " ('u', 't'): 'ut',\n",
       " ('s', 'u'): 'su',\n",
       " ('m', 'a'): 'ma',\n",
       " ('l', 'a'): 'la',\n",
       " ('o', 't'): 'ot',\n",
       " ('p', 'e'): 'pe',\n",
       " ('f', 'e'): 'fe',\n",
       " ('c', 'e'): 'ce',\n",
       " ('o', 'l'): 'ol',\n",
       " ('v', 'i'): 'vi',\n",
       " ('c', 'k'): 'ck',\n",
       " ('c', 't'): 'ct',\n",
       " ('d', 'i'): 'di',\n",
       " ('g', 'o'): 'go',\n",
       " ('u', 's'): 'us',\n",
       " ('p', 'o'): 'po',\n",
       " ('h', 'e'): 'he',\n",
       " ('r', 'u'): 'ru',\n",
       " ('d', 'er'): 'der',\n",
       " ('ne', 'w'): 'new',\n",
       " ('b', 'e'): 'be',\n",
       " ('s', 'i'): 'si',\n",
       " ('at', 'ion'): 'ation',\n",
       " ('m', 'o'): 'mo',\n",
       " ('an', 'd'): 'and',\n",
       " ('o', 'w'): 'ow',\n",
       " ('a', 'st'): 'ast',\n",
       " ('gh', 't'): 'ght',\n",
       " ('fi', 're'): 'fire',\n",
       " ('i', 'r'): 'ir',\n",
       " ('w', 'e'): 'we',\n",
       " ('p', 'l'): 'pl',\n",
       " ('c', 'on'): 'con',\n",
       " ('e', 'x'): 'ex',\n",
       " ('an', 't'): 'ant',\n",
       " ('in', 'e'): 'ine',\n",
       " ('ac', 'k'): 'ack',\n",
       " ('v', 'er'): 'ver',\n",
       " ('o', 'p'): 'op',\n",
       " ('a', 'b'): 'ab',\n",
       " ('u', 'l'): 'ul',\n",
       " ('s', 'p'): 'sp',\n",
       " ('am', 'p'): 'amp',\n",
       " ('e', 'm'): 'em',\n",
       " ('s', 'c'): 'sc',\n",
       " ('h', 'i'): 'hi',\n",
       " ('st', 'or'): 'stor',\n",
       " ('i', 'd'): 'id',\n",
       " ('f', 'f'): 'ff',\n",
       " ('n', 'i'): 'ni',\n",
       " ('b', 'o'): 'bo',\n",
       " ('q', 'u'): 'qu',\n",
       " ('il', 'l'): 'ill',\n",
       " ('t', 'i'): 'ti',\n",
       " ('c', 'a'): 'ca',\n",
       " ('e', 'd'): 'ed',\n",
       " ('s', 'o'): 'so',\n",
       " ('h', 'a'): 'ha',\n",
       " ('li', 'ke'): 'like',\n",
       " ('t', 'ra'): 'tra',\n",
       " ('a', 'k'): 'ak',\n",
       " ('u', 'm'): 'um',\n",
       " ('w', 'or'): 'wor',\n",
       " ('t', 'y'): 'ty',\n",
       " ('stor', 'm'): 'storm',\n",
       " ('il', 'd'): 'ild',\n",
       " ('y', 'e'): 'ye',\n",
       " ('r', 'y'): 'ry',\n",
       " ('o', 'k'): 'ok',\n",
       " ('in', 'd'): 'ind',\n",
       " ('m', 'an'): 'man',\n",
       " ('p', 'ro'): 'pro',\n",
       " ('in', 'k'): 'ink',\n",
       " ('at', 't'): 'att',\n",
       " ('un', 'd'): 'und',\n",
       " ('it', 'y'): 'ity',\n",
       " ('a', 's'): 'as',\n",
       " ('a', 'ge'): 'age',\n",
       " ('t', 'e'): 'te',\n",
       " ('c', 'al'): 'cal',\n",
       " ('f', 'o'): 'fo',\n",
       " ('d', 'is'): 'dis',\n",
       " ('om', 'b'): 'omb',\n",
       " ('a', 'g'): 'ag',\n",
       " ('co', 'l'): 'col',\n",
       " ('p', 're'): 'pre',\n",
       " ('or', 't'): 'ort',\n",
       " ('i', 'g'): 'ig',\n",
       " ('lo', 'w'): 'low',\n",
       " ('b', 'u'): 'bu',\n",
       " ('i', 'st'): 'ist',\n",
       " ('b', 'omb'): 'bomb',\n",
       " ('en', 'd'): 'end',\n",
       " ('p', 'er'): 'per',\n",
       " ('s', 'a'): 'sa',\n",
       " ('c', 'y'): 'cy',\n",
       " ('d', 'on'): 'don',\n",
       " ('ar', 'd'): 'ard',\n",
       " ('f', 'or'): 'for',\n",
       " ('new', 's'): 'news',\n",
       " ('ac', 'e'): 'ace',\n",
       " ('on', 'e'): 'one',\n",
       " ('ur', 'n'): 'urn',\n",
       " ('at', 'h'): 'ath',\n",
       " ('u', 'p'): 'up',\n",
       " ('b', 'le'): 'ble',\n",
       " ('t', 'o'): 'to',\n",
       " ('w', 'a'): 'wa',\n",
       " ('u', 'se'): 'use',\n",
       " ('co', 'm'): 'com',\n",
       " ('ar', 't'): 'art',\n",
       " ('ro', 'w'): 'row',\n",
       " ('ic', 'e'): 'ice',\n",
       " ('t', 'on'): 'ton',\n",
       " ('d', 'ay'): 'day',\n",
       " ('lo', 'od'): 'lood',\n",
       " ('al', 'l'): 'all',\n",
       " ('n', 'a'): 'na',\n",
       " ('sh', 'i'): 'shi',\n",
       " ('m', 'ent'): 'ment',\n",
       " ('o', 'ut'): 'out',\n",
       " ('u', 're'): 'ure',\n",
       " ('o', 'b'): 'ob',\n",
       " ('ac', 't'): 'act',\n",
       " ('a', 're'): 'are',\n",
       " ('u', 'e'): 'ue',\n",
       " ('c', 'i'): 'ci',\n",
       " ('c', 'ra'): 'cra',\n",
       " ('ge', 't'): 'get',\n",
       " ('el', 'l'): 'ell',\n",
       " ('s', 'on'): 'son',\n",
       " ('g', 'en'): 'gen',\n",
       " ('t', 'a'): 'ta',\n",
       " ('m', 'i'): 'mi',\n",
       " ('e', 't'): 'et',\n",
       " ('s', 'ur'): 'sur',\n",
       " ('m', 'ar'): 'mar',\n",
       " ('p', 'h'): 'ph',\n",
       " ('w', 'ar'): 'war',\n",
       " ('i', 'm'): 'im',\n",
       " ('ti', 'me'): 'time',\n",
       " ('n', 'ow'): 'now',\n",
       " ('a', 'il'): 'ail',\n",
       " ('ic', 'i'): 'ici',\n",
       " ('u', 'st'): 'ust',\n",
       " ('al', 'ly'): 'ally',\n",
       " ('w', 'at'): 'wat',\n",
       " ('b', 're'): 'bre',\n",
       " ('l', 'and'): 'land',\n",
       " ('e', 'v'): 'ev',\n",
       " ('an', 'e'): 'ane',\n",
       " ('po', 'li'): 'poli',\n",
       " ('b', 'l'): 'bl',\n",
       " ('z', 'e'): 'ze',\n",
       " ('f', 'u'): 'fu',\n",
       " ('b', 'er'): 'ber',\n",
       " ('r', 'or'): 'ror',\n",
       " ('ic', 'k'): 'ick',\n",
       " ('a', 'ir'): 'air',\n",
       " ('o', 'ff'): 'off',\n",
       " ('pe', 'op'): 'peop',\n",
       " ('peop', 'le'): 'people',\n",
       " ('b', 'r'): 'br',\n",
       " ('k', 'ill'): 'kill',\n",
       " ('re', 'd'): 'red',\n",
       " ('l', 'd'): 'ld',\n",
       " ('i', 'an'): 'ian',\n",
       " ('a', 'v'): 'av',\n",
       " ('am', 'e'): 'ame',\n",
       " ('ar', 'y'): 'ary',\n",
       " ('w', 'ay'): 'way',\n",
       " ('it', 'e'): 'ite',\n",
       " ('b', 'urn'): 'burn',\n",
       " ('c', 'ar'): 'car',\n",
       " ('u', 'd'): 'ud',\n",
       " ('ow', 'n'): 'own',\n",
       " ('g', 'er'): 'ger',\n",
       " ('d', 'o'): 'do',\n",
       " ('vi', 'de'): 'vide',\n",
       " ('p', 'ort'): 'port',\n",
       " ('m', 'on'): 'mon',\n",
       " ('g', 're'): 'gre',\n",
       " ('an', 'ce'): 'ance',\n",
       " ('f', 'am'): 'fam',\n",
       " ('p', 'an'): 'pan',\n",
       " ('co', 'me'): 'come',\n",
       " ('vide', 'o'): 'video',\n",
       " ('c', 'le'): 'cle',\n",
       " ('i', 'o'): 'io',\n",
       " ('s', 'er'): 'ser',\n",
       " ('i', 've'): 'ive',\n",
       " ('i', 'e'): 'ie',\n",
       " ('m', 'in'): 'min',\n",
       " ('ye', 'ar'): 'year',\n",
       " ('n', 'o'): 'no',\n",
       " ('ast', 'er'): 'aster',\n",
       " ('li', 't'): 'lit',\n",
       " ('li', 'ght'): 'light',\n",
       " ('le', 't'): 'let',\n",
       " ('ar', 'm'): 'arm',\n",
       " ('f', 'lood'): 'flood',\n",
       " ('b', 'od'): 'bod',\n",
       " ('w', 'o'): 'wo',\n",
       " ('ho', 'me'): 'home',\n",
       " ('p', 'ho'): 'pho',\n",
       " ('bre', 'ak'): 'break',\n",
       " ('lo', 've'): 'love',\n",
       " ('i', 'a'): 'ia',\n",
       " ('bod', 'y'): 'body',\n",
       " ('em', 'er'): 'emer',\n",
       " ('cra', 'sh'): 'crash',\n",
       " ('am', 'a'): 'ama',\n",
       " ('ter', 'ror'): 'terror',\n",
       " ('e', 'p'): 'ep',\n",
       " ('ex', 'p'): 'exp',\n",
       " ('a', 'u'): 'au',\n",
       " ('ut', 'e'): 'ute',\n",
       " ('in', 'j'): 'inj',\n",
       " ('dis', 'aster'): 'disaster',\n",
       " ('li', 'de'): 'lide',\n",
       " ('c', 'l'): 'cl',\n",
       " ('bu', 'ild'): 'build',\n",
       " ('re', 'am'): 'ream',\n",
       " ('emer', 'gen'): 'emergen',\n",
       " ('sh', 'o'): 'sho',\n",
       " ('emergen', 'cy'): 'emergency',\n",
       " ('lo', 'ok'): 'look',\n",
       " ('f', 'at'): 'fat',\n",
       " ('att', 'ack'): 'attack',\n",
       " ('wor', 'k'): 'work',\n",
       " ('re', 'n'): 'ren',\n",
       " ('de', 'st'): 'dest',\n",
       " ('a', 'f'): 'af',\n",
       " ('s', 'pe'): 'spe',\n",
       " ('t', 'ro'): 'tro',\n",
       " ('t', 'ru'): 'tru',\n",
       " ('l', 'l'): 'll',\n",
       " ('f', 'l'): 'fl',\n",
       " ('il', 'y'): 'ily',\n",
       " ('w', 'ind'): 'wind',\n",
       " ('b', 'ag'): 'bag',\n",
       " ('t', 'r'): 'tr',\n",
       " ('go', 'od'): 'good',\n",
       " ('a', 'ss'): 'ass',\n",
       " ('un', 'der'): 'under',\n",
       " ('a', 'se'): 'ase',\n",
       " ('j', 'o'): 'jo',\n",
       " ('ch', 'e'): 'che',\n",
       " ('n', 'or'): 'nor',\n",
       " ('un', 't'): 'unt',\n",
       " ('c', 're'): 'cre',\n",
       " ('li', 'fe'): 'life',\n",
       " ('c', 'at'): 'cat',\n",
       " ('wor', 'ld'): 'world',\n",
       " ('w', 're'): 'wre',\n",
       " ('ap', 'p'): 'app',\n",
       " ('k', 'now'): 'know',\n",
       " ('poli', 'ce'): 'police',\n",
       " ('fat', 'al'): 'fatal',\n",
       " ('n', 'u'): 'nu',\n",
       " ('wre', 'ck'): 'wreck',\n",
       " ('o', 'o'): 'oo',\n",
       " ('r', 'ic'): 'ric',\n",
       " ('g', 'u'): 'gu',\n",
       " ('de', 'l'): 'del',\n",
       " ('s', 'ion'): 'sion',\n",
       " ('pl', 'ay'): 'play',\n",
       " ('row', 'n'): 'rown',\n",
       " ('w', 'ild'): 'wild',\n",
       " ('i', 're'): 'ire',\n",
       " ('th', 'e'): 'the',\n",
       " ('t', 'w'): 'tw',\n",
       " ('c', 'an'): 'can',\n",
       " ('on', 'g'): 'ong',\n",
       " ('ct', 'ion'): 'ction',\n",
       " ('e', 'k'): 'ek',\n",
       " ('ac', 'c'): 'acc',\n",
       " ('exp', 'lo'): 'explo',\n",
       " ('t', 'od'): 'tod',\n",
       " ('th', 'er'): 'ther',\n",
       " ('n', 'ing'): 'ning',\n",
       " ('ev', 'ac'): 'evac',\n",
       " ('cal', 'i'): 'cali',\n",
       " ('r', 't'): 'rt',\n",
       " ('le', 'g'): 'leg',\n",
       " ('wat', 'ch'): 'watch',\n",
       " ('sur', 'vi'): 'survi',\n",
       " ('s', 'ink'): 'sink',\n",
       " ('re', 'sc'): 'resc',\n",
       " ('e', 'ar'): 'ear',\n",
       " ('evac', 'u'): 'evacu',\n",
       " ('build', 'ing'): 'building',\n",
       " ('tra', 'in'): 'train',\n",
       " ('th', 'ink'): 'think',\n",
       " ('fam', 'ily'): 'family',\n",
       " ('f', 't'): 'ft',\n",
       " ('is', 'm'): 'ism',\n",
       " ('re', 'e'): 'ree',\n",
       " ('m', 'er'): 'mer',\n",
       " ('an', 'k'): 'ank',\n",
       " ('ni', 'a'): 'nia',\n",
       " ('an', 'ge'): 'ange',\n",
       " ('ra', 'in'): 'rain',\n",
       " ('id', 'ent'): 'ident',\n",
       " ('st', 'er'): 'ster',\n",
       " ('cle', 'ar'): 'clear',\n",
       " ('re', 'st'): 'rest',\n",
       " ('cali', 'for'): 'califor',\n",
       " ('ma', 'ss'): 'mass',\n",
       " ('h', 'el'): 'hel',\n",
       " ('c', 'ur'): 'cur',\n",
       " ('j', 'e'): 'je',\n",
       " ('en', 'ce'): 'ence',\n",
       " ('col', 'l'): 'coll',\n",
       " ('ol', 'd'): 'old',\n",
       " ('d', 'it'): 'dit',\n",
       " ('califor', 'nia'): 'california',\n",
       " ('li', 've'): 'live',\n",
       " ('in', 't'): 'int',\n",
       " ('w', 'ant'): 'want',\n",
       " ('p', 'ar'): 'par',\n",
       " ('e', 'st'): 'est',\n",
       " ('j', 'ack'): 'jack',\n",
       " ('ici', 'de'): 'icide',\n",
       " ('s', 'ay'): 'say',\n",
       " ('su', 'icide'): 'suicide',\n",
       " ('o', 'us'): 'ous',\n",
       " ('vi', 'e'): 'vie',\n",
       " ('ap', 'se'): 'apse',\n",
       " ('sc', 'ream'): 'scream',\n",
       " ('r', 'un'): 'run',\n",
       " ('de', 'ath'): 'death',\n",
       " ('w', 'h'): 'wh',\n",
       " ('shi', 'ma'): 'shima',\n",
       " ('e', 'le'): 'ele',\n",
       " ('g', 'a'): 'ga',\n",
       " ('u', 'be'): 'ube',\n",
       " ('coll', 'apse'): 'collapse',\n",
       " ('li', 'ter'): 'liter',\n",
       " ('d', 'am'): 'dam',\n",
       " ('in', 'ter'): 'inter',\n",
       " ('der', 'ail'): 'derail',\n",
       " ('de', 'ad'): 'dead',\n",
       " ('ac', 'h'): 'ach',\n",
       " ('p', 'm'): 'pm',\n",
       " ('c', 'li'): 'cli',\n",
       " ('h', 'or'): 'hor',\n",
       " ('b', 'an'): 'ban',\n",
       " ('l', 't'): 'lt',\n",
       " ('f', 'ul'): 'ful',\n",
       " ('ro', 'y'): 'roy',\n",
       " ('ir', 'l'): 'irl',\n",
       " ('s', 'ol'): 'sol',\n",
       " ('c', 'r'): 'cr',\n",
       " ('b', 'la'): 'bla',\n",
       " ('l', 'ine'): 'line',\n",
       " ('ch', 'ar'): 'char',\n",
       " ('m', 'is'): 'mis',\n",
       " ('s', 'w'): 'sw',\n",
       " ('de', 'mo'): 'demo',\n",
       " ('pl', 'an'): 'plan',\n",
       " ('fe', 'el'): 'feel',\n",
       " ('s', 'ch'): 'sch',\n",
       " ('a', 'h'): 'ah',\n",
       " ('z', 'ard'): 'zard',\n",
       " ('ne', 'ar'): 'near',\n",
       " ('se', 't'): 'set',\n",
       " ('acc', 'ident'): 'accident',\n",
       " ('ur', 'y'): 'ury',\n",
       " ('re', 'ad'): 'read',\n",
       " ('me', 'm'): 'mem',\n",
       " ('a', 'ke'): 'ake',\n",
       " ('ca', 'use'): 'cause',\n",
       " ('g', 't'): 'gt',\n",
       " ('tod', 'ay'): 'today',\n",
       " ('ne', 'ed'): 'need',\n",
       " ('con', 't'): 'cont',\n",
       " ('nu', 'clear'): 'nuclear',\n",
       " ('fe', 'ct'): 'fect',\n",
       " ('he', 'ad'): 'head',\n",
       " ('th', 'under'): 'thunder',\n",
       " ('di', 'e'): 'die',\n",
       " ('ic', 'al'): 'ical',\n",
       " ('am', 'er'): 'amer',\n",
       " ('d', 'own'): 'down',\n",
       " ('w', 'om'): 'wom',\n",
       " ('m', 'p'): 'mp',\n",
       " ('n', 'ot'): 'not',\n",
       " ('b', 'us'): 'bus',\n",
       " ('m', 'ur'): 'mur',\n",
       " ('d', 'rown'): 'drown',\n",
       " ('ath', 'er'): 'ather',\n",
       " ('b', 'ig'): 'big',\n",
       " ('y', 'out'): 'yout',\n",
       " ('de', 'v'): 'dev',\n",
       " ('n', 'y'): 'ny',\n",
       " ('ri', 'ght'): 'right',\n",
       " ('al', 'k'): 'alk',\n",
       " ('fu', 'ck'): 'fuck',\n",
       " ('shi', 'p'): 'ship',\n",
       " ('dest', 'roy'): 'destroy',\n",
       " ('d', 'an'): 'dan',\n",
       " ('ke', 't'): 'ket',\n",
       " ('a', 've'): 'ave',\n",
       " ('g', 'n'): 'gn',\n",
       " ('yout', 'ube'): 'youtube',\n",
       " ('s', 'en'): 'sen',\n",
       " ('m', 'al'): 'mal',\n",
       " ('de', 'ton'): 'deton',\n",
       " ('b', 'ad'): 'bad',\n",
       " ('f', 'all'): 'fall',\n",
       " ('c', 'la'): 'cla',\n",
       " ('y', 'o'): 'yo',\n",
       " ('b', 'low'): 'blow',\n",
       " ('hi', 'ro'): 'hiro',\n",
       " ('c', 'ru'): 'cru',\n",
       " ('p', 'u'): 'pu',\n",
       " ('l', 'ate'): 'late',\n",
       " ('s', 'k'): 'sk',\n",
       " ('g', 'r'): 'gr',\n",
       " ('hiro', 'shima'): 'hiroshima',\n",
       " ('hi', 'jack'): 'hijack',\n",
       " ('i', 'sh'): 'ish',\n",
       " ('b', 'lood'): 'blood',\n",
       " ('li', 'e'): 'lie',\n",
       " ('m', 'h'): 'mh',\n",
       " ('ser', 'v'): 'serv',\n",
       " ('m', 'at'): 'mat',\n",
       " ('pl', 'ace'): 'place',\n",
       " ('st', 'e'): 'ste',\n",
       " ('d', 'r'): 'dr',\n",
       " ('b', 'ro'): 'bro',\n",
       " ('hel', 'p'): 'help',\n",
       " ('su', 'e'): 'sue',\n",
       " ('ap', 'on'): 'apon',\n",
       " ('ho', 'st'): 'host',\n",
       " ('ho', 't'): 'hot',\n",
       " ('fe', 'ar'): 'fear',\n",
       " ('an', 'ni'): 'anni',\n",
       " ('we', 'apon'): 'weapon',\n",
       " ('resc', 'ue'): 'rescue',\n",
       " ('ob', 'liter'): 'obliter',\n",
       " ('pho', 'to'): 'photo',\n",
       " ('h', 'app'): 'happ',\n",
       " ('bl', 'ack'): 'black',\n",
       " ('c', 'ent'): 'cent',\n",
       " ('se', 'e'): 'see',\n",
       " ('mur', 'der'): 'murder',\n",
       " ('b', 'c'): 'bc',\n",
       " ('d', 'u'): 'du',\n",
       " ('re', 'al'): 'real',\n",
       " ('ur', 'al'): 'ural',\n",
       " ('f', 'in'): 'fin',\n",
       " ('p', 'i'): 'pi',\n",
       " ('ele', 'ct'): 'elect',\n",
       " ('fi', 'r'): 'fir',\n",
       " ('lo', 'se'): 'lose',\n",
       " ('u', 'ge'): 'uge',\n",
       " ('inj', 'ury'): 'injury',\n",
       " ('re', 'port'): 'report',\n",
       " ('en', 'g'): 'eng',\n",
       " ('p', 'at'): 'pat',\n",
       " ('st', 're'): 'stre',\n",
       " ('qu', 'ar'): 'quar',\n",
       " ('t', 'or'): 'tor',\n",
       " ('ab', 'le'): 'able',\n",
       " ('ch', 'ange'): 'change',\n",
       " ('j', 'ap'): 'jap',\n",
       " ('ri', 'ot'): 'riot',\n",
       " ('s', 'lide'): 'slide',\n",
       " ('b', 'y'): 'by',\n",
       " ('ri', 'd'): 'rid',\n",
       " ('f', 'ind'): 'find',\n",
       " ('serv', 'ice'): 'service',\n",
       " ('sp', 'on'): 'spon',\n",
       " ('s', 'y'): 'sy',\n",
       " ('l', 'ong'): 'long',\n",
       " ('wat', 'er'): 'water',\n",
       " ('wild', 'fire'): 'wildfire',\n",
       " ('so', 'u'): 'sou',\n",
       " ('f', 'ri'): 'fri',\n",
       " ('c', 'ity'): 'city',\n",
       " ('d', 'y'): 'dy',\n",
       " ('dev', 'ast'): 'devast',\n",
       " ('d', 'ate'): 'date',\n",
       " ('ro', 'ss'): 'ross',\n",
       " ('p', 't'): 'pt',\n",
       " ('lo', 't'): 'lot',\n",
       " ('n', 'at'): 'nat',\n",
       " ('qu', 'e'): 'que',\n",
       " ('m', 'us'): 'mus',\n",
       " ('survi', 've'): 'survive',\n",
       " ('c', 'ute'): 'cute',\n",
       " ('wo', 'und'): 'wound',\n",
       " ('ca', 're'): 'care',\n",
       " ('g', 'od'): 'god',\n",
       " ('th', 'ing'): 'thing',\n",
       " ('g', 'in'): 'gin',\n",
       " ('e', 'ye'): 'eye',\n",
       " ('stor', 'y'): 'story',\n",
       " ('b', 'ab'): 'bab',\n",
       " ('lo', 'l'): 'lol',\n",
       " ('g', 'ro'): 'gro',\n",
       " ('con', 'fir'): 'confir',\n",
       " ('he', 'ar'): 'hear',\n",
       " ('sa', 've'): 'save',\n",
       " ('c', 'er'): 'cer',\n",
       " ('mis', 's'): 'miss',\n",
       " ('p', 'le'): 'ple',\n",
       " ('li', 'sh'): 'lish',\n",
       " ('explo', 'de'): 'explode',\n",
       " ('ha', 'zard'): 'hazard',\n",
       " ('wa', 've'): 'wave',\n",
       " ('b', 'ar'): 'bar',\n",
       " ('v', 'ol'): 'vol',\n",
       " ('ne', 'ss'): 'ness',\n",
       " ('evacu', 'ate'): 'evacuate',\n",
       " ('or', 'd'): 'ord',\n",
       " ('s', 'un'): 'sun',\n",
       " ('st', 'ate'): 'state',\n",
       " ('d', 'ri'): 'dri',\n",
       " ('f', 'ace'): 'face',\n",
       " ('ast', 'ro'): 'astro',\n",
       " ('wom', 'an'): 'woman',\n",
       " ('d', 'ro'): 'dro',\n",
       " ('col', 'lide'): 'collide',\n",
       " ('su', 'al'): 'sual',\n",
       " ('fatal', 'ity'): 'fatality',\n",
       " ('t', 'ry'): 'try',\n",
       " ('gre', 'at'): 'great',\n",
       " ('i', 'se'): 'ise',\n",
       " ('st', 'and'): 'stand',\n",
       " ('pl', 'ane'): 'plane',\n",
       " ('re', 'spon'): 'respon',\n",
       " ('arm', 'y'): 'army',\n",
       " ('st', 'ru'): 'stru',\n",
       " ('pan', 'ic'): 'panic',\n",
       " ('ve', 'st'): 'vest',\n",
       " ('ear', 'th'): 'earth',\n",
       " ('o', 'ol'): 'ool',\n",
       " ('m', 'y'): 'my',\n",
       " ('dam', 'age'): 'damage',\n",
       " ('st', 'op'): 'stop',\n",
       " ('ho', 'use'): 'house',\n",
       " ('or', 'y'): 'ory',\n",
       " ('si', 'gn'): 'sign',\n",
       " ('mi', 'lit'): 'milit',\n",
       " ('bomb', 'ing'): 'bombing',\n",
       " ('att', 'le'): 'attle',\n",
       " ('b', 'io'): 'bio',\n",
       " ('host', 'age'): 'hostage',\n",
       " ('ant', 'ine'): 'antine',\n",
       " ('g', 'irl'): 'girl',\n",
       " ('ni', 'ght'): 'night',\n",
       " ('th', 'ank'): 'thank',\n",
       " ('ta', 'ke'): 'take',\n",
       " ('li', 'st'): 'list',\n",
       " ('v', 'il'): 'vil',\n",
       " ('ma', 'ge'): 'mage',\n",
       " ('f', 'a'): 'fa',\n",
       " ('p', 'r'): 'pr',\n",
       " ('bo', 'y'): 'boy',\n",
       " ('quar', 'antine'): 'quarantine',\n",
       " ('fo', 'rest'): 'forest',\n",
       " ('sch', 'ool'): 'school',\n",
       " ('ad', 'o'): 'ado',\n",
       " ('s', 'al'): 'sal',\n",
       " ('le', 'ave'): 'leave',\n",
       " ('v', 'o'): 'vo',\n",
       " ('i', 'ght'): 'ight',\n",
       " ('om', 'ic'): 'omic',\n",
       " ('in', 'di'): 'indi',\n",
       " ('ther', 'n'): 'thern',\n",
       " ('cat', 'astro'): 'catastro',\n",
       " ('b', 'li'): 'bli',\n",
       " ('g', 'ra'): 'gra',\n",
       " ('a', 'in'): 'ain',\n",
       " ('f', 'ree'): 'free',\n",
       " ('hi', 'gh'): 'high',\n",
       " ('i', 'i'): 'ii',\n",
       " ('c', 'ap'): 'cap',\n",
       " ('re', 'fu'): 'refu',\n",
       " ('cru', 'sh'): 'crush',\n",
       " ('f', 'r'): 'fr',\n",
       " ('spe', 'ct'): 'spect',\n",
       " ('h', 'it'): 'hit',\n",
       " ('leg', 'ion'): 'legion',\n",
       " ('s', 'n'): 'sn',\n",
       " ('ho', 'pe'): 'hope',\n",
       " ('terror', 'ism'): 'terrorism',\n",
       " ('ca', 'sual'): 'casual',\n",
       " ('co', 'unt'): 'count',\n",
       " ('su', 'm'): 'sum',\n",
       " ('ic', 'a'): 'ica',\n",
       " ('g', 'un'): 'gun',\n",
       " ('le', 'y'): 'ley',\n",
       " ('w', 'in'): 'win',\n",
       " ('ha', 'il'): 'hail',\n",
       " ('in', 'vest'): 'invest',\n",
       " ('ci', 'al'): 'cial',\n",
       " ('l', 'ad'): 'lad',\n",
       " ('st', 'art'): 'start',\n",
       " ('ob', 'ama'): 'obama',\n",
       " ('elect', 'ro'): 'electro',\n",
       " ('s', 'mo'): 'smo',\n",
       " ('he', 'av'): 'heav',\n",
       " ('si', 'de'): 'side',\n",
       " ('d', 'a'): 'da',\n",
       " ('g', 'ame'): 'game',\n",
       " ('t', 'le'): 'tle',\n",
       " ('b', 'ul'): 'bul',\n",
       " ('h', 'il'): 'hil',\n",
       " ('po', 'st'): 'post',\n",
       " ('st', 'r'): 'str',\n",
       " ('f', 'er'): 'fer',\n",
       " ('casual', 'ty'): 'casualty',\n",
       " ('deton', 'ate'): 'detonate',\n",
       " ('n', 'er'): 'ner',\n",
       " ('k', 'id'): 'kid',\n",
       " ('b', 'ay'): 'bay',\n",
       " ('d', 'n'): 'dn',\n",
       " ('t', 'urn'): 'turn',\n",
       " ('ro', 'ad'): 'road',\n",
       " ('off', 'ici'): 'offici',\n",
       " ('lo', 'g'): 'log',\n",
       " ('gh', 'ter'): 'ghter',\n",
       " ('ar', 'ch'): 'arch',\n",
       " ('sh', 'it'): 'shit',\n",
       " ('we', 'ek'): 'week',\n",
       " ('in', 'a'): 'ina',\n",
       " ('y', 'a'): 'ya',\n",
       " ('w', 'er'): 'wer',\n",
       " ('t', 'ri'): 'tri',\n",
       " ('ot', 'e'): 'ote',\n",
       " ('nor', 'thern'): 'northern',\n",
       " ('s', 'it'): 'sit',\n",
       " ('fl', 'ame'): 'flame',\n",
       " ('h', 'ur'): 'hur',\n",
       " ('del', 'uge'): 'deluge',\n",
       " ('co', 'p'): 'cop',\n",
       " ('f', 'ol'): 'fol',\n",
       " ('ta', 'in'): 'tain',\n",
       " ('w', 'ell'): 'well',\n",
       " ('ve', 'l'): 'vel',\n",
       " ('w', 'r'): 'wr',\n",
       " ('f', 'an'): 'fan',\n",
       " ('we', 'ather'): 'weather',\n",
       " ('h', 'and'): 'hand',\n",
       " ('se', 'l'): 'sel',\n",
       " ('au', 'g'): 'aug',\n",
       " ('lo', 'ud'): 'loud',\n",
       " ('ist', 'er'): 'ister',\n",
       " ('confir', 'm'): 'confirm',\n",
       " ('legion', 'na'): 'legionna',\n",
       " ('demo', 'lish'): 'demolish',\n",
       " ('electro', 'cute'): 'electrocute',\n",
       " ('red', 'dit'): 'reddit',\n",
       " ('m', 'ay'): 'may',\n",
       " ('c', 'ross'): 'cross',\n",
       " ('le', 'ad'): 'lead',\n",
       " ('e', 'f'): 'ef',\n",
       " ('re', 'le'): 'rele',\n",
       " ('invest', 'ig'): 'investig',\n",
       " ('legionna', 'ire'): 'legionnaire',\n",
       " ('are', 'a'): 'area',\n",
       " ('rid', 'ge'): 'ridge',\n",
       " ('su', 're'): 'sure',\n",
       " ('u', 'ght'): 'ught',\n",
       " ('si', 'ren'): 'siren',\n",
       " ('s', 'm'): 'sm',\n",
       " ('p', 'ick'): 'pick',\n",
       " ('h', 'ell'): 'hell',\n",
       " ('n', 'ation'): 'nation',\n",
       " ('st', 'ar'): 'star',\n",
       " ('v', 'a'): 'va',\n",
       " ('w', 'it'): 'wit',\n",
       " ('pre', 'ss'): 'press',\n",
       " ('jap', 'an'): 'japan',\n",
       " ('co', 'ver'): 'cover',\n",
       " ('mal', 'ay'): 'malay',\n",
       " ('bomb', 'er'): 'bomber',\n",
       " ('sum', 'mer'): 'summer',\n",
       " ('v', 'al'): 'val',\n",
       " ('u', 'g'): 'ug',\n",
       " ('p', 'are'): 'pare',\n",
       " ('is', 'is'): 'isis',\n",
       " ('so', 'und'): 'sound',\n",
       " ('rele', 'ase'): 'release',\n",
       " ('at', 'omic'): 'atomic',\n",
       " ('st', 'o'): 'sto',\n",
       " ('si', 'a'): 'sia',\n",
       " ('de', 'sol'): 'desol',\n",
       " ('obliter', 'ate'): 'obliterate',\n",
       " ('f', 'la'): 'fla',\n",
       " ('fri', 'end'): 'friend',\n",
       " ('is', 'sue'): 'issue',\n",
       " ('mem', 'ber'): 'member',\n",
       " ('i', 'z'): 'iz',\n",
       " ('s', 'or'): 'sor',\n",
       " ('an', 'g'): 'ang',\n",
       " ('p', 'he'): 'phe',\n",
       " ('t', 'own'): 'town',\n",
       " ('qu', 'ake'): 'quake',\n",
       " ('co', 'un'): 'coun',\n",
       " ('fol', 'low'): 'follow',\n",
       " ('tru', 'ck'): 'truck',\n",
       " ('offici', 'al'): 'official',\n",
       " ('a', 'ut'): 'aut',\n",
       " ('t', 'om'): 'tom',\n",
       " ('al', 'low'): 'allow',\n",
       " ('inj', 'ure'): 'injure',\n",
       " ('b', 'attle'): 'battle',\n",
       " ('m', 'ad'): 'mad',\n",
       " ('at', 'or'): 'ator',\n",
       " ('ty', 'pho'): 'typho',\n",
       " ('typho', 'on'): 'typhoon',\n",
       " ('lo', 'ck'): 'lock',\n",
       " ('h', 'al'): 'hal',\n",
       " ('er', 'o'): 'ero',\n",
       " ('f', 're'): 'fre',\n",
       " ('fi', 'ght'): 'fight',\n",
       " ('fo', 'od'): 'food',\n",
       " ('f', 'un'): 'fun',\n",
       " ('ch', 'ild'): 'child',\n",
       " ('mo', 'vie'): 'movie',\n",
       " ('sp', 'ort'): 'sport',\n",
       " ('bla', 'ze'): 'blaze',\n",
       " ('h', 'ar'): 'har',\n",
       " ('m', 'or'): 'mor',\n",
       " ('cont', 'ent'): 'content',\n",
       " ('up', 'date'): 'update',\n",
       " ('ho', 'le'): 'hole',\n",
       " ('sho', 'ot'): 'shoot',\n",
       " ('il', 'li'): 'illi',\n",
       " ('re', 'at'): 'reat',\n",
       " ('b', 'it'): 'bit',\n",
       " ('me', 'd'): 'med',\n",
       " ('p', 'ri'): 'pri',\n",
       " ('b', 'all'): 'ball',\n",
       " ('ir', 'an'): 'iran',\n",
       " ('vie', 'w'): 'view',\n",
       " ('i', 'de'): 'ide',\n",
       " ('f', 'ail'): 'fail',\n",
       " ('i', 'al'): 'ial',\n",
       " ('o', 'il'): 'oil',\n",
       " ('thunder', 'storm'): 'thunderstorm',\n",
       " ('happ', 'en'): 'happen',\n",
       " ('ar', 'son'): 'arson',\n",
       " ('he', 'art'): 'heart',\n",
       " ('bo', 'ok'): 'book',\n",
       " ('ma', 'ke'): 'make',\n",
       " ('je', 'ct'): 'ject',\n",
       " ('anni', 'hil'): 'annihil',\n",
       " ('d', 'en'): 'den',\n",
       " ('o', 'f'): 'of',\n",
       " ('le', 'ss'): 'less',\n",
       " ('w', 'an'): 'wan',\n",
       " ('p', 'ic'): 'pic',\n",
       " ('as', 'on'): 'ason',\n",
       " ('ro', 'ck'): 'rock',\n",
       " ('m', 'ic'): 'mic',\n",
       " ('br', 'is'): 'bris',\n",
       " ('l', 'am'): 'lam',\n",
       " ('ri', 'st'): 'rist',\n",
       " ('ap', 'e'): 'ape',\n",
       " ('char', 'ge'): 'charge',\n",
       " ('co', 'ur'): 'cour',\n",
       " ('st', 'ri'): 'stri',\n",
       " ('che', 'ck'): 'check',\n",
       " ('j', 'ob'): 'job',\n",
       " ('ex', 't'): 'ext',\n",
       " ('p', 'ass'): 'pass',\n",
       " ('we', 'et'): 'weet',\n",
       " ('p', 'al'): 'pal',\n",
       " ('po', 'wer'): 'power',\n",
       " ('r', 'ant'): 'rant',\n",
       " ('earth', 'quake'): 'earthquake',\n",
       " ('evacu', 'ation'): 'evacuation',\n",
       " ('t', 'ur'): 'tur',\n",
       " ('s', 'and'): 'sand',\n",
       " ('lit', 'tle'): 'little',\n",
       " ('ph', 'one'): 'phone',\n",
       " ('l', 'u'): 'lu',\n",
       " ('milit', 'ary'): 'military',\n",
       " ('i', 'um'): 'ium',\n",
       " ('de', 'al'): 'deal',\n",
       " ('al', 'th'): 'alth',\n",
       " ('s', 'end'): 'send',\n",
       " ('smo', 'ke'): 'smoke',\n",
       " ('a', 'a'): 'aa',\n",
       " ('m', 'om'): 'mom',\n",
       " ('t', 'ell'): 'tell',\n",
       " ('su', 'p'): 'sup',\n",
       " ('gu', 'y'): 'guy',\n",
       " ('p', 'ra'): 'pra',\n",
       " ('an', 's'): 'ans',\n",
       " ('r', 'al'): 'ral',\n",
       " ('h', 'um'): 'hum',\n",
       " ('o', 'h'): 'oh',\n",
       " ('ine', 'ss'): 'iness',\n",
       " ('ne', 't'): 'net',\n",
       " ('com', 'm'): 'comm',\n",
       " ('se', 'arch'): 'search',\n",
       " ('ric', 'ane'): 'ricane',\n",
       " ('stru', 'ct'): 'struct',\n",
       " ('m', 'ig'): 'mig',\n",
       " ('as', 'k'): 'ask',\n",
       " ('or', 'der'): 'order',\n",
       " ('wo', 'od'): 'wood',\n",
       " ('it', 'ion'): 'ition',\n",
       " ('u', 'ble'): 'uble',\n",
       " ('ist', 'an'): 'istan',\n",
       " ('w', 's'): 'ws',\n",
       " ('ap', 'o'): 'apo',\n",
       " ('am', 'i'): 'ami',\n",
       " ('se', 'a'): 'sea',\n",
       " ('malay', 'sia'): 'malaysia',\n",
       " ('g', 'ive'): 'give',\n",
       " ('he', 'at'): 'heat',\n",
       " ('en', 'e'): 'ene',\n",
       " ('de', 'bris'): 'debris',\n",
       " ('su', 'spect'): 'suspect',\n",
       " ('vi', 'ol'): 'viol',\n",
       " ('i', 'ence'): 'ience',\n",
       " ('d', 'ra'): 'dra',\n",
       " ('ad', 'd'): 'add',\n",
       " ('ge', 'e'): 'gee',\n",
       " ('nat', 'ural'): 'natural',\n",
       " ('ver', 'e'): 'vere',\n",
       " ('mar', 'ket'): 'market',\n",
       " ('ic', 'le'): 'icle',\n",
       " ('ho', 'ur'): 'hour',\n",
       " ('dri', 've'): 'drive',\n",
       " ('b', 'al'): 'bal',\n",
       " ('go', 'ver'): 'gover',\n",
       " ('p', 'ak'): 'pak',\n",
       " ('nation', 'al'): 'national',\n",
       " ('cal', 'l'): 'call',\n",
       " ('l', 'er'): 'ler',\n",
       " ('terror', 'ist'): 'terrorist',\n",
       " ('refu', 'gee'): 'refugee',\n",
       " ('dan', 'ger'): 'danger',\n",
       " ('de', 'mon'): 'demon',\n",
       " ('mig', 'rant'): 'migrant',\n",
       " ('g', 'i'): 'gi',\n",
       " ('al', 'e'): 'ale',\n",
       " ('g', 'ue'): 'gue',\n",
       " ('un', 'ion'): 'union',\n",
       " ('v', 'or'): 'vor',\n",
       " ('y', 'r'): 'yr',\n",
       " ('bab', 'y'): 'baby',\n",
       " ('b', 'ridge'): 'bridge',\n",
       " ('op', 'en'): 'open',\n",
       " ('b', 'a'): 'ba',\n",
       " ('k', 'er'): 'ker',\n",
       " ('m', 'id'): 'mid',\n",
       " ('explo', 'sion'): 'explosion',\n",
       " ('tra', 'p'): 'trap',\n",
       " ('w', 'w'): 'ww',\n",
       " ('go', 'v'): 'gov',\n",
       " ('u', 'ma'): 'uma',\n",
       " ('p', 'ul'): 'pul',\n",
       " ('t', 'ot'): 'tot',\n",
       " ('ver', 's'): 'vers',\n",
       " ('light', 'ning'): 'lightning',\n",
       " ('hur', 'ricane'): 'hurricane',\n",
       " ('se', 'vere'): 'severe',\n",
       " ('sou', 'th'): 'south',\n",
       " ('wa', 'it'): 'wait',\n",
       " ('e', 'li'): 'eli',\n",
       " ('v', 'is'): 'vis',\n",
       " ('t', 'alk'): 'talk',\n",
       " ('ss', 'i'): 'ssi',\n",
       " ('me', 'di'): 'medi',\n",
       " ('g', 'an'): 'gan',\n",
       " ('blood', 'y'): 'bloody',\n",
       " ('sh', 'ow'): 'show',\n",
       " ('is', 'ra'): 'isra',\n",
       " ('s', 'now'): 'snow',\n",
       " ('bl', 'ue'): 'blue',\n",
       " ('sa', 'u'): 'sau',\n",
       " ('c', 'amp'): 'camp',\n",
       " ('wor', 'd'): 'word',\n",
       " ('ac', 're'): 'acre',\n",
       " ('sp', 'ill'): 'spill',\n",
       " ('devast', 'ate'): 'devastate',\n",
       " ('h', 'ard'): 'hard',\n",
       " ('te', 'st'): 'test',\n",
       " ('l', 'ar'): 'lar',\n",
       " ('per', 'son'): 'person',\n",
       " ('p', 'a'): 'pa',\n",
       " ('wh', 'ite'): 'white',\n",
       " ('l', 'ink'): 'link',\n",
       " ('un', 'g'): 'ung',\n",
       " ('ho', 'l'): 'hol',\n",
       " ('ru', 'in'): 'ruin',\n",
       " ('g', 'ot'): 'got',\n",
       " ('li', 'p'): 'lip',\n",
       " ('ton', 'ight'): 'tonight',\n",
       " ('mo', 've'): 'move',\n",
       " ('ex', 'per'): 'exper',\n",
       " ('is', 'land'): 'island',\n",
       " ('s', 'day'): 'sday',\n",
       " ('b', 'ra'): 'bra',\n",
       " ('ch', 'er'): 'cher',\n",
       " ('survi', 'vor'): 'survivor',\n",
       " ('ch', 'rist'): 'christ',\n",
       " ('a', 'way'): 'away',\n",
       " ('so', 'on'): 'soon',\n",
       " ('mo', 's'): 'mos',\n",
       " ('bus', 'iness'): 'business',\n",
       " ('li', 'm'): 'lim',\n",
       " ('ban', 'g'): 'bang',\n",
       " ('in', 'und'): 'inund',\n",
       " ('c', 'lose'): 'close',\n",
       " ('tor', 'n'): 'torn',\n",
       " ('torn', 'ado'): 'tornado',\n",
       " ('ha', 'ha'): 'haha',\n",
       " ('p', 'ast'): 'past',\n",
       " ('de', 'p'): 'dep',\n",
       " ('hal', 'f'): 'half',\n",
       " ('v', 'ic'): 'vic',\n",
       " ('min', 'ute'): 'minute',\n",
       " ('it', 'al'): 'ital',\n",
       " ('el', 'y'): 'ely',\n",
       " ('y', 'p'): 'yp',\n",
       " ('ma', 'g'): 'mag',\n",
       " ('la', 'b'): 'lab',\n",
       " ('si', 've'): 'sive',\n",
       " ('k', 'o'): 'ko',\n",
       " ('dro', 'ught'): 'drought',\n",
       " ('fail', 'ure'): 'failure',\n",
       " ('ch', 'ina'): 'china',\n",
       " ('co', 'st'): 'cost',\n",
       " ('act', 'or'): 'actor',\n",
       " ('d', 'ust'): 'dust',\n",
       " ('d', 'ent'): 'dent',\n",
       " ('s', 'ite'): 'site',\n",
       " ('we', 'st'): 'west',\n",
       " ('s', 'an'): 'san',\n",
       " ('te', 'am'): 'team',\n",
       " ('at', 'er'): 'ater',\n",
       " ('di', 'o'): 'dio',\n",
       " ('ri', 'ble'): 'rible',\n",
       " ('wa', 'ke'): 'wake',\n",
       " ('am', 'bul'): 'ambul',\n",
       " ('ambul', 'ance'): 'ambulance',\n",
       " ('w', 'alk'): 'walk',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the hyperparameter of vocabulary size\n",
    "vocab_size = 10000\n",
    "corpus = tweets_train['clean_text'].tolist()\n",
    "\n",
    "# create a BPE tokenizer object\n",
    "MyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\n",
    "\n",
    "# train BPE tokenizer with Wikipedia corpus\n",
    "MyBPE.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokennize the training and testing data\n",
    "tweets_train['tokenized_text'] = tweets_train['clean_text'].apply(MyBPE.tokenize)\n",
    "tweets_test['tokenized_text'] = tweets_test['clean_text'].apply(MyBPE.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0         [deed, reason, earthquake, allah, forgive]  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, ask, shelter, place, notify, office...  \n",
       "3  [people, receive, wildfire, evacuation, order,...  \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, light, ing, spokane, wildfire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                 Just happened a terrible car crash   \n",
       "1  Heard about #earthquake is different cities, s...   \n",
       "2  there is a forest fire at spot pond, geese are...   \n",
       "3           Apocalypse lighting. #Spokane #wildfires   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0                     [happen, terrible, car, crash]  \n",
       "1    [hear, earthquake, different, city, stay, safe]  \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...  \n",
       "3        [apocalypse, light, ing, spokane, wildfire]  \n",
       "4           [typhoon, soudelor, kill, china, taiwan]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to convert Text into Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF vectorizer to the training data\n",
    "tweets_train['tokenized_text_tfidf'] = tweets_train['tokenized_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(tweets_train['tokenized_text_tfidf'])\n",
    "y_train = tweets_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF matrix to a dense NumPy array\n",
    "dense_tfidf_matrix = X_train.todense()\n",
    "\n",
    "# Convert the dense matrix to a DataFrame\n",
    "tweets_train_TFIDF = pd.DataFrame(dense_tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "# Add the target column to the DataFrame\n",
    "tweets_train_TFIDF['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aace</th>\n",
       "      <th>aal</th>\n",
       "      <th>aan</th>\n",
       "      <th>aar</th>\n",
       "      <th>aashiqui</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonewolf</th>\n",
       "      <th>zor</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zrnf</th>\n",
       "      <th>zss</th>\n",
       "      <th>zu</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  8608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaaa  aaaaa  aaaaaa  aace  aal  aan  aar  aashiqui   ab  ...  zone  \\\n",
       "0  0.0   0.0    0.0     0.0   0.0  0.0  0.0  0.0       0.0  0.0  ...   0.0   \n",
       "1  0.0   0.0    0.0     0.0   0.0  0.0  0.0  0.0       0.0  0.0  ...   0.0   \n",
       "2  0.0   0.0    0.0     0.0   0.0  0.0  0.0  0.0       0.0  0.0  ...   0.0   \n",
       "3  0.0   0.0    0.0     0.0   0.0  0.0  0.0  0.0       0.0  0.0  ...   0.0   \n",
       "4  0.0   0.0    0.0     0.0   0.0  0.0  0.0  0.0       0.0  0.0  ...   0.0   \n",
       "\n",
       "   zonewolf  zor  zouma  zrnf  zss   zu   zy   zz  zzy  \n",
       "0       0.0  0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 8608 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse matrix with TF-IDF values, where eacg row represents a tweet and each column represents a word\n",
    "tweets_train_TFIDF.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
