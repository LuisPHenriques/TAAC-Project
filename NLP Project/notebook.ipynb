{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Custom Functions & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T23:10:35.149706Z",
     "start_time": "2023-12-29T23:10:30.436532800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import re\n",
    "import math\n",
    "import spacy \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification, \\\n",
    "                         RobertaTokenizer, RobertaForSequenceClassification, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, \\\n",
    "                         PreTrainedTokenizerFast\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Ignore RuntimeWarning and UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T23:10:37.072317200Z",
     "start_time": "2023-12-29T23:10:36.877905600Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub('http\\S*', ' ', text)\n",
    "    \n",
    "    # remove non-alphabetic\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove one character word\n",
    "    text = re.sub(\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(\"^[a-zA-Z]\\s+\", '', text)\n",
    "    \n",
    "    # replace double space to one space\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    \n",
    "    # tokenize, lemmatize, remove stop words\n",
    "    doc = nlp(text)\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def train_BPE_tokenizer(train_text_df, text_column, vocab_size=5000, min_frequency=2):\n",
    "    # Extract the text from the specified column\n",
    "    texts = train_text_df[text_column].tolist()\n",
    "\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Train the BPE tokenizer on the text\n",
    "    tokenizer.train_from_iterator(texts, vocab_size=vocab_size, min_frequency=min_frequency)\n",
    "\n",
    "    # Tokenize the text in the DataFrame and remove \"Ġ\" character\n",
    "    train_text_df[text_column + '_tokenized'] = train_text_df[text_column].apply(lambda x: [token.replace(\"Ġ\", \"\") for token in tokenizer.encode(x).tokens])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize_text(tokenizer, df, text_column):\n",
    "    # Extract the text from the specified column\n",
    "    texts = df[text_column].tolist()\n",
    "\n",
    "    # Tokenize the text in the DataFrame using the pre-trained tokenizer and remove \"Ġ\" character\n",
    "    df[text_column + '_tokenized'] = [[token.replace(\"Ġ\", \"\") for token in tokenizer.encode(text).tokens] for text in texts]\n",
    "\n",
    "\n",
    "\n",
    "def get_tfidf_matrix(df, vectorizer):\n",
    "    \n",
    "    # Convert the TF-IDF matrix to a dense NumPy array\n",
    "    matrix = df.todense()\n",
    "\n",
    "    # Convert the dense matrix to a DataFrame\n",
    "    matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# The sequences being in the formar ['word1', 'word2', 'word3', ...], preprocess it\n",
    "def string2embedding_idx(text_sequence, model):\n",
    "\n",
    "    sequence = []\n",
    "    for token in text_sequence:\n",
    "        try:\n",
    "            sequence.append(model.wv.key_to_index[token])\n",
    "        except:\n",
    "            sequence.append(2899)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def TSNE_10ClosestWords(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "            \n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity = 10)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "class TweetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, word2vec_model):\n",
    "        self.df = df\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx, -1 if self.word2vec_model == 'skipgram' else -2]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        # Convert sequence to a 1D tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Convert label to a 1D tensor (scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return sequence_tensor, label_tensor\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print('Device:', device)\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def TokenizeBERT(tokenizer, df, batch_size = 32, shuffle = True):\n",
    "    # Tokenize training data\n",
    "    encodings = tokenizer(df['clean_text'].tolist(), add_special_tokens = True, truncation = True, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    # Convert labels to PyTorch tensors\n",
    "    labels = torch.tensor(df['target'].tolist())\n",
    "\n",
    "    # Create a DataLoader for training data\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def TokenizeBERTweet(tokenizer, df, batch_size = 32, shuffle = True, tokenizer_normalizeTweet = None):\n",
    "\n",
    "    # Normalize the text\n",
    "    if tokenizer_normalizeTweet is not None:\n",
    "        normalized_tweets = df['text'].apply(lambda x: tokenizer_normalizeTweet.normalizeTweet(x))\n",
    "    else:\n",
    "        normalized_tweets = df['text'].apply(lambda x: tokenizer.normalizeTweet(x))\n",
    "\n",
    "    # Tokenize training data\n",
    "    encodings = tokenizer(normalized_tweets.tolist(), add_special_tokens = True, truncation = True, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    # Convert labels to PyTorch tensors\n",
    "    labels = torch.tensor(df['target'].tolist())\n",
    "\n",
    "    # Create a DataLoader for training data\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - loss_func (torch.nn.Module): The loss function used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_acc_hist (list): List containing training accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_fn', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predicted = (output > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in pbar:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (output > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_BERT(model, train_loader, test_loader, optimizer, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_acc_hist (list): List containing training accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(input_ids, attention_mask=attn_mask, labels=target)\n",
    "            logits = output.logits.squeeze(-1)\n",
    "\n",
    "            loss = output.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].to(device)\n",
    "                model.zero_grad()\n",
    "                output = model(input_ids, attention_mask=attn_mask, labels=target)\n",
    "                logits = output.logits.squeeze(-1)\n",
    "\n",
    "                loss = output.loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "def train_BERTweet(model, train_loader, test_loader, optimizer, scheduler, loss_func, epochs, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): The device on which the training will be performed.\n",
    "    - **kwargs: Additional arguments for customization.\n",
    "\n",
    "    Returns:\n",
    "    - train_loss_hist (list): List containing training loss values for each epoch.\n",
    "    - train_acc_hist (list): List containing training accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = kwargs.get('num_epochs', epochs)\n",
    "    loss_fn = kwargs.get('loss_func', loss_func)\n",
    "    device = kwargs.get('device', device)\n",
    "\n",
    "    train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        print('======== Training phase ========')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].float().unsqueeze(1).to(device)\n",
    "            model.zero_grad()\n",
    "            logits, bertweet_output = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            predicted = (logits > 0.0).float()\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            mini_batch_size = target.size(0)\n",
    "            accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "            # Compute F1-score\n",
    "            f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "            total += mini_batch_size\n",
    "            correct += correct_batch\n",
    "            # Append targets and predictions to the lists\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "            )\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        train_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Training Cross-Entropy Loss = {train_loss:.4f}')\n",
    "        print(f'Training Accuracy = {train_accuracy:.2f}%')\n",
    "        print(f'Training F1-Score = {train_f1_score:.2f}%')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        print('======== Validation phase ========')\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        total = 0.\n",
    "        correct = 0.\n",
    "        # Lists to store targets and predictions\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in pbar:\n",
    "                input_ids = data[0].to(device)\n",
    "                attn_mask = data[1].to(device)\n",
    "                target = data[2].float().unsqueeze(1).to(device)\n",
    "                model.zero_grad()\n",
    "                logits, bertweet_output = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "                loss = loss_fn(logits, target)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (logits > 0.0).float()\n",
    "                correct_batch = (predicted == target).sum().item()\n",
    "                mini_batch_size = target.size(0)\n",
    "                accuracy_batch = 100 * correct_batch / mini_batch_size\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1_score_batch = 100 * f1_score(target.cpu().numpy(), predicted.cpu().numpy(), average='binary')\n",
    "\n",
    "                total += mini_batch_size\n",
    "                correct += correct_batch\n",
    "                # Append targets and predictions to the lists\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Loss = {loss:.4f} | Accuracy = {accuracy_batch:.2f}% | F1-Score = {f1_score_batch:.2f}% | Batch ID = {batch_idx + 1} '\n",
    "                )\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        test_f1_score = 100 * f1_score(all_targets, all_predictions, average='binary')\n",
    "\n",
    "        print(f'Test Cross-Entropy Loss = {test_loss:.4f}')\n",
    "        print(f'Test Accuracy = {test_accuracy:.2f}%')\n",
    "        print(f'Test F1-Score = {test_f1_score:.2f}%')\n",
    "        print()\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_f1_lst.append(train_f1_score)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        test_f1_lst.append(test_f1_score)\n",
    "\n",
    "    return train_loss_lst, train_f1_lst, test_loss_lst, test_f1_lst\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings\n",
    "class CustomLSTM(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings = torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "        return output\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings\n",
    "class CustomGRU(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings = torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])  # Use the last time step's output\n",
    "        return output\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, output):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attn(output)\n",
    "\n",
    "        return torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# GRU model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomGRU_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomGRU_Attention, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        self.attention = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        attention_weights = torch.nn.functional.softmax(self.attention(output), dim = 1)\n",
    "        output = torch.sum(attention_weights * output, dim = 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and attention mechanism\n",
    "class CustomLSTM_Attention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, num_layers = 1, bidirectional = False, freeze_embeddings = True):\n",
    "        super(CustomLSTM_Attention, self).__init__()\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.attention = Attention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embedding of each token in the sequence\n",
    "        x = self.embedding(x)\n",
    "        # Apply LSTM to the sequence of embeddings\n",
    "        output, _ = self.lstm(x)\n",
    "        # Apply the attention mechanism and get attention weights\n",
    "        attn_weights = self.attention(output)\n",
    "        # Compute the weighted sum of the hidden states\n",
    "        output = torch.sum(attn_weights * output, dim = 1)\n",
    "        # Compute the logits\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# LSTM model with pre-trained Word2Vec embeddings and multi-head attention mechanism\n",
    "class CustomLSTM_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, hidden_size, output_size, dropout=0.1, num_layers=1, bidirectional=False, freeze_embeddings=True, num_heads=8):\n",
    "        super(CustomLSTM_MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embedding_dim = word2vec_model.vector_size\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=freeze_embeddings)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.sequence_size = 49\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attention = torch.nn.MultiheadAttention(embed_dim=hidden_size * (2 if bidirectional else 1), num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Fully-connected layers for classification head\n",
    "        self.fc1 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1) * self.sequence_size, hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc2 = torch.nn.Linear(hidden_size * (2 if bidirectional else 1), output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size * (2 if bidirectional else 1))\n",
    "        self.classification_head = torch.nn.Sequential(self.fc1, self.relu, self.bn, self.dropout, self.fc2)\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization\n",
    "        for layer in self.classification_head:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity = 'relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the multi-head attention\n",
    "        torch.nn.init.kaiming_uniform_(self.multihead_attention.in_proj_weight, nonlinearity = 'relu')\n",
    "\n",
    "        # Initialize the weights with the kaiming uniform initialization for the LSTM\n",
    "        for layer in self.lstm._all_weights:\n",
    "            for param_name in layer:\n",
    "                if 'weight' in param_name:\n",
    "                    torch.nn.init.kaiming_uniform_(getattr(self.lstm, param_name), nonlinearity = 'relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embedding of each token in the sequence\n",
    "        embed = self.embedding(x)\n",
    "        # Apply LSTM to the sequence of embeddings\n",
    "        hx, cx = self.lstm(embed)\n",
    "        # Apply multihead attention and get attention weights\n",
    "        attn_output, attn_weights = self.multihead_attention(hx, hx, hx)\n",
    "        # Flatten or pool the multihead attention outputs across the sequence dimension\n",
    "        flattened_output = attn_output.reshape(attn_output.size(0), -1)\n",
    "        # Compute the logits considering the weighted values (V) of the multihead attention\n",
    "        logits = self.classification_head(flattened_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.positional_encoding = torch.zeros((1, max_len, d_model))\n",
    "        self.positional_encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, embedding_dim, num_heads, hidden_dim, num_layers, max_len=512, dropout=0.1, freeze_embeddings = True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_len)\n",
    "        \n",
    "        self.transformer_encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "                                                                          d_model=embedding_dim,\n",
    "                                                                          nhead=num_heads,\n",
    "                                                                          dim_feedforward=hidden_dim,\n",
    "                                                                          dropout=dropout\n",
    "                                                                          )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(self.transformer_encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.permute(1, 0, 2)  # Change from (batch_size, seq_len, embedding_dim) to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Change back to (batch_size, seq_len, embedding_dim)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEncoderForClassification(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, embedding_dim, num_heads, hidden_dim, num_layers, max_len=512, dropout=0.1, freeze_embeddings = True, num_classes=1):\n",
    "        super(TransformerEncoderForClassification, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder = TransformerEncoder(word2vec_model, num_heads, hidden_dim, num_layers, max_len, dropout, freeze_embeddings)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = torch.nn.Linear(self.embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        \n",
    "        # Global average pooling along the sequence dimension\n",
    "        pooled_output = torch.nn.functional.adaptive_avg_pool1d(encoder_output.permute(0, 2, 1), 1).squeeze(-1)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# FCNN model to be used with the TF-IDF features\n",
    "class CustomFCNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate=0.1):\n",
    "        super(CustomFCNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.outlayer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn(self.relu(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.outlayer(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class BERTweetForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, bertweet_model, hidden_size, output_size, dropout_rate = 0.1):\n",
    "        super(BERTweetForSequenceClassification, self).__init__()\n",
    "\n",
    "        self.bertweet_model = bertweet_model\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.BatchNorm = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.dense = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Apply batch normalization to the output of the BERTweet model\n",
    "        berteet_output = self.bertweet_model(input_ids, attention_mask, output_attentions=True)\n",
    "        # Apply batch normalization to the output of the BERTweet model\n",
    "        output = self.BatchNorm(berteet_output.pooler_output)\n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        # Apply the output layer\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, berteet_output\n",
    "    \n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in pbar:\n",
    "            input_ids = data[0].to(device)\n",
    "            attn_mask = data[1].to(device)\n",
    "            target = data[2].float().flatten().to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "            predicted = (logits > 0.0).float().flatten()\n",
    "\n",
    "            predictions.extend(predicted.tolist())\n",
    "            labels.extend(target.tolist())\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "def ComputeConfusionMatrix(labels, preds):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for the predicted and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "    - labels_gt (numpy.ndarray): Numpy array containing the ground truth labels.\n",
    "    - labels_pred (numpy.ndarray): Numpy array containing the predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    confusion_table = tabulate(cm,\n",
    "                           headers = ['Predicted ' + cls for cls in ['Not a Disaster', 'Disaster']],\n",
    "                           showindex = ['Actual ' + cls for cls in ['Not a Disaster', 'Disaster']],\n",
    "                           tablefmt=\"fancy_grid\",\n",
    "                           stralign=\"center\",\n",
    "                           numalign=\"center\"\n",
    "                           )\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_table)\n",
    "\n",
    "\n",
    "def ComputeClassificationMetrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Computes the classification metrics for the predicted and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "    - labels_gt (numpy.ndarray): Numpy array containing the ground truth labels.\n",
    "    - labels_pred (numpy.ndarray): Numpy array containing the predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the classification metrics\n",
    "    accuracy = accuracy_score(labels, preds) * 100\n",
    "    precision = precision_score(labels, preds, average='binary') * 100\n",
    "    recall = recall_score(labels, preds, average='binary') * 100\n",
    "    f1 = f1_score(labels, preds, average='binary') * 100\n",
    "\n",
    "    results_table = [\n",
    "            [\"Accuracy\", f\"{accuracy:.2f}%\"],\n",
    "            [\"Weighted Precision\", f\"{precision:.2f}%\"],\n",
    "            [\"Weighted Recall\", f\"{recall:.2f}%\"],\n",
    "            [\"Weighted F1 Score\", f\"{f1:.2f}%\"],\n",
    "        ]\n",
    "\n",
    "    print(tabulate(results_table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\", stralign=\"center\", numalign=\"center\"))\n",
    "\n",
    "\n",
    "def ComputeAttentionMaps(dataset, model, input_id, num_head):\n",
    "\n",
    "    device = set_device()\n",
    "\n",
    "    # Get all input ids and attention masks from the test dataset\n",
    "    inputs = torch.stack([data[0] for data in dataset]).to(device)\n",
    "    attn_masks = torch.stack([data[1] for data in dataset]).to(device)\n",
    "\n",
    "    logits, berteet_output = model(inputs[input_id].unsqueeze(0), attention_mask=attn_masks[input_id].unsqueeze(0))\n",
    "\n",
    "    attn_weights = berteet_output.attentions\n",
    "    # convert tuple to torch tensor\n",
    "    attn_weights = torch.stack(attn_weights).squeeze(1)\n",
    "    attn_weights = attn_weights[num_head, num_head, :, :].cpu().detach().numpy()\n",
    "\n",
    "    len_seq = sum(attn_masks[input_id]).item()\n",
    "    attention_layer_head = attn_weights[1:len_seq-1, 1:len_seq-1]\n",
    "\n",
    "    sequence = inputs[input_id][1:len_seq-1]\n",
    "    # Get the tokens from the sequence\n",
    "    tokens = BERTweetbase_tokenizer.convert_ids_to_tokens(sequence.cpu().numpy())\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(attention_layer_head, cmap=\"binary\", xticklabels=tokens, yticklabels=tokens, annot=False, fmt=\".2f\", cbar=False)\n",
    "    plt.yticks(rotation=0)\n",
    "    ax.xaxis.tick_top() # x axis on top\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:48:51.252105700Z",
     "start_time": "2023-12-29T16:48:49.728914500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                 Just happened a terrible car crash       1\n",
       "1  Heard about #earthquake is different cities, s...       1\n",
       "2  there is a forest fire at spot pond, geese are...       1\n",
       "3           Apocalypse lighting. #Spokane #wildfires       1\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_train = pd.read_csv('../data/tweets_data/train.csv')[['text', 'target']].reset_index(drop=True)\n",
    "tweets_test = pd.read_csv('../data/tweets_data/test.csv')[['id', 'text']]\n",
    "tweets_labels = pd.read_csv('../data/tweets_data/test_labels.csv', encoding='latin-1')[['choose_one', 'text']]\n",
    "\n",
    "tweets_labels['target'] = (tweets_labels['choose_one']=='Relevant').astype(int)\n",
    "tweets_labels['id'] = tweets_labels.index\n",
    "\n",
    "tweets_test = pd.merge(left = tweets_test, right = tweets_labels, on='id', how = 'left')[['id', 'text_x', 'target']]\n",
    "tweets_test.rename(columns={'text_x': 'text'}, inplace=True)\n",
    "tweets_test = tweets_test[['text', 'target']]\n",
    "\n",
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text using key-words and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.782176300Z",
     "start_time": "2023-12-21T15:27:58.466800200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_train['clean_text'] = tweets_train['text'].apply(preprocess)\n",
    "tweets_test['clean_text'] = tweets_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:29:01.798553700Z",
     "start_time": "2023-12-21T15:29:01.785391100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \n",
       "0               deed reason earthquake allah forgive  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3    people receive wildfire evacuation order cal...  \n",
       "4  got send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \n",
       "0                     happen terrible car crash  \n",
       "1      hear earthquake different city stay safe  \n",
       "2  forest fire spot pond goose flee street save  \n",
       "3          apocalypse lighting spokane wildfire  \n",
       "4            typhoon soudelor kill china taiwan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word tokenization with BERT Tokenizer for Byte-Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding():\n",
    "    def __init__(self, corpus_df, vocab_size, min_frequency, maxlen):\n",
    "        self.corpus = corpus_df.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize a tokenizer\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        # Initialize a pre-tokenizer\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        # Train the tokenizer on the corpus\n",
    "        trainer = BpeTrainer(vocab_size = self.vocab_size, min_frequency = self.min_frequency, special_tokens = [\"[PAD]\"])\n",
    "        self.tokenizer.train_from_iterator(iterator = self.corpus, trainer = trainer)\n",
    "\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object = self.tokenizer, pad_token = '[PAD]', truncation = True, padding = True)\n",
    "\n",
    "        self.tokenizer = fast_tokenizer\n",
    "\n",
    "    \n",
    "    def tokenize(self, corpus_df):\n",
    "        corpus = corpus_df.tolist()\n",
    "        input_ids = []\n",
    "        attn_masks_corpus = []\n",
    "        tokenized_corpus = []\n",
    "        for tweet in corpus:\n",
    "            # Tokenize the tweet\n",
    "            tokenized_tweet = self.tokenizer.encode_plus(tweet, max_length = self.maxlen, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            # Retrieve the input sequence\n",
    "            input_ids.append(tokenized_tweet['input_ids'])\n",
    "            # Retrieve the attention mask\n",
    "            attn_masks_corpus.append(tokenized_tweet['attention_mask'])\n",
    "            # Retrieve the tokenized tweet\n",
    "            tokenized_corpus.append(self.tokenizer.convert_ids_to_tokens(tokenized_tweet['input_ids'].squeeze()))\n",
    "            \n",
    "        return tokenized_corpus, input_ids, attn_masks_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an object for Byte Pair Encoding\n",
    "BPETokenizer = BytePairEncoding(corpus_df = tweets_train['clean_text'], vocab_size = 20000, min_frequency = 1, maxlen = 50)\n",
    "# Train the BPE tokenizer on the training data\n",
    "BPETokenizer.train()\n",
    "\n",
    "# Tokenize the training data using the pre-trained tokenizer\n",
    "tokenized_train_corpus, input_ids_train, attn_masks_train = BPETokenizer.tokenize(corpus_df = tweets_train['clean_text'])\n",
    "# Tokenize the test data using the pre-trained tokenizer\n",
    "tokenized_test_corpus, input_ids_test, attn_masks_test = BPETokenizer.tokenize(corpus_df = tweets_test['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the tokenized tweets\n",
    "tweets_train['clean_text_tokenized'] = tokenized_train_corpus\n",
    "tweets_test['clean_text_tokenized'] = tokenized_test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...  \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...  \n",
       "2  [resident, ask, shelter, place, notify, office...  \n",
       "3  [people, receive, wildfire, evacuation, order,...  \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...  \n",
       "1  [hear, earthquake, different, city, stay, safe...  \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...  \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...  \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Train the BPE tokenizer on the training data\\nTokenizer = train_BPE_tokenizer(train_text_df = tweets_train, text_column = 'clean_text', vocab_size = 20000, min_frequency = 1)\\n\\n# Assuming Tokenizer is the pre-trained tokenizer from the training data\\n# Tokenize the test data using the pre-trained tokenizer\\ntokenize_text(tokenizer = Tokenizer, df = tweets_test, text_column = 'clean_text')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Train the BPE tokenizer on the training data\n",
    "Tokenizer = train_BPE_tokenizer(train_text_df = tweets_train, text_column = 'clean_text', vocab_size = 20000, min_frequency = 1)\n",
    "\n",
    "# Assuming Tokenizer is the pre-trained tokenizer from the training data\n",
    "# Tokenize the test data using the pre-trained tokenizer\n",
    "tokenize_text(tokenizer = Tokenizer, df = tweets_test, text_column = 'clean_text')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('Training data')\\ndisplay(tweets_train.head())\\nprint()\\nprint('Testing data')\\ndisplay(tweets_test.head())\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:30:29.849421800Z",
     "start_time": "2023-12-21T15:29:11.378522300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# set the hyperparameter of vocabulary size\\nvocab_size = 3000\\ncorpus = tweets_train['clean_text'].tolist()\\n\\n# create a BPE tokenizer object\\nMyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\\n\\n# train BPE tokenizer with Wikipedia corpus\\nMyBPE.train()\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# set the hyperparameter of vocabulary size\n",
    "vocab_size = 3000\n",
    "corpus = tweets_train['clean_text'].tolist()\n",
    "\n",
    "# create a BPE tokenizer object\n",
    "MyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\n",
    "\n",
    "# train BPE tokenizer with Wikipedia corpus\n",
    "MyBPE.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:13.493640100Z",
     "start_time": "2023-12-21T15:32:21.919839300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tweets_train['tokenized_text'] = tweets_train['clean_text'].apply(lambda x: MyBPE.tokenize(x))\\ntweets_test['tokenized_text'] = tweets_test['clean_text'].apply(lambda x: MyBPE.tokenize(x))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tweets_train['tokenized_text'] = tweets_train['clean_text'].apply(lambda x: MyBPE.tokenize(x))\n",
    "tweets_test['tokenized_text'] = tweets_test['clean_text'].apply(lambda x: MyBPE.tokenize(x))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text into Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.476189500Z",
     "start_time": "2023-12-21T15:33:21.430017200Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Filter out [PAD] tokens\n",
    "filtered_tokens = tweets_train['clean_text_tokenized'].apply(lambda tokens: [token for token in tokens if token != '[PAD]'])\n",
    "X_train = vectorizer.fit_transform(filtered_tokens.apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_train['TFIDF'] = X_train.toarray().tolist()\n",
    "\n",
    "# Filter out [PAD] tokens\n",
    "filtered_tokens = tweets_test['clean_text_tokenized'].apply(lambda tokens: [token for token in tokens if token != '[PAD]'])\n",
    "X_test = vectorizer.fit_transform(filtered_tokens.apply(lambda tokens: ' '.join(tokens)))\n",
    "# Add a new column 'TFIDF' to the original DataFrame with the TF-IDF arrays\n",
    "tweets_test['TFIDF'] = X_test.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:33:23.661826500Z",
     "start_time": "2023-12-21T15:33:23.626279500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...   \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [people, receive, wildfire, evacuation, order,...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...   \n",
       "1  [hear, earthquake, different, city, stay, safe...   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...   \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (CBOW and Skip-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97446400, 97446400)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model = Word2Vec(sentences = None, vector_size = 512, window = 5, min_count = 1, workers = 4, sg = 0)\n",
    "#cbow_model.wv.key_to_index = BPETokenizer.tokenizer.get_vocab()\n",
    "cbow_model.wv.index_to_key = list(BPETokenizer.tokenizer.get_vocab().keys())\n",
    "# Set the vocabulary and initialize vectors\n",
    "cbow_model.build_vocab([list(BPETokenizer.tokenizer.get_vocab().keys())])\n",
    "cbow_model.train(corpus_iterable = tweets_train['clean_text_tokenized'].tolist(), total_examples = len(tweets_train['clean_text_tokenized']), epochs = 256)\n",
    "\n",
    "skipgram_model = Word2Vec(sentences = None, vector_size = 512, window = 5, min_count = 1, workers = 4, sg = 1)\n",
    "#cbow_model.wv.key_to_index = BPETokenizer.tokenizer.get_vocab()\n",
    "skipgram_model.wv.index_to_key = list(BPETokenizer.tokenizer.get_vocab().keys())\n",
    "# Set the vocabulary and initialize vectors\n",
    "skipgram_model.build_vocab([list(BPETokenizer.tokenizer.get_vocab().keys())])\n",
    "skipgram_model.train(corpus_iterable = tweets_train['clean_text_tokenized'].tolist(), total_examples = len(tweets_train['clean_text_tokenized']), epochs = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2embedding_idx(tokens, model):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding embedding indices.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list): List of tokens.\n",
    "    - model (Word2Vec): Gensim Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of embedding indices.\n",
    "    \"\"\"\n",
    "    # Filter out tokens not in the vocabulary\n",
    "    #tokens = [token for token in tokens if token in model.wv.key_to_index]\n",
    "\n",
    "    # Convert tokens to their embedding indices\n",
    "    embedding_indices = [model.wv.key_to_index[token] for token in tokens]\n",
    "\n",
    "    return embedding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the string2embedding_idx function to create a new column\n",
    "tweets_train['CBOW_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_train['SkipGram_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))\n",
    "tweets_test['CBOW_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_test['SkipGram_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:07.773767Z",
     "start_time": "2023-12-21T15:33:29.941367700Z"
    }
   },
   "outputs": [],
   "source": [
    "'''# Train Word2Vec model\n",
    "cbow_model = Word2Vec(sentences = tweets_train['clean_text_tokenized'], vector_size = 1024, window = 5, min_count = 1, workers = 4, sg = 0, epochs = 256)\n",
    "# Add the <pad> token to the cbow_model so that the last token is the <pad> token\n",
    "cbow_model.wv.key_to_index['<pad>'] = len(cbow_model.wv)\n",
    "# Add the embedding of <pad> token to the cbow_model\n",
    "cbow_model.wv.vectors = np.append(cbow_model.wv.vectors, np.zeros((1, 1024)), axis=0)\n",
    "\n",
    "\n",
    "skipgram_model = Word2Vec(sentences = tweets_train['clean_text_tokenized'], vector_size = 1024, window = 5, min_count = 1, workers = 4, sg = 1, epochs = 256)\n",
    "# Add the <pad> token to the skipgram_model so that the last token is the <pad> token\n",
    "skipgram_model.wv.key_to_index['<pad>'] = len(skipgram_model.wv)\n",
    "# Add the embedding of <pad> token to the skipgram_model\n",
    "skipgram_model.wv.vectors = np.append(skipgram_model.wv.vectors, np.zeros((1, 1024)), axis=0)\n",
    "\n",
    "# Add the <pad> token to all the tokenized text until the length of each tokenized text is 50\n",
    "tweets_train['clean_text_tokenized'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: tokens + ['<pad>'] * (45 - len(tokens)))\n",
    "tweets_test['clean_text_tokenized'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: tokens + ['<pad>'] * (45 - len(tokens)))\n",
    "\n",
    "# Apply the string2embedding_idx function to create a new column\n",
    "tweets_train['CBOW_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_train['SkipGram_sequences'] = tweets_train['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))\n",
    "tweets_test['CBOW_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, cbow_model))\n",
    "tweets_test['SkipGram_sequences'] = tweets_test['clean_text_tokenized'].apply(lambda tokens: string2embedding_idx(tokens, skipgram_model))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:36.442786700Z",
     "start_time": "2023-12-21T15:34:36.381065300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>[deed, reason, earthquake, allah, forgive, [PA...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[10567, 17090, 17411, 12887, 6705, 12356, 1235...</td>\n",
       "      <td>[10567, 17090, 17411, 12887, 6705, 12356, 1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[12585, 107, 1867, 1096, 17855, 5734, 15899, 1...</td>\n",
       "      <td>[12585, 107, 1867, 1096, 17855, 5734, 15899, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8964, 15880, 13162, 16317, 1072, 2498, 9713, ...</td>\n",
       "      <td>[8964, 15880, 13162, 16317, 1072, 2498, 9713, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3564, 607, 13136, 9713, 9086, 10537, 12356, 1...</td>\n",
       "      <td>[3564, 607, 13136, 9713, 9086, 10537, 12356, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[6022, 8003, 12391, 8330, 6801, 6357, 13136, 1...</td>\n",
       "      <td>[6022, 8003, 12391, 8330, 6801, 6357, 13136, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0               deed reason earthquake allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3    people receive wildfire evacuation order cal...   \n",
       "4  got send photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [deed, reason, earthquake, allah, forgive, [PA...   \n",
       "1  [forest, fire, near, la, ronge, sask, canada, ...   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [people, receive, wildfire, evacuation, order,...   \n",
       "4  [got, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [10567, 17090, 17411, 12887, 6705, 12356, 1235...   \n",
       "1  [12585, 107, 1867, 1096, 17855, 5734, 15899, 1...   \n",
       "2  [8964, 15880, 13162, 16317, 1072, 2498, 9713, ...   \n",
       "3  [3564, 607, 13136, 9713, 9086, 10537, 12356, 1...   \n",
       "4  [6022, 8003, 12391, 8330, 6801, 6357, 13136, 1...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [10567, 17090, 17411, 12887, 6705, 12356, 1235...  \n",
       "1  [12585, 107, 1867, 1096, 17855, 5734, 15899, 1...  \n",
       "2  [8964, 15880, 13162, 16317, 1072, 2498, 9713, ...  \n",
       "3  [3564, 607, 13136, 9713, 9086, 10537, 12356, 1...  \n",
       "4  [6022, 8003, 12391, 8330, 6801, 6357, 13136, 1...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_tokenized</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>CBOW_sequences</th>\n",
       "      <th>SkipGram_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash, [PAD], [PAD], [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3839, 15965, 11160, 9494, 12356, 12356, 12356...</td>\n",
       "      <td>[3839, 15965, 11160, 9494, 12356, 12356, 12356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[hear, earthquake, different, city, stay, safe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8378, 17411, 4065, 4495, 6403, 3220, 12356, 1...</td>\n",
       "      <td>[8378, 17411, 4065, 4495, 6403, 3220, 12356, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "      <td>[forest, fire, spot, pond, go, ose, flee, stre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[12585, 107, 13552, 5031, 3866, 11610, 19225, ...</td>\n",
       "      <td>[12585, 107, 13552, 5031, 3866, 11610, 19225, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire, [PAD...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8688, 11255, 5438, 13136, 12356, 12356, 12356...</td>\n",
       "      <td>[8688, 11255, 5438, 13136, 12356, 12356, 12356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan, [PAD]...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[14202, 9045, 5837, 6881, 18675, 12356, 12356,...</td>\n",
       "      <td>[14202, 9045, 5837, 6881, 18675, 12356, 12356,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0                 Just happened a terrible car crash       1   \n",
       "1  Heard about #earthquake is different cities, s...       1   \n",
       "2  there is a forest fire at spot pond, geese are...       1   \n",
       "3           Apocalypse lighting. #Spokane #wildfires       1   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
       "\n",
       "                                     clean_text  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond goose flee street save   \n",
       "3          apocalypse lighting spokane wildfire   \n",
       "4            typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                clean_text_tokenized  \\\n",
       "0  [happen, terrible, car, crash, [PAD], [PAD], [...   \n",
       "1  [hear, earthquake, different, city, stay, safe...   \n",
       "2  [forest, fire, spot, pond, go, ose, flee, stre...   \n",
       "3  [apocalypse, lighting, spokane, wildfire, [PAD...   \n",
       "4  [typhoon, soudelor, kill, china, taiwan, [PAD]...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      CBOW_sequences  \\\n",
       "0  [3839, 15965, 11160, 9494, 12356, 12356, 12356...   \n",
       "1  [8378, 17411, 4065, 4495, 6403, 3220, 12356, 1...   \n",
       "2  [12585, 107, 13552, 5031, 3866, 11610, 19225, ...   \n",
       "3  [8688, 11255, 5438, 13136, 12356, 12356, 12356...   \n",
       "4  [14202, 9045, 5837, 6881, 18675, 12356, 12356,...   \n",
       "\n",
       "                                  SkipGram_sequences  \n",
       "0  [3839, 15965, 11160, 9494, 12356, 12356, 12356...  \n",
       "1  [8378, 17411, 4065, 4495, 6403, 3220, 12356, 1...  \n",
       "2  [12585, 107, 13552, 5031, 3866, 11610, 19225, ...  \n",
       "3  [8688, 11255, 5438, 13136, 12356, 12356, 12356...  \n",
       "4  [14202, 9045, 5837, 6881, 18675, 12356, 12356,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training data')\n",
    "display(tweets_train.head())\n",
    "print()\n",
    "print('Testing data')\n",
    "display(tweets_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:38.609225400Z",
     "start_time": "2023-12-21T15:34:37.441115300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAGoCAYAAADVQZiIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJxUlEQVR4nO3deVzU5f7//+eAOiDCKJgMKIopLkTmQhpaiZZGeiyrU7mlZlnu0UZ56hdgqalpnY+ebDmlttqiaR49Lrkv5YJiuWRHQ8WEyCVQFBDm/fvDr5MTqJgMI28e99ttbvG+rmuueV1jOk/e21gMwzAEAAAA0/DydAEAAAAoWwQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AKZlsVg0b948T5cBAOWOgAcAAGAyBDwAAACTIeABuCq9/fbbqlu3rhwOh0v7XXfdpQEDBkiSpk+frkaNGqlatWpq2rSpPvzww4vOeejQIfXq1UuBgYHy8/NTdHS0Nm7cKEnat2+f7r77bgUHB6tGjRq68cYb9c0337g8Pzw8XOPGjdOgQYPk7++v+vXr65133nEZ88MPP6hz587y9fVVUFCQHnvsMZ08efJK3w4AuCwEPABXpfvvv19HjhzRypUrnW3Hjx/XkiVL1LdvX3311Vd64okn9PTTT2vHjh16/PHH9fDDD7uMP9/JkyfVsWNHHT58WF9//bW2b9+uhIQEZ4A8efKkunXrpm+++Ubbtm3THXfcoR49eujgwYMu80yePFnR0dHatm2bhg0bpqFDh+rHH3+UJJ06dUpxcXGqVauWNm/erC+++ELffPONRowY4aZ3CQAuwACAq9Rdd91lDBo0yLn99ttvG3a73SgsLDTat29vDB482GX8/fffb3Tr1s25Lcn46quvnM/19/c3jh49WurXj4yMNKZOnercbtCggdGvXz/ntsPhMOrUqWNMnz7dMAzDeOedd4xatWoZJ0+edI5ZuHCh4eXlZWRmZpb6dQHgSrEHD8BVq2/fvpozZ47y8/MlSR9//LF69eolb29v7d69Wx06dHAZ36FDB+3evbvEuVJTU9WqVSsFBgaW2J+bm6uEhARFRkaqZs2aqlGjhn788cdie/BatGjh/NlischutysrK0uStHv3bt1www3y8/NzqcnhcGjPnj2X/wYAwF9EwANw1erRo4ccDocWLlyo9PR0rV27Vv369XP2WywWl/GGYRRrO8fX1/eir/Xss89qzpw5Gjt2rNauXavU1FRdf/31KigocBlXtWpVl22LxeI8zHux179QOwC4AwEPwFXL19dX9957rz7++GN9+umnatKkidq0aSNJat68udatW+cyfsOGDWrevHmJc7Vo0UKpqak6duxYif1r167VwIEDdc899+j666+X3W7X/v37L6veyMhIpaamKjc319m2fv16eXl5qUmTJpc1FwBcCQIegKta3759tXDhQr3//vsue++effZZzZw5U2+99Zb+97//acqUKZo7d66eeeaZEufp3bu37Ha7evbsqfXr1+vnn3/WnDlz9O2330qSGjdurLlz5yo1NVXbt29Xnz59il3BW5pafXx8NGDAAO3YsUMrV67UyJEj9dBDDyk4OPivvwkAcJmqeLqAK+VwOHT48GH5+/tzCAQwoejoaNWqVUt79uxRjx49lJOTI0nq3LmzXn31VU2YMEGjRo1SgwYN9Oabb6p169bOMdLZK1vPbc+ZM0cvvPCCunXrpsLCQjVt2lSTJ09WTk6OxowZo+HDh6t9+/YKCgpSfHy8jh8/roKCAufzDcNQXl6ey/wOh0P5+fkur/Hcc8/pxhtvlK+vr+666y6NGzfO5TkAzv59OnHihEJDQ+Xlxf6msmYxDMPwdBFX4tChQwoLC/N0GQAA4C9IT09XvXr1PF2G6VT4PXj+/v6Szv4PEhAQ4OFqAPwVm34+pkGzNl9y3PsDblTba0u+ChZAxZKTk6OwsDDn5zjKVoUPeOcOywYEBBDwgAqqUwt/1a2zT5nZeSrpkIJFkt3mo04tGsjbi1MxADPh9Cr34KA3AI/z9rIosUekpLNh7nznthN7RBLuAKCUCHgArgpxUSGa3q+17DYfl3a7zUfT+7VWXFSIhyoDgIqnwh+iBWAecVEh6hJp16a0Y8o6kac6/j5q2zCQPXcAcJkIeACuKt5eFsU0CvJ0GQBQoXGIFgAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZTbgFv/Pjxslgsio+Pd7YZhqGkpCSFhobK19dXsbGx2rlzZ3mVBAAAYErlEvA2b96sd955Ry1atHBpnzhxoqZMmaJp06Zp8+bNstvt6tKli06cOFEeZQEAAJiS2wPeyZMn1bdvX7377ruqVauWs90wDL3xxht64YUXdO+99yoqKkqzZs3SqVOn9Mknn7i7LAAAANNye8AbPny4unfvrttvv92lPS0tTZmZmeratauzzWq1qmPHjtqwYcMF58vPz1dOTo7LAwAAAH+o4s7JZ8+era1bt2rz5s3F+jIzMyVJwcHBLu3BwcE6cODABeccP368kpOTy7ZQAAAAE3HbHrz09HQ98cQT+uijj+Tj43PBcRaLxWXbMIxibecbPXq0srOznY/09PQyqxkAAMAM3LYHLyUlRVlZWWrTpo2zraioSGvWrNG0adO0Z88eSWf35IWEhDjHZGVlFdurdz6r1Sqr1equsgEAACo8t+3Bu+222/TDDz8oNTXV+YiOjlbfvn2Vmpqqa6+9Vna7XcuWLXM+p6CgQKtXr1b79u3dVRYAACiBxWLRvHnzPF0Gyojb9uD5+/srKirKpc3Pz09BQUHO9vj4eI0bN04RERGKiIjQuHHjVL16dfXp08ddZQEAAJieWy+yuJSEhASdPn1aw4YN0/Hjx9WuXTstXbpU/v7+niwLAACgQivXrypbtWqV3njjDee2xWJRUlKSMjIylJeXp9WrVxfb6wcAgJnFxsZq1KhRSkhIUGBgoOx2u5KSkpz92dnZeuyxx1SnTh0FBASoc+fO2r59u7M/KSlJLVu21Ntvv62wsDBVr15d999/v37//XfnmM2bN6tLly6qXbu2bDabOnbsqK1bt5bjKlHe+C5aAAA8bNasWfLz89PGjRs1ceJEjRkzRsuWLZNhGOrevbsyMzO1aNEipaSkqHXr1rrtttt07Ngx5/P37t2rzz//XAsWLNDixYuVmpqq4cOHO/tPnDihAQMGaO3atfruu+8UERGhbt268c1RJubRQ7QAAEBq0aKFEhMTJUkRERGaNm2ali9fLm9vb/3www/Kyspy3kHitdde07x58/Tll1/qsccekyTl5eVp1qxZqlevniRp6tSp6t69uyZPniy73a7OnTu7vN7bb7+tWrVqafXq1frb3/5WjitFeSHgAQBQzoochjalHVPWiTzlnD6jm9rc4NIfEhKirKwspaSk6OTJkwoKCnLpP336tPbt2+fcrl+/vjPcSVJMTIwcDof27Nkju92urKwsvfTSS1qxYoV+/fVXFRUV6dSpUzp48KB7FwqPIeABAFCOFu/IUPKCXcrIzpMkZWbkKGP7r7prR4bios7eF9ZiscjhcMjhcCgkJESrVq0qNk/NmjUv+BrnvjDg3H8HDhyo3377TW+88YYaNGggq9WqmJgYFRQUlO3icNUg4AEAUE4W78jQ0I+2yvhTe25+oYZ+tFXT+7V2hjxJat26tTIzM1WlShWFh4dfcN6DBw/q8OHDCg0NlSR9++238vLyUpMmTSRJa9eu1Ztvvqlu3bpJOvttU0eOHCnTteHqwkUWAACUgyKHoeQFu4qFu/MlL9ilIscfI26//XbFxMSoZ8+eWrJkifbv368NGzboxRdf1JYtW5zjfHx8NGDAAG3fvl1r167VqFGj9MADD8hut0uSGjdurA8//FC7d+/Wxo0b1bdvX/n6+rprqbgKEPAAACgHm9KOOQ/LlsSQlJGdp01pf1wda7FYtGjRIt16660aNGiQmjRpol69emn//v0uX+vZuHFj3XvvverWrZu6du2qqKgovfnmm87+999/X8ePH1erVq300EMPadSoUapTp45b1omrg8UwjIv9MnHVy8nJkc1mU3Z2tgICAjxdDgAAJZqf+ouemJ16yXH/7NVSd7esW+p5k5KSNG/ePKWmXnruqwmf3+7FHjwAAMpBHX+fMh0HXAwBDwCActC2YaBCbD6yXKDfIinE5qO2DQPLsyyYFAEPAIBy4O1lUWKPSEkqFvLObSf2iJS314UiYMmSkpIq3OFZuB8BDwCAchIXFaLp/VrLbnM9DGu3+RS7RQpwJbgPHgAA5SguKkRdIu3Ob7Ko43/2sOzl7rkDLoaABwBAOfP2siimUdClBwJ/EYdoAQAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHjwKIvFonnz5nm6DAAATIWABwAAYDIEPLhVeHi43njjDZe2li1bKikpSeHh4ZKke+65RxaLxbktSV9//bWio6Pl4+Oj2rVr69577y2/ogEAqOAIePCYzZs3S5JmzJihjIwM5/bChQt17733qnv37tq2bZuWL1+u6OhoT5YKAECFUsXTBcBcihyGNqUdU9aJPNXx97no2GuuuUaSVLNmTdntdmf72LFj1atXLyUnJzvbbrjhBvcUDACACRHwUGYW78hQ8oJdysjOc7ZlZOdpd0bOZc2TmpqqwYMHl3V5AABUGhyiRZlYvCNDQz/a6hLuJMlhSF9sSdfiHRnOtjNnzlx0Ll9fX7fUCABAZUHAwxUrchhKXrBLRgl9XtVtKjp5TMkLdqnIYSgnJ0dpaWnO/qpVq6qoqMjlOS1atNDy5cvdXDUAAObFIVpcsU1px4rtuTvHp0EL5f6wXGmN2+rTJVbNeWeyvL29nf3h4eFavny5OnToIKvVqlq1aikxMVG33XabGjVqpF69eqmwsFD//e9/lZCQUF5LAgCgQnPrHrzp06erRYsWCggIUEBAgGJiYvTf//7X2W8YhpKSkhQaGipfX1/FxsZq586d7iwJbpB1ouRwJ0m2mx6QNSxKWV+O0ZODHlTPnj3VqFEjZ//kyZO1bNkyhYWFqVWrVpKk2NhYffHFF/r666/VsmVLde7cWRs3bnT7OgAAMAuLYRglHVkrEwsWLJC3t7caN24sSZo1a5YmTZqkbdu26brrrtOECRM0duxYzZw5U02aNNErr7yiNWvWaM+ePfL39y/Va+Tk5Mhmsyk7O1sBAQHuWgou4tt9R9X73e8uOe7TwTcpplFQOVQEALja8fntXm4NeCUJDAzUpEmTNGjQIIWGhio+Pl7PPfecJCk/P1/BwcGaMGGCHn/88VLNx/8gnlfkMHTzhBXKzM4r8Tw8iyS7zUfrnussby9LeZcHALgK8fntXuV2kUVRUZFmz56t3NxcxcTEKC0tTZmZmeratatzjNVqVceOHbVhw4YLzpOfn6+cnByXBzzL28uixB6Rks6GufOd207sEUm4AwCgnLg94P3www+qUaOGrFarhgwZoq+++kqRkZHKzMyUJAUHB7uMDw4OdvaVZPz48bLZbM5HWFiYW+tH6cRFhWh6v9ay21xvbmy3+Wh6v9aKiwrxUGUAAFQ+br+KtmnTpkpNTdXvv/+uOXPmaMCAAVq9erWz32Jx3atjGEaxtvONHj1aTz31lHM7JyeHkHeViIsKUZdIu8s3WbRtGMieOwAAypnbA161atWcF1lER0dr8+bN+uc//+k87y4zM1MhIX/s3cnKyiq2V+98VqtVVqvVvUXjL/P2snAhBQAAHlbuNzo2DEP5+flq2LCh7Ha7li1b5uwrKCjQ6tWr1b59+/IuCwAAwDTcugfvH//4h+68806FhYXpxIkTmj17tlatWqXFixfLYrEoPj5e48aNU0REhCIiIjRu3DhVr15dffr0cWdZAAAApubWgPfrr7/qoYceUkZGhmw2m1q0aKHFixerS5cukqSEhASdPn1aw4YN0/Hjx9WuXTstXbq01PfAAwAAQHHlfh+8ssZ9dAAAqHj4/Havcj8HDwAAAO5FwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAA4DLFxsYqPj7e02UAwAUR8AAAAEyGgAcAAGAyBDwAuEKLFy+WzWbTBx98oIEDB6pnz54aN26cgoODVbNmTSUnJ6uwsFDPPvusAgMDVa9ePb3//vueLhuAiRHwAOAKzJ49Ww888IA++OAD9e/fX5K0YsUKHT58WGvWrNGUKVOUlJSkv/3tb6pVq5Y2btyoIUOGaMiQIUpPT/dw9QDMioAHAJdQ5DD07b6jmp/6i77dd1TG/2t/8803NWTIEM2fP1933323c3xgYKD+7//+T02bNtWgQYPUtGlTnTp1Sv/4xz8UERGh0aNHq1q1alq/fr1nFgTA9Kp4ugAAuJot3pGh5AW7lJGd52w7dvC4dv34ubKPHdG6devUtm1bl+dcd9118vL64/fn4OBgRUVFObe9vb0VFBSkrKws9y8AQKXEHjwAuIDFOzI09KOtLuFOkgoKHcqtUU8BtYI0Y8YMGYbh0l+1alWXbYvFUmKbw+FwT+EAKj0CHgCUoMhhKHnBLhkX6K9SM0T1+r2q+fPna+TIkeVaGwBcCgEPAEqwKe1YsT13f/Z71dqaMmuu5syZw42PAVxVOAcPAEqQdeLi4e4c32vCtGLFCsXGxsrb29vNVQFA6RDwAKAEdfx9Lthn7/Oqy7jmjerq119/veD4VatWFWvbv3//lZQHABfFIVoAKEHbhoEKsfnIcoF+i6QQm4/aNgwsz7IAoFQIeABQAm8vixJ7REpSsZB3bjuxR6S8vS4UAQHAcwh4AHABcVEhmt6vtew218O1dpuPpvdrrbioEA9VBgAXxzl4AHARcVEh6hJp16a0Y8o6kac6/mcPy7LnDsDVjIAHAJfg7WVRTKMgT5cBAKXm1kO048eP14033ih/f3/VqVNHPXv21J49e1zGGIahpKQkhYaGytfXV7Gxsdq5c6c7ywIAADA1twa81atXa/jw4fruu++0bNkyFRYWqmvXrsrNzXWOmThxoqZMmaJp06Zp8+bNstvt6tKli06cOOHO0gAAAEzLYvz5SxTd6LffflOdOnW0evVq3XrrrTIMQ6GhoYqPj9dzzz0nScrPz1dwcLAmTJigxx9//JJz5uTkyGazKTs7WwEBAe5eAgAAKAN8frtXuV5Fm52dLUkKDDx736i0tDRlZmaqa9euzjFWq1UdO3bUhg0bSpwjPz9fOTk5Lg8AAAD8odwCnmEYeuqpp3TzzTcrKipKkpSZmSlJCg4OdhkbHBzs7Puz8ePHy2azOR9hYWHuLRwAAKCCKbeAN2LECH3//ff69NNPi/VZLK63GzAMo1jbOaNHj1Z2drbzkZ6e7pZ6AQAAKqpyuU3KyJEj9fXXX2vNmjWqV6+es91ut0s6uycvJOSPG4ZmZWUV26t3jtVqldVqdW/BAAAAFZhb9+AZhqERI0Zo7ty5WrFihRo2bOjS37BhQ9ntdi1btszZVlBQoNWrV6t9+/buLA0AAMC03LoHb/jw4frkk080f/58+fv7O8+rs9ls8vX1lcViUXx8vMaNG6eIiAhFRERo3Lhxql69uvr06ePO0gAAAEzLrQFv+vTpkqTY2FiX9hkzZmjgwIGSpISEBJ0+fVrDhg3T8ePH1a5dOy1dulT+/v7uLA0AAMC0yvU+eO7AfXQAAKh4+Px2r3K9Dx4AAADcj4AHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAADAZAh4AAIDJEPAAAABMhoAHAABgMgQ8AAAAkyHgAQAAmAwBDwAAwGQIeAAAACZDwAMAAHCD/fv3y2KxKDU1tdxfm4AHAAA8pnv37oqPj/doDQMHDlTPnj3LfN6wsDBlZGQoKiqqzOe+lCrl/ooAAACVgLe3t+x2u0demz14AADAY9atW6d//vOfslgsslgsCgoK0uTJk539PXv2VJUqVZSTkyNJyszMlMVi0Z49eyRJBQUFSkhIUN26deXn56d27dpp1apVzufPnDlTNWvW1JIlS9S8eXPVqFFDcXFxysjIkCQlJSVp1qxZmj9/vrOGVatWqXPnzhoxYoRLrUePHpXVatWKFSskSeHh4Ro3bpwGDRokf39/1a9fX++8845z/J8P0a5atUoWi0XLly9XdHS0qlevrvbt2zvXcs6CBQvUpk0b+fj46Nprr1VycrIKCwsv630l4AEAAI9p27atBg8erIyMDGVkZKh///7OgGYYhtauXatatWpp3bp1kqSVK1fKbreradOmkqSHH35Y69ev1+zZs/X999/r/vvvV1xcnP73v/85X+PUqVN67bXX9OGHH2rNmjU6ePCgnnnmGUnSM888owceeMAZ+jIyMtS+fXs9+uij+uSTT5Sfn++c5+OPP1ZoaKg6derkbJs8ebKio6O1bds2DRs2TEOHDtWPP/540TW/8MILmjx5srZs2aIqVapo0KBBzr4lS5aoX79+GjVqlHbt2qW3335bM2fO1NixYy/vjTUquOzsbEOSkZ2d7elSAABAKZ37/L755puNJ554wtn+9ddfGzabzSgqKjJSU1ONa665xnjyySeNZ5991jAMw3jssceMBx980DAMw9i7d69hsViMX375xWXu2267zRg9erRhGIYxY8YMQ5Kxd+9eZ/+//vUvIzg42Lk9YMAA4+6773aZIy8vzwgMDDQ+++wzZ1vLli2NpKQk53aDBg2Mfv36ObcdDodRp04dY/r06YZhGEZaWpohydi2bZthGIaxcuVKQ5LxzTffOJ+zcOFCQ5Jx+vRpwzAM45ZbbjHGjRvnUsuHH35ohISEXOTdLM6te/DWrFmjHj16KDQ0VBaLRfPmzftzuFRSUpJCQ0Pl6+ur2NhY7dy5050lAQAADytyGNr087ES+2699VadOHFC27Zt0+rVq9WxY0d16tRJq1evlnT2MGfHjh0lSVu3bpVhGGrSpIlq1KjhfKxevVr79u1zzlm9enU1atTIuR0SEqKsrKyL1mi1WtWvXz+9//77kqTU1FRt375dAwcOdBnXokUL588Wi0V2u/2Sc5//nJCQEElyPiclJUVjxoxxWc+5PZynTp266Lznc+tFFrm5ubrhhhv08MMP67777ivWP3HiRE2ZMkUzZ85UkyZN9Morr6hLly7as2eP/P393VkaAADwgMU7MpS8YJd+ySo54NlsNrVs2VKrVq3Shg0b1LlzZ91yyy1KTU3V//73P/3000+KjY2VJDkcDnl7eyslJUXe3t4u89SoUcP5c9WqVV36LBaLDMO4ZK2PPvqoWrZsqUOHDun999/XbbfdpgYNGriMKWluh8Nx0XnPf47FYnGu5dx/k5OTde+99xZ7no+PzyVrPsetAe/OO+/UnXfeWWKfYRh644039MILLzgXMWvWLAUHB+uTTz7R448/7s7SAABAOVu8I0NDP9qq86NV1apVVVRU5DIuNjZWK1eu1MaNGzVmzBjVrFlTkZGReuWVV1SnTh01b95cktSqVSsVFRUpKytLt9xyy1+uq1q1asVqkKTrr79e0dHRevfdd/XJJ59o6tSpf/k1Sqt169bas2ePGjdufEXzeOwii7S0NGVmZqpr167ONqvVqo4dO2rDhg0XfF5+fr5ycnJcHgAA4OpW5DCUvGCX/rzfrH79+tq4caP279+vI0eOyOFwKDY2VosXL5bFYlFkZKSks6Hv448/dh6elaQmTZqob9++6t+/v+bOnau0tDRt3rxZEyZM0KJFi0pdW3h4uL7//nvt2bNHR44c0ZkzZ5x9jz76qF599VUVFRXpnnvuuaL3oDReeuklffDBB0pKStLOnTu1e/duffbZZ3rxxRcvax6PBbzMzExJUnBwsEt7cHCws68k48ePl81mcz7CwsLcWicAALhym9KOKSM7r1j7qFGj5O3trcjISF1zzTU6ePCgbr31VklSx44dnYcwO3bsqKKiIpeAJ0kzZsxQ//799fTTT6tp06a66667tHHjxsvKB4MHD1bTpk0VHR2ta665RuvXr3f29e7dW1WqVFGfPn0u6xDpX3XHHXfoP//5j5YtW6Ybb7xRN910k6ZMmVLs0PClWIzSHIQuAxaLRV999ZXzTtEbNmxQhw4ddPjwYecJhtLZNzk9PV2LFy8ucZ78/HyXS5ZzcnIUFham7OxsBQQEuHUNAADgr5mf+ouemJ3q3Hbkn1L6Gw9c9Z/f6enpCg8P1+bNm9W6dWtPl1NqHvsmi3N3ds7MzHQJeFlZWcX26p3ParXKarW6vT4AAFB26vi7f+9XWTpz5owyMjL0/PPP66abbqpQ4U7y4CHahg0bym63a9myZc62goICrV69Wu3bt/dUWQAAwA3aNgxUiM1HFk8XUkrr169XgwYNlJKSorfeesvT5Vw2t+7BO3nypPbu3evcTktLU2pqqgIDA1W/fn3Fx8dr3LhxioiIUEREhMaNG6fq1aurT58+7iwLAACUM28vixJ7RGroR1srRMiLjY0t1a1UrlZuDXhbtmxx+TqPp556SpI0YMAAzZw5UwkJCTp9+rSGDRum48ePq127dlq6dCn3wAMAwITiokI0vV/r/3cfvNLftBeXr9wusnCXnJwc2Wy2q/4kTQAAcFaRw9DK7w+oS6uGfH67icfOwQMAAJWTt5dFba8N9HQZpmaagNe9e3fFx8d7ugwAAACPM03AAwAAwFkEvHJmGIYKCws9XQYAADAxUwU8h8OhhIQEBQYGym63KykpSZK0f/9+WSwWpaamOsf+/vvvslgsWrVqlSRp1apVslgsWrJkiVq1aiVfX1917txZWVlZ+u9//6vmzZsrICBAvXv31qlTf1z5k5+fr1GjRqlOnTry8fHRzTffrM2bNzv7z583OjpaVqtVa9euLY+3AwAAVFKmCnizZs2Sn5+fNm7cqIkTJ2rMmDEuN1IujaSkJE2bNk0bNmxQenq6HnjgAb3xxhv65JNPtHDhQi1btkxTp051jk9ISNCcOXM0a9Ysbd26VY0bN9Ydd9yhY8eOucybkJCg8ePHa/fu3WrRokWZrBcAAKAkpgl4OafP6PoWLZSYmKiIiAj1799f0dHRWr58+WXN88orr6hDhw5q1aqVHnnkEa1evVrTp09Xq1atdMstt+jvf/+7Vq5cKUnKzc3V9OnTNWnSJN15552KjIzUu+++K19fX7333nsu844ZM0ZdunRRo0aNFBQUVGbrBgAA+DPTBLw9mSe090ygFu/IcLaFhIQoKyvrsuY5f+9acHCwqlevrmuvvdal7dyc+/bt05kzZ9ShQwdnf9WqVdW2bVvt3r3bZd7o6OjLqgMAAOCvMk3Ak6RThdLQj7Y6Q57FYpHD4ZCX19llnn9P5zNnzpQ4R9WqVZ0/WywWl+3z5zx/PovF9UtXDMMo1ubn5/dXlgQAAHDZTBXwzklesEtFjj/C3DXXXCNJysj4Y+/e+Rdc/FWNGzdWtWrVtG7dOmfbmTNntGXLFjVv3vyK5wcAAPgr3PpdtJ5gSMrIztOmtD8ucvD19dVNN92kV199VeHh4Tpy5IhefPHFK34tPz8/DR06VM8++6wCAwNVv359TZw4UadOndIjjzxyxfMDAAD8FaYLeOdknchz2X7//fc1aNAgRUdHq2nTppo4caK6du16xa/z6quvyuFw6KGHHtKJEycUHR2tJUuWqFatWlc8NwAAwF9hMc4/Ma0CysnJkc1mU1j85/KyVne2fzr4JsU04mpVAACuRuc+v7OzsxUQEODpckzHdHvwLJLsNh+1bciXGAMAgMrJVBdZnLtuNbFHpLy9LBcdCwAAYFam2oNnt/kosUek4qJCPF0KAACAx5gm4L0/4EZ1atGAPXcAAKDSM80h2rbXBhLuAAAAZKKABwAAgLMIeAAAACZDwAOuQrGxsYqPj3f761gsFs2bN8/trwMAKF8EPKASSEpKUsuWLT1dBgCgnBDwAAAATIaAB3hYbm6u+vfvrxo1aigkJESTJ0926S8oKFBCQoLq1q0rPz8/tWvXTqtWrXL2z5w5UzVr1tS8efPUpEkT+fj4qEuXLkpPT3f2Jycna/v27bJYLLJYLJo5c6bz+UeOHNE999yj6tWrKyIiQl9//XV5LBsA4EYEPMDDnn32Wa1cuVJfffWVli5dqlWrViklJcXZ//DDD2v9+vWaPXu2vv/+e91///2Ki4vT//73P+eYU6dOaezYsZo1a5bWr1+vnJwc9erVS5L04IMP6umnn9Z1112njIwMZWRk6MEHH3Q+Nzk5WQ888IC+//57devWTX379tWxY8fK7w0AAJQ5Ah7gAUUOQ9/uO6rZG37Sv//9niZOnKQuXbro+uuv16xZs1RUVCRJ2rdvnz799FN98cUXuuWWW9SoUSM988wzuvnmmzVjxgznfGfOnNG0adMUExOjNm3aaNasWdqwYYM2bdokX19f1ahRQ1WqVJHdbpfdbpevr6/zuQMHDlTv3r3VuHFjjRs3Trm5udq0aVO5vycAgLJjmm+yACqKxTsylLxglzKy81SQ9bPOnCnQpFSHal2fobioEAUGBqpp06aSpK1bt8owDDVp0sRljvz8fAUFBTm3q1SpoujoaOd2s2bNVLNmTe3evVtt27a9aD0tWrRw/uzn5yd/f39lZWWVxVIBAB5CwAPK0eIdGRr60VYZ5xr+3w+/ncjX0I+2anq/1i7fpexwOOTt7a2UlBR5e3u7zFWjRg2XbYul+De5lNT2Z1WrVi32HIfDcenFAACuWhyiBcpJkcNQ8oJdf4Q7SVVqhUheVZT3yx5JUvKCXTpy9Jh++uknSVKrVq1UVFSkrKwsNW7c2OVht9ud8xQWFmrLli3O7T179uj3339Xs2bNJEnVqlVzHvYFAJgfAQ8oJ5vSjikjO8+lzauar2q06KLjq97Xqf2pOrD3R93zYF95eZ39q9mkSRP17dtX/fv319y5c5WWlqbNmzdrwoQJWrRokXOeqlWrauTIkdq4caO2bt2qhx9+WDfddJPz8Gx4eLjS0tKUmpqqI0eOKD8/v/wWDgAodwQ8oJxkncgrsb1Wp0HyCYvSb3Nf1q+fvahG17dRmzZtnP0zZsxQ//799fTTT6tp06a66667tHHjRoWFhTnHVK9eXc8995z69OmjmJgY+fr6avbs2c7+++67T3FxcerUqZOuueYaffrpp+5bKADA4yyGYRiXHnb1ysnJkc1mU3Z2tgICAjxdDnBB3+47qt7vfnfJcZ8OvkkxjYIuOe6cmTNnKj4+Xr///vsVVAcA5YvPb/diDx5QTto2DFSIzUcXuuzBIinE5qO2DQPLsywAgAkR8IBy4u1lUWKPSEkqFvLObSf2iJS316WvfAUA4GIIeEA5iosK0fR+rWW3+bi0220+xW6RUloDBw7k8CwAwAX3wQPKWVxUiLpE2rUp7ZiyTuSpjv/Zw7LsuQMAlJWrYg/em2++qYYNG8rHx0dt2rTR2rVrPV0S4FbeXhbFNArS3S3rKqZREOEOAFCmPB7wPvvsM8XHx+uFF17Qtm3bdMstt+jOO+/UwYMHPV0aAABAheTx26S0a9dOrVu31vTp051tzZs3V8+ePTV+/PhLPp/LrAEAqHj4/HYvj+7BKygoUEpKirp27erS3rVrV23YsKHE5+Tn5ysnJ8flAQAAgD94NOAdOXJERUVFCg4OdmkPDg5WZmZmic8ZP368bDab83H+3fwBAJcvNjZW8fHxf/n5SUlJatmyZZnVA+DKefwcPEmyWFxPMDcMo1jbOaNHj1Z2drbzkZ6eXh4lAgAAVBgevU1K7dq15e3tXWxvXVZWVrG9eudYrVZZrdbyKA8AAKBC8ugevGrVqqlNmzZatmyZS/uyZcvUvn17D1UFAJWPw+FQQkKCAgMDZbfblZSU5OzLzs7WY489pjp16iggIECdO3fW9u3bLzrfjBkz1Lx5c/n4+KhZs2Z68803nX379++XxWLR3Llz1alTJ1WvXl033HCDvv32W+eYAwcOqEePHqpVq5b8/Px03XXXadGiRWW+bsCsPH6j46eeekoPPfSQoqOjFRMTo3feeUcHDx7UkCFDPF0aAFQas2bN0lNPPaWNGzfq22+/1cCBA9WhQwfdfvvt6t69uwIDA7Vo0SLZbDa9/fbbuu222/TTTz8pMLD4dye/++67SkxM1LRp09SqVStt27ZNgwcPlp+fnwYMGOAc98ILL+i1115TRESEXnjhBfXu3Vt79+5VlSpVNHz4cBUUFGjNmjXy8/PTrl27VKNGjfJ8S4AKzeMB78EHH9TRo0c1ZswYZWRkKCoqSosWLVKDBg08XRoAVBotWrRQYmKiJCkiIkLTpk3T8uXL5e3trR9++EFZWVnO02Nee+01zZs3T19++aUee+yxYnO9/PLLmjx5su69915JUsOGDbVr1y69/fbbLgHvmWeeUffu3SVJycnJuu6667R37141a9ZMBw8e1H333afrr79eknTttde6df2A2Xg84EnSsGHDNGzYME+XAQCVRpHDcH5dXs7pM7qpzQ0u/SEhIcrKylJKSopOnjypoKAgl/7Tp09r3759xeb97bfflJ6erkceeUSDBw92thcWFspms7mMbdGihcvrSWfPwW7WrJlGjRqloUOHaunSpbr99tt13333uYwHcHFXRcADAJSfxTsylLxglzKy8yRJmRk5ytj+q+7akaG4qLNBy2KxyOFwyOFwKCQkRKtWrSo2T82aNYu1ORwOSWcP07Zr186lz9vb22W7atWqzp/P3Tnh3PMfffRR3XHHHVq4cKGWLl2q8ePHa/LkyRo5cuRfWzRQyRDwAKASWbwjQ0M/2qo/f4VRbn6hhn60VdP7tXaGPElq3bq1MjMzVaVKFYWHh19y/uDgYNWtW1c///yz+vbte0W1hoWFaciQIRoyZIhGjx6td999l4AHlBIBDwAqiSKHoeQFu4qFu/MlL9ilLpF25/btt9+umJgY9ezZUxMmTFDTpk11+PBhLVq0SD179lR0dHSxOZKSkjRq1CgFBATozjvvVH5+vrZs2aLjx4/rqaeeKlWt8fHxuvPOO9WkSRMdP35cK1asUPPmzS93yUClRcADgEpiU9ox52HZkhiSMrLztCntmLPNYrFo0aJFeuGFFzRo0CD99ttvstvtuvXWWy94v9JHH31U1atX16RJk5SQkCA/Pz9df/31l/VtGUVFRRo+fLgOHTqkgIAAxcXF6fXXXy/184HKzmIYxsV+mbvq8WXFAFA681N/0ROzUy857p+9WurulnXdXxAqNT6/3euq+KoyAID71fH3KdNxAK5eBDwAqCTaNgxUiM1HJX/Tt2SRFGLzUduGxW9eDKBiIeABQCXh7WVRYo9ISSoW8s5tJ/aIlLfXhSIggIqCgAcAlUhcVIim92stu831MKzd5lPsFikAKi6uogWASiYuKkRdIu3Ob7Ko43/2sCx77gDzIOABQCXk7WVRTKOgSw8EUCFxiBYAAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBk3Brwxo4dq/bt26t69eqqWbNmiWMOHjyoHj16yM/PT7Vr19aoUaNUUFDgzrIAAABMrYo7Jy8oKND999+vmJgYvffee8X6i4qK1L17d11zzTVat26djh49qgEDBsgwDE2dOtWdpQEAAJiWWwNecnKyJGnmzJkl9i9dulS7du1Senq6QkNDJUmTJ0/WwIEDNXbsWAUEBLizPAAAAFPy6Dl43377raKiopzhTpLuuOMO5efnKyUlpcTn5OfnKycnx+UBAACAP3g04GVmZio4ONilrVatWqpWrZoyMzNLfM748eNls9mcj7CwsPIoFQAAoMK47ICXlJQki8Vy0ceWLVtKPZ/FYinWZhhGie2SNHr0aGVnZzsf6enpl7sEAAAAU7vsc/BGjBihXr16XXRMeHh4qeay2+3auHGjS9vx48d15syZYnv2zrFarbJaraWaHwAAoDK67IBXu3Zt1a5du0xePCYmRmPHjlVGRoZCQkIknb3wwmq1qk2bNmXyGgAAAJWNW6+iPXjwoI4dO6aDBw+qqKhIqampkqTGjRurRo0a6tq1qyIjI/XQQw9p0qRJOnbsmJ555hkNHjyYK2gBAAD+IrcGvJdeekmzZs1ybrdq1UqStHLlSsXGxsrb21sLFy7UsGHD1KFDB/n6+qpPnz567bXX3FkWAACAqVkMwzA8XcSVyMnJkc1mU3Z2Nnv9AACoIPj8di++ixYAAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGbcFvP379+uRRx5Rw4YN5evrq0aNGikxMVEFBQUu4w4ePKgePXrIz89PtWvX1qhRo4qNAQAAQOlVcdfEP/74oxwOh95++201btxYO3bs0ODBg5Wbm6vXXntNklRUVKTu3bvrmmuu0bp163T06FENGDBAhmFo6tSp7ioNAADA1CyGYRjl9WKTJk3S9OnT9fPPP0uS/vvf/+pvf/ub0tPTFRoaKkmaPXu2Bg4cqKysLAUEBFxyzpycHNlsNmVnZ5dqPAAA8Dw+v92rXM/By87OVmBgoHP722+/VVRUlDPcSdIdd9yh/Px8paSklGdpAAAAplFuAW/fvn2aOnWqhgwZ4mzLzMxUcHCwy7hatWqpWrVqyszMLHGe/Px85eTkuDwAAEDpxcbGatSoUUpISFBgYKDsdruSkpKc/RaLRf/+9791zz33qHr16oqIiNDXX3/tMseuXbvUrVs31ahRQ8HBwXrooYd05MiRcl4JLuSyA15SUpIsFstFH1u2bHF5zuHDhxUXF6f7779fjz76qEufxWIp9hqGYZTYLknjx4+XzWZzPsLCwi53CQAAVHqzZs2Sn5+fNm7cqIkTJ2rMmDFatmyZsz85OVkPPPCAvv/+e3Xr1k19+/bVsWPHJEkZGRnq2LGjWrZsqS1btmjx4sX69ddf9cADD3hqOfiTyz4H78iRI5dM6OHh4fLx8ZF0Ntx16tRJ7dq108yZM+Xl9UemfOmllzR//nxt377d2Xb8+HEFBgZqxYoV6tSpU7G58/PzlZ+f79zOyclRWFgYx/DLUWxsrFq2bKk33nijVOMtFou++uor9ezZ0611AQBKJzY2VkVFRVq7dq2zrW3bturcubNeffVVWSwWvfjii3r55ZclSbm5ufL399eiRYsUFxenl156SRs3btSSJUuczz906JDCwsK0Z88eNWnS5JI1cA6ee132VbS1a9dW7dq1SzX2l19+UadOndSmTRvNmDHDJdxJUkxMjMaOHauMjAyFhIRIkpYuXSqr1ao2bdqUOKfVapXVar3cslGG5s6dq6pVq3q6DADAZShyGNqUdkxZJ/KUc/qMbmpzg0t/SEiIsrKynNstWrRw/uzn5yd/f39nf0pKilauXKkaNWoUe519+/aVKuDBvdx2m5TDhw8rNjZW9evX12uvvabffvvN2We32yVJXbt2VWRkpB566CFNmjRJx44d0zPPPKPBgweT5q9i518oAwC4+i3ekaHkBbuUkZ0nScrMyFHG9l91144MxUWd3cFisVjkcDicz/nzL/Ln9zscDvXo0UMTJkwo9lrndtjAs9x2kcXSpUu1d+9erVixQvXq1VNISIjzcY63t7cWLlwoHx8fdejQQQ888IB69uzpvE8erk6xsbGKj4+XdPZw/Msvv6w+ffqoRo0aCg0NLfEehkeOHLnoybqrV69W27ZtZbVaFRISoueff16FhYUur3mxE4Kls1dpP/bYY6pTp44CAgLUuXNnl8P/AFAZLd6RoaEfbXWGu3Ny8ws19KOtWrwj47LnbN26tXbu3Knw8HA1btzY5eHn51dWpeMKuC3gDRw4UIZhlPg4X/369fWf//xHp06d0tGjRzV16lQOwVYwkyZNUosWLbR161aNHj1aTz75pMuJutLFT9b95Zdf1K1bN914443avn27pk+frvfee0+vvPKKyxwXOyHYMAx1795dmZmZWrRokVJSUtS6dWvddtttztcBgMqmyGEoecEuXexk++QFu1TkuLxb4g4fPlzHjh1T7969tWnTJv38889aunSpBg0apKKioisrGmWC76LFFevQoYOef/55NWnSRCNHjtTf//53vf766y5jBg4cqN69e6tx48YaN26ccnNztWnTJknSm2++qbCwME2bNk3NmjVTz549lZycrMmTJ7scLmjRooUSExMVERGh/v37Kzo6WsuXL5ckrVy5Uj/88IO++OILRUdHKyIiQq+99ppq1qypL7/8svzeDAC4imxKO1Zsz935DEkZ2XnalHZ5vwiHhoZq/fr1Kioq0h133KGoqCg98cQTstlsxc63h2e47Rw8mMufT849f09sTEyMy9iYmJhiV9he7GTd3bt3KyYmxuXWOB06dNDJkyd16NAh1a9fv9gckusJwSkpKTp58qSCgoJcxpw+fVr79u37i6sGgIot60TJ4c7e59Vi4+bNm+fcLukGG7///rvLdkREhObOnXvFNcI9CHi4pBJPzt1ySHde5LyNP9/H8GIn65Z038Nz/7ic336pE35DQkK0atWqYrXUrFnzIqsDAPOq4+9TpuNQcRDwcFHnTs798+9y507OPX2mSN99951L33fffadmzZqV+jUiIyM1Z84cl6C3YcMG+fv7q27duqWao3Xr1srMzFSVKlUUHh5e6tcGADNr2zBQITYfZWbnlXgenkWS3eajtg25O4LZcKAcF1Sak3N/P3VG69ev18SJE/XTTz/pX//6l7744gs98cQTpX6dYcOGKT09XSNHjtSPP/6o+fPnKzExUU899VSpz+W4/fbbFRMTo549e2rJkiXav3+/NmzYoBdffLHYN6sAQGXh7WVRYo9ISWfD3PnObSf2iJS3V8nfHoWKi4CHCyrNyblFDkMPPDxUKSkpatWqlV5++WVNnjxZd9xxR6lfp27dulq0aJE2bdqkG264QUOGDNEjjzyiF198sdRzWCwWLVq0SLfeeqsGDRqkJk2aqFevXtq/f3+x7zsGgMokLipE0/u1lt3mehjWbvPR9H6tnffBg7lc9leVXW34qhP3mZ/6i56YnXrRMYemD9KAx4bpvUmJ5VMUAOAvOf9iuTr+Zw/LenLPHZ/f7sU5eLig0p5062/lfyMAuNp5e1kU0yjo0gNhChyixQWdOzn3Qr/fWXT2H4z6QdXLsywAAHAJBDxcUGlOzp2/dpueevLJcq0LAABcHAEPF8XJuQAAVDycPIVLiosKUZdI+1V1ci4AALgwAh5KhZNzAQCoODhECwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADAAAwGQIeAACAyRDwAAAATIaABwAAYDIEPAAAAJMh4AEAAJgMAQ8AAMBkCHgAAAAmQ8ADgHIyc+ZM1axZ09NlSLq6agFQ9gh4AOAG4eHheuONNzxdBoBKioAHAGWooKDA0yUAAAEPQOVmGIYmTpyoa6+9Vr6+vrrhhhv05ZdfSpKKior0yCOPqGHDhvL19VXTpk31z3/+0+X5AwcOVM+ePTV+/HiFhoaqSZMmio2N1YEDB/Tkk0/KYrHIYrG4PGfJkiVq3ry5atSoobi4OGVkZDj7ioqK9NRTT6lmzZoKCgpSQkKCBgwYoJ49ezrHlLR3sGXLlkpKSnJuT5kyRddff738/PwUFhamYcOG6eTJkxd8H44ePaq2bdvqrrvuUl5e3kXfFwBXPwIegErtxRdf1IwZMzR9+nTt3LlTTz75pPr166fVq1fL4XCoXr16+vzzz7Vr1y699NJL+sc//qHPP//cZY7ly5dr9+7dWrZsmf7zn/9o7ty5qlevnsaMGaOMjAyXAHfq1Cm99tpr+vDDD7VmzRodPHhQzzzzjLN/8uTJev/99/Xee+9p3bp1OnbsmL766qvLXpeXl5f+7//+Tzt27NCsWbO0YsUKJSQklDj20KFDuuWWW9SsWTPNnTtXPj4+F31fAFQARgWXnZ1tSDKys7M9XQqACqCwyGFs2HvEmLftkLH8+wOGj4+PsWHDBpcxjzzyiNG7d+8Snz9s2DDjvvvuc24PGDDACA4ONvLz813GNWjQwHj99ddd2mbMmGFIMvbu3ets+9e//mUEBwc7t0NCQoxXX33VuX3mzBmjXr16xt13333RuW+44QYjMTHxguv+/PPPjaCgIJdabDabsWfPHqN+/frGyJEjDYfDYRiGYZw8efKy3xfgcvH57V5VPJwvAaDcLN6RoeQFu5SRnSdJys/4SXl5eep82+3y9vrjMGpBQYFatWolSXrrrbf073//WwcOHNDp06dVUFCgli1busx7/fXXq1q1aqWqoXr16mrUqJFzOyQkRFlZWZKk7OxsZWRkKCYmxtlfpUoVRUdHyzCMy1rrypUrNW7cOO3atUs5OTkqLCxUXl6ecnNz5efnJ0k6ffq0br75ZvXu3dvl0POuXbuUl5enLl26uMx5/vsC4OpGwANQKSzekaGhH22VS0z6f6GpZs//T2P73apbm9RxdlmtVn3++ed68sknNXnyZMXExMjf31+TJk3Sxo0bXeY+F5hKo2rVqi7bFovlssObl5dXseecOXPG+fOBAwfUrVs3DRkyRC+//LICAwO1bt06PfLIIy7jrFarbr/9di1cuFDPPvus6tWrJ0lyOBySpIULF6pu3bour2O1Wi+rVgCeQcADYHpFDkPJC3bpzzGqalCY5F1VhTm/6d3tpzUgrpHLnryJEyeqffv2GjZsmLNt3759pXrNatWqqaio6LLqtNlsCgkJ0Xfffadbb71VklRYWKiUlBS1bt3aOe6aa65xOa8vJydHaWlpzu0tW7aosLBQkydPlpfX2VOt/3zeoHQ2KH744Yfq06ePOnfurFWrVik0NFSRkZGyWq06ePCgOnbseFlrAHB1qPAB79xvsTk5OR6uBMDVatPPx/RL1rES+wLa9NCx5e/qpzN5+jiqihrW9NKmTZvk5+enevXq6YMPPtDcuXMVHh6u2bNna/PmzWrQoIHz35wzZ86osLCw2L9B9erV04oVK9S9e3dZrVYFBQXp9OnTklz/vTp16pRL2+OPP67x48erbt26atq0qaZNm6bff//d5TU6dOigDz74QJ07d1bNmjU1duxYeXt7Kz8/Xzk5OQoODlZhYaEmTZqkuLg4bdy4UdOnT3e+jpeXl7OW3NxcTZ8+XYMGDVJsbKwWLlyo4OBgjRw5UvHx8crNzdVNN92kEydOON+XPn36lNUfDSqxc/8/X+4ebJSOxajg7+yhQ4cUFhbm6TIAAMBfkJ6e7jw9AGWnwgc8h8Ohw4cPy9/f3+VeUzk5OQoLC1N6eroCAgI8WGH5qqzrlirv2ivruqXKs/ahQ4cqOztbn3zyibOtsqz9zyrruiXzrd0wDJ04cUKhoaHOUwlQdir8IVovL6+LJv+AgABT/EW4XJV13VLlXXtlXbdk/rVXrVpVVapUKXGNZl/7hVTWdUvmWrvNZvN0CaZFZAYAADCZCr8HDwDMbubMmZ4uAUAFY9o9eFarVYmJiZXunk2Vdd1S5V17ZV23xNor49or67qlyr12XL4Kf5EFAAAAXJl2Dx4AAEBlRcADAAAwGQIeAACAyRDwAAAATMaUAS88PFwWi8Xl8fzzz7uMOXjwoHr06CE/Pz/Vrl1bo0aNUkFBgYcqLlv5+flq2bKlLBaLUlNTXfrMuu677rpL9evXl4+Pj0JCQvTQQw/p8OHDLmPMtvb9+/frkUceUcOGDeXr66tGjRopMTGx2JrMtu5zxo4dq/bt26t69eqqWbNmiWPMuvY333xTDRs2lI+Pj9q0aaO1a9d6uqQyt2bNGvXo0UOhoaGyWCyaN2+eS79hGEpKSlJoaKh8fX0VGxurnTt3eqbYMjR+/HjdeOON8vf3V506ddSzZ0/t2bPHZYxZ146yZcqAJ0ljxoxRRkaG8/Hiiy86+4qKitS9e3fl5uZq3bp1mj17tubMmaOnn37agxWXnYSEBIWGhhZrN/O6O3XqpM8//1x79uzRnDlztG/fPv3973939ptx7T/++KMcDofefvtt7dy5U6+//rreeust/eMf/3COMeO6zykoKND999+voUOHlthv1rV/9tlnio+P1wsvvKBt27bplltu0Z133qmDBw96urQylZubqxtuuEHTpk0rsX/ixImaMmWKpk2bps2bN8tut6tLly46ceJEOVdatlavXq3hw4fru+++07Jly1RYWKiuXbsqNzfXOcasa0cZM0yoQYMGxuuvv37B/kWLFhleXl7GL7/84mz79NNPDavVamRnZ5dDhe6zaNEio1mzZsbOnTsNSca2bdtc+sy67j+bP3++YbFYjIKCAsMwKs/aJ06caDRs2NC5XRnWPWPGDMNmsxVrN+va27ZtawwZMsSlrVmzZsbzzz/voYrcT5Lx1VdfObcdDodht9uNV1991dmWl5dn2Gw246233vJAhe6TlZVlSDJWr15tGEblWjuujGn34E2YMEFBQUFq2bKlxo4d63JY5ttvv1VUVJTLXq477rhD+fn5SklJ8US5ZeLXX3/V4MGD9eGHH6p69erF+s267j87duyYPv74Y7Vv315Vq1aVVHnWnp2drcDAQOd2ZVl3Scy49oKCAqWkpKhr164u7V27dtWGDRs8VFX5S0tLU2Zmpsv7YLVa1bFjR9O9D9nZ2ZLk/HtdmdaOK2PKgPfEE09o9uzZWrlypUaMGKE33nhDw4YNc/ZnZmYqODjY5Tm1atVStWrVlJmZWd7llgnDMDRw4EANGTJE0dHRJY4x47rP99xzz8nPz09BQUE6ePCg5s+f7+wz+9olad++fZo6daqGDBnibKsM674QM679yJEjKioqKrau4ODgCrumv+LcWs3+PhiGoaeeeko333yzoqKiJFWetePKVZiAl5SUVOzCiT8/tmzZIkl68skn1bFjR7Vo0UKPPvqo3nrrLb333ns6evSocz6LxVLsNQzDKLHdk0q77qlTpyonJ0ejR4++6HwVZd3S5f2ZS9Kzzz6rbdu2aenSpfL29lb//v1lnPdFLRVl7Ze7bkk6fPiw4uLidP/99+vRRx916aso65b+2tovpiKt/XL8uX4zrOmvMPv7MGLECH3//ff69NNPi/WZfe24clU8XUBpjRgxQr169bromPDw8BLbb7rpJknS3r17FRQUJLvdro0bN7qMOX78uM6cOVPstyJPK+26X3nlFX333XfFvqMwOjpaffv21axZsyrUuqXL/zOvXbu2ateurSZNmqh58+YKCwvTd999p5iYmAq19std9+HDh9WpUyfFxMTonXfecRlXkdYtXdnf8z+raGsvjdq1a8vb27vYnpqsrKwKu6a/wm63Szq7NyskJMTZbqb3YeTIkfr666+1Zs0a1atXz9leGdaOMuKhc//K1YIFCwxJxoEDBwzD+OPk68OHDzvHzJ49u0KffH3gwAHjhx9+cD6WLFliSDK+/PJLIz093TAMc677Qg4ePGhIMlauXGkYhnnXfujQISMiIsLo1auXUVhYWKzfrOs+36UusjDb2tu2bWsMHTrUpa158+aV8iKLCRMmONvy8/NNcaGBw+Ewhg8fboSGhho//fRTif1mXTvKlukC3oYNG4wpU6YY27ZtM37++Wfjs88+M0JDQ4277rrLOaawsNCIiooybrvtNmPr1q3GN998Y9SrV88YMWKEBysvW2lpacWuojXrujdu3GhMnTrV2LZtm7F//35jxYoVxs0332w0atTIyMvLMwzDnGv/5ZdfjMaNGxudO3c2Dh06ZGRkZDgf55hx3eccOHDA2LZtm5GcnGzUqFHD2LZtm7Ft2zbjxIkThmGYd+2zZ882qlatarz33nvGrl27jPj4eMPPz8/Yv3+/p0srUydOnHD+mUpy/rt+7hf1V1991bDZbMbcuXONH374wejdu7cREhJi5OTkeLjyKzN06FDDZrMZq1atcvk7ferUKecYs64dZct0AS8lJcVo166dYbPZDB8fH6Np06ZGYmKikZub6zLuwIEDRvfu3Q1fX18jMDDQGDFihDMMmEFJAc8wzLnu77//3ujUqZMRGBhoWK1WIzw83BgyZIhx6NAhl3FmW/uMGTMMSSU+zme2dZ8zYMCAEtd+bq+tYZh37f/617+MBg0aGNWqVTNat27tvIWGmaxcubLEP98BAwYYhnF2T1ZiYqJht9sNq9Vq3HrrrcYPP/zg2aLLwIX+Ts+YMcM5xqxrR9myGMZ5Z6EDAACgwqswV9ECAACgdAh4AAAAJkPAAwAAMBkCHgAAgMkQ8AAAAEyGgAcAAGAyBDwAAACTIeABAACYDAEPAADAZAh4AAAAJkPAAwAAMBkCHgAAgMn8/9KnwoX74rJsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'earthquake', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:39.868591200Z",
     "start_time": "2023-12-21T15:34:39.426708900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'earthquake', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:40.997210400Z",
     "start_time": "2023-12-21T15:34:40.562299400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(cbow_model, 'disaster', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:42.087371100Z",
     "start_time": "2023-12-21T15:34:41.642630900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSNE_10ClosestWords(skipgram_model, 'disaster', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Testing our Pre-Trained Embeddings to the PyTorch's Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:46.149953400Z",
     "start_time": "2023-12-21T15:34:46.078377300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-5.0731e-01,  7.5852e-02, -1.1329e-01,  ..., -4.4345e-01,\n",
       "          3.6930e-01,  7.5587e-02],\n",
       "        [ 3.1537e-02, -2.5761e-02, -6.0394e-02,  ...,  7.5270e-03,\n",
       "          1.0388e-01, -7.0228e-02],\n",
       "        [-2.8784e-01, -1.2370e-01, -1.8160e-03,  ..., -2.6632e-01,\n",
       "         -8.1540e-02,  1.0816e-01],\n",
       "        ...,\n",
       "        [-1.3160e-03, -4.5204e-04,  5.7964e-04,  ..., -8.5001e-04,\n",
       "          9.7234e-04,  1.9003e-03],\n",
       "        [-2.0624e-01, -2.0519e-02,  2.6044e-02,  ...,  8.5239e-02,\n",
       "          1.5685e-01,  7.6616e-02],\n",
       "        [-1.8469e-01,  4.5949e-02,  1.4662e-01,  ..., -3.0854e-01,\n",
       "         -1.4936e-01, -3.3686e-02]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embedding layer with pre-trained weights\n",
    "pretrained_embeddings_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(skipgram_model.wv.vectors))\n",
    "# check weights of the pre-trained embedding layer\n",
    "pretrained_embeddings_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:46.893954100Z",
     "start_time": "2023-12-21T15:34:46.867690500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10567, 17090, 17411, 12887,  6705, 12356, 12356, 12356, 12356, 12356,\n",
       "         12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356,\n",
       "         12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356,\n",
       "         12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356,\n",
       "         12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356, 12356]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainTweetsDataset = TweetsDataset(tweets_train, 'skipgram')\n",
    "TrainTweetsDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:48.356003700Z",
     "start_time": "2023-12-21T15:34:48.329539400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the first token:          tensor([-3.9371e-01, -1.1980e-01,  1.1813e-01, -2.8433e-01, -7.5813e-02,\n",
      "        -4.2028e-01,  3.1487e-01, -2.3130e-02, -2.3645e-01, -1.4021e-01,\n",
      "        -2.4358e-01, -2.4396e-01, -4.7679e-02, -3.0591e-01,  5.4786e-02,\n",
      "        -3.1441e-02, -7.2947e-01,  1.9720e-01,  3.6674e-01,  2.2637e-02,\n",
      "        -2.6352e-01, -2.6901e-01, -2.1950e-01, -2.3007e-02, -4.4575e-01,\n",
      "        -4.0512e-01, -5.1890e-02,  1.7410e-01,  1.3925e-01,  2.1854e-01,\n",
      "         1.0626e-01, -1.4575e-01,  1.7065e-01,  1.9407e-01,  4.9148e-02,\n",
      "        -1.9354e-01,  1.3097e-01,  5.3276e-02,  3.9240e-02, -1.9900e-04,\n",
      "         9.7612e-03,  4.3491e-02, -7.3982e-02,  2.5682e-02,  1.1152e-01,\n",
      "        -3.8208e-02, -2.0727e-01,  3.7191e-01,  1.2214e-01, -2.8391e-01,\n",
      "         7.0827e-02, -1.3308e-01, -4.1310e-01, -1.8449e-01, -3.9741e-01,\n",
      "        -8.3091e-02,  1.6264e-01, -2.8553e-01, -5.9275e-02, -1.4996e-02,\n",
      "         3.7002e-01,  4.0950e-01, -4.8001e-03,  2.4244e-02, -1.2979e-01,\n",
      "        -1.0511e-01, -2.6302e-01, -1.8477e-01, -9.0162e-02, -6.1414e-02,\n",
      "         4.3911e-02,  3.0256e-01,  2.4366e-02, -3.5996e-02, -3.7669e-03,\n",
      "        -5.6711e-02, -5.7067e-02,  5.1336e-01, -1.3873e-01, -3.7994e-01,\n",
      "         1.1686e-02, -2.3412e-01, -1.8529e-01,  1.2872e-01, -3.3212e-02,\n",
      "        -8.4163e-03, -3.0649e-02, -6.2348e-02, -3.1172e-02,  3.3331e-02,\n",
      "         1.1795e-01, -4.6612e-02, -3.2230e-01, -1.4410e-01,  1.5001e-01,\n",
      "        -8.2309e-02, -3.2232e-02, -5.0491e-02, -1.4001e-01, -3.1539e-01,\n",
      "        -5.4330e-02,  7.3366e-02, -8.8960e-02, -1.1886e-01,  3.4364e-01,\n",
      "        -2.5519e-01,  2.1927e-01,  6.0497e-02,  4.2019e-02,  1.3579e-01,\n",
      "        -2.4538e-01,  7.1228e-02,  1.1174e-01,  1.1585e-01,  3.7800e-02,\n",
      "         2.9486e-02, -1.3519e-01,  2.8950e-01,  1.2530e-01, -1.9786e-01,\n",
      "        -7.4543e-02, -2.4796e-02,  1.2715e-01,  3.0212e-01,  7.1372e-02,\n",
      "        -1.5228e-02,  1.6775e-01,  2.0364e-02, -7.3691e-02,  1.8218e-02,\n",
      "         1.0563e-01, -7.4879e-02,  1.3248e-01, -1.8287e-02, -4.1433e-04,\n",
      "         3.4605e-02, -1.2427e-01, -2.4629e-02,  2.7399e-01, -6.5340e-02,\n",
      "         1.1366e-01, -2.2596e-01,  1.1838e-01,  6.9051e-03,  7.4093e-02,\n",
      "         9.0145e-02,  2.0488e-01,  2.3073e-01,  3.4596e-01,  1.1953e-01,\n",
      "         1.6917e-01,  3.1729e-01,  2.7866e-01, -2.6624e-01, -1.8585e-01,\n",
      "         1.7154e-01,  5.4121e-02,  4.0046e-01, -5.8891e-02, -1.8194e-01,\n",
      "         1.3057e-01, -1.6062e-01,  3.8594e-02, -5.0015e-02,  2.8520e-01,\n",
      "        -2.8009e-01, -4.3453e-02,  3.9233e-02, -3.3118e-01, -1.6004e-01,\n",
      "         4.8436e-02,  2.0983e-01,  8.5848e-02,  1.3016e-01, -8.7658e-02,\n",
      "         2.1644e-01, -1.3878e-01, -1.9527e-01, -6.9879e-02,  1.4734e-01,\n",
      "        -1.6084e-01,  2.0154e-02,  1.9413e-01, -4.8370e-02, -1.6506e-01,\n",
      "         1.3545e-01, -2.5221e-01, -5.5176e-02,  3.9294e-02,  8.4238e-02,\n",
      "         1.7837e-01,  3.9403e-01,  8.0283e-03,  4.3885e-01, -1.3574e-01,\n",
      "        -2.2628e-01,  2.2928e-01,  1.1586e-03,  1.5932e-01,  9.3592e-03,\n",
      "         1.6090e-01, -1.6678e-01, -2.4486e-01, -3.1871e-01, -5.2609e-02,\n",
      "         7.8809e-02, -1.3223e-01, -6.7644e-02,  2.7957e-01, -1.2416e-02,\n",
      "         3.0884e-01, -9.1906e-02, -5.9404e-02, -1.9800e-01,  1.4609e-01,\n",
      "        -2.3219e-01,  1.0614e-01, -1.1528e-01, -1.3702e-02,  2.7709e-03,\n",
      "         1.2346e-02, -1.1256e-02,  1.4913e-01,  1.3421e-01,  9.3848e-02,\n",
      "        -3.1444e-01, -1.3934e-02,  2.2432e-01,  3.1908e-04,  6.2197e-02,\n",
      "         4.9416e-02,  1.4136e-01, -1.3488e-01,  3.0388e-01, -1.4064e-01,\n",
      "        -1.7100e-02, -3.6816e-01, -3.4560e-03,  1.0799e-01, -1.9603e-01,\n",
      "        -1.5230e-01, -6.0667e-02,  3.0189e-02,  2.3711e-01, -2.1275e-02,\n",
      "         7.2902e-02, -2.0988e-01, -1.6165e-02,  5.8038e-02, -3.6078e-01,\n",
      "         4.7466e-03, -5.5086e-02,  1.5866e-01, -2.1624e-01, -2.1334e-01,\n",
      "         1.5398e-01, -1.5119e-01,  9.8332e-02, -9.0171e-02, -2.6636e-01,\n",
      "         8.2899e-02, -5.4997e-02,  2.8476e-01, -9.7640e-02,  7.6473e-02,\n",
      "        -2.0659e-01,  5.3419e-02,  7.8696e-02,  8.0115e-02,  7.4026e-02,\n",
      "         3.3431e-02, -2.6063e-01,  3.4343e-02,  2.7044e-01, -4.4733e-02,\n",
      "         1.5026e-01,  1.6450e-01,  6.2991e-02, -1.2138e-01, -1.5149e-01,\n",
      "         5.1994e-02,  1.1049e-01, -8.6898e-02, -1.9140e-01,  3.1442e-01,\n",
      "        -3.0537e-02,  2.2161e-01,  1.8417e-01,  1.6173e-01, -2.8067e-01,\n",
      "         1.5584e-01,  1.2379e-01,  2.1767e-01, -1.8524e-01, -1.2053e-01,\n",
      "         8.6635e-02,  3.4654e-02,  2.8696e-01, -1.3945e-01, -3.1698e-02,\n",
      "         1.8138e-01, -3.8685e-02,  2.5850e-02, -1.4246e-01,  1.2247e-01,\n",
      "         1.0489e-01,  1.7636e-01, -1.9918e-02,  2.7086e-01,  1.1989e-01,\n",
      "         3.2286e-01, -1.9351e-01,  5.8541e-02,  2.3863e-01,  1.5153e-01,\n",
      "         8.7618e-02, -1.7820e-01,  3.4503e-02, -1.2963e-01,  3.2500e-01,\n",
      "        -1.6073e-01, -1.2833e-01, -1.7822e-01, -1.6746e-01,  8.8169e-02,\n",
      "        -1.1419e-01,  1.2569e-01, -2.7392e-01,  3.3681e-01, -1.9802e-01,\n",
      "         1.1818e-01, -1.6436e-02, -6.0319e-02, -7.4947e-02,  7.3571e-02,\n",
      "        -5.0374e-02, -1.1688e-01, -5.3500e-02,  2.1505e-01, -4.1380e-01,\n",
      "         7.9682e-02, -1.3507e-01,  3.0444e-02, -1.7380e-02,  2.2797e-03,\n",
      "        -1.3878e-01, -1.9204e-01, -3.5653e-01, -1.3982e-01, -2.7241e-02,\n",
      "        -2.7884e-02, -2.7440e-01, -8.4746e-02, -1.2606e-01, -1.0253e-01,\n",
      "         3.4218e-01, -2.1857e-01,  1.0948e-01, -9.5236e-02, -3.5051e-01,\n",
      "         8.3217e-02,  2.4193e-02,  1.3334e-01, -1.5510e-01, -3.1771e-02,\n",
      "        -1.2744e-01, -1.2896e-01,  2.4917e-02,  1.1668e-01,  1.9382e-01,\n",
      "         2.1733e-01,  3.2032e-01, -2.2035e-01, -1.5690e-01,  3.0553e-01,\n",
      "         8.6525e-02, -2.2966e-01, -4.0272e-02,  3.6639e-05,  1.4360e-01,\n",
      "        -2.1480e-03,  1.7345e-01,  2.3102e-01, -7.8231e-02,  2.6349e-01,\n",
      "        -9.5431e-03,  2.1114e-01,  4.5274e-02,  3.3064e-01, -7.8475e-02,\n",
      "         5.7151e-02,  3.4173e-01,  1.7329e-01, -2.2461e-01, -2.3126e-01,\n",
      "        -3.9186e-02,  3.5847e-02, -2.1098e-01,  8.7273e-02,  2.8787e-02,\n",
      "         2.4707e-01,  9.2340e-02,  2.2928e-01, -2.5392e-02, -1.1907e-01,\n",
      "         2.6775e-01, -8.4903e-02, -3.1108e-01, -1.6755e-01, -3.3051e-02,\n",
      "        -1.6803e-01, -1.8032e-01, -1.1589e-01, -2.7006e-01, -1.1550e-01,\n",
      "         1.0406e-01, -2.5911e-01,  6.2868e-02,  1.9997e-02,  4.7422e-03,\n",
      "         1.9301e-01, -3.5847e-01,  1.3131e-01,  2.8150e-01, -2.2066e-02,\n",
      "        -4.4261e-02, -1.7197e-01,  3.2093e-01, -4.1421e-01, -2.1327e-02,\n",
      "        -3.8961e-01, -1.8825e-01, -9.6193e-03, -1.0584e-01, -1.0978e-01,\n",
      "         1.0694e-01,  3.2672e-01,  2.4498e-01,  5.1601e-01, -1.6834e-02,\n",
      "        -1.2425e-01,  2.6505e-02,  5.9536e-02, -1.9245e-01,  3.1963e-02,\n",
      "         9.2785e-02, -3.5873e-02, -1.2895e-02, -3.9343e-01, -4.5531e-01,\n",
      "         2.2171e-01,  2.1914e-03,  4.5584e-02,  4.0518e-01, -5.4186e-02,\n",
      "        -3.9569e-01, -1.0922e-01,  2.7514e-01,  1.8855e-01,  3.7035e-01,\n",
      "        -9.3184e-02,  4.5893e-02, -1.2226e-02,  5.7800e-02,  2.9207e-01,\n",
      "         2.0733e-01, -6.7453e-02, -2.3149e-01,  1.9775e-02, -6.8451e-02,\n",
      "         1.0082e-01,  3.1432e-01, -2.7448e-01, -4.0144e-01,  3.0866e-02,\n",
      "         1.5167e-01, -7.8782e-02, -4.5688e-01,  3.9019e-01, -3.5385e-03,\n",
      "         7.0092e-01,  5.0276e-02, -2.6343e-01, -3.3500e-01,  2.8511e-01,\n",
      "        -3.8385e-01, -3.3710e-01,  4.5034e-01, -2.6188e-01, -3.9126e-01,\n",
      "        -1.8151e-01,  1.1994e-01,  7.1899e-02,  1.1003e-01, -1.6816e-01,\n",
      "         2.2369e-01,  9.0867e-03,  3.2128e-01,  3.6735e-01,  3.0539e-01,\n",
      "        -7.5525e-02,  4.1609e-02,  1.5224e-02, -6.3055e-03, -1.1414e-02,\n",
      "         4.1525e-01, -2.1688e-01, -3.6087e-02,  1.2070e-01, -1.4570e-01,\n",
      "         6.5238e-03, -4.1248e-02])\n",
      "Embedding of the second token:         tensor([-0.3469,  0.0676,  0.2920, -0.5002,  0.7767, -0.9008,  0.2419, -0.2355,\n",
      "        -0.1814,  0.3286,  0.0168, -0.4332, -0.0317,  0.2354,  0.4144, -0.3866,\n",
      "        -0.6704,  0.0651,  0.5296,  0.4631,  0.1342,  0.1827, -0.0908, -0.1758,\n",
      "         0.0287, -0.4466, -0.4293, -0.2413,  0.0793, -0.1404,  0.2673, -0.0331,\n",
      "         0.1661,  0.3112,  0.0021, -0.6212,  0.3083,  0.3195,  0.3608, -0.3620,\n",
      "         0.0520,  0.0465, -0.0082,  0.2367, -0.1339, -0.3572, -0.0460,  0.1968,\n",
      "         0.1733,  0.1476,  0.2113,  0.3547, -0.1090,  0.3144, -0.2383,  0.6438,\n",
      "         0.8062, -0.5538,  0.2733,  0.1876,  1.1043,  0.4970,  0.5150, -0.0170,\n",
      "         0.0892, -0.1259, -0.0035, -0.0344, -0.6251,  0.4360, -0.2189, -0.2122,\n",
      "        -0.3479,  0.3714, -0.1287,  0.1722, -0.0804,  0.4114,  0.0770,  0.0308,\n",
      "         0.3067, -0.2237, -0.4316,  0.1690, -0.4286, -0.9510,  0.3575, -0.9092,\n",
      "         0.8333, -0.2733, -0.0945,  0.2007,  0.6551, -0.5425, -0.2923,  0.2309,\n",
      "        -0.0437, -0.2726, -0.1162, -0.3148, -0.4982, -0.4189, -0.4257, -0.5251,\n",
      "         0.1579, -0.7910, -0.1615, -0.3701, -0.1188,  0.0172, -0.7701, -0.3344,\n",
      "        -0.3457,  0.2463,  0.2026, -0.1100, -0.1224,  0.2888,  0.3644, -0.3554,\n",
      "         0.4036,  0.6660, -0.5090,  0.0904,  0.2652,  0.2815, -0.1120, -0.1971,\n",
      "         0.0585, -0.1271, -0.1230, -0.5157,  0.1622,  0.8612,  0.5240, -0.0867,\n",
      "        -1.1714, -0.1664,  0.4006, -0.0168,  0.5157, -0.3241,  0.1384,  0.2291,\n",
      "         0.5040,  0.2175,  0.1145,  0.3384,  0.3828,  0.6251,  0.5943,  0.3804,\n",
      "         0.0135, -0.4189,  0.3100, -0.3140,  0.1281,  0.0928,  0.1984, -0.1454,\n",
      "         0.1936, -0.8756,  0.6328, -0.3388,  1.2295,  0.2617, -0.1528, -0.1474,\n",
      "        -0.5219,  0.4308,  0.7229,  0.4932,  0.1197,  0.3975, -0.4104,  0.7339,\n",
      "        -0.0955, -0.6964, -0.3557, -0.1201, -0.0190, -0.0272, -0.0783, -0.4985,\n",
      "        -0.6304, -0.0462, -0.7037,  0.2303, -0.2164, -0.6499, -0.0423,  0.6905,\n",
      "         0.0593,  0.1021, -0.0716, -0.8309, -0.0236, -0.2060, -0.2387,  0.2699,\n",
      "         0.3062, -0.4156, -0.2746, -1.1875,  0.0142,  0.5188, -0.2115, -0.1864,\n",
      "         0.8399, -0.0551,  0.4295,  0.1095,  1.1475, -0.5400,  0.4364, -0.5814,\n",
      "        -0.1868,  0.0917,  0.5270,  0.5797, -0.2036, -0.5952, -0.0116, -0.4306,\n",
      "         0.0300, -0.4887,  0.2112, -0.2127,  0.0175,  0.3924,  0.2370, -0.0851,\n",
      "        -0.2718,  0.7045, -0.3775, -0.0934, -0.4460, -0.3511, -0.0069, -0.4998,\n",
      "         0.2737, -0.1745, -0.1995,  0.2957,  0.3633,  0.1828, -0.1465,  0.4755,\n",
      "        -0.2828, -0.1703,  0.7316,  0.2707, -0.2032, -0.5346, -0.7999,  0.2289,\n",
      "        -0.6584,  0.3030, -0.7075, -0.5926, -0.4754,  0.2600,  0.0950, -0.6159,\n",
      "        -0.1297, -0.2706,  0.2338,  0.4759, -0.4677,  0.3031, -0.2260, -0.8013,\n",
      "        -0.3036,  0.2001, -0.7093,  0.5536, -0.4994, -0.3517, -0.2927, -0.5119,\n",
      "        -0.0515,  0.3982,  0.0471, -0.2502,  0.0570,  0.3561,  0.0571,  0.1230,\n",
      "         0.5299, -0.4715,  0.3391, -0.2867,  0.5172, -0.6776, -0.6060, -0.3624,\n",
      "         0.1543,  0.6476, -0.5701,  0.4806, -0.0853,  0.4818, -0.0228, -0.5472,\n",
      "         0.1549, -0.0961,  0.3589, -0.4936,  0.2601, -0.1282, -0.0610, -0.2390,\n",
      "        -0.2384,  0.0997,  0.0917,  0.4865, -0.2874,  0.5119, -0.5767, -0.0943,\n",
      "         0.2113, -0.0805, -0.3695,  0.0766,  0.2972,  0.4166, -0.2436, -0.6838,\n",
      "        -0.3590,  0.3658, -0.3697, -0.5932,  0.0286,  0.2904, -0.8370, -0.6783,\n",
      "        -0.2997,  0.3980,  0.1155, -0.4252, -0.8151,  0.1703,  0.1389,  0.1752,\n",
      "         0.3643, -0.2068, -0.2050, -0.4425,  0.3497, -0.3480, -0.0745, -0.1984,\n",
      "         0.3492, -0.7728, -0.5656,  0.4356,  0.4792, -0.0229, -0.0941, -0.0186,\n",
      "        -0.4857,  0.1903,  0.2901, -0.4335, -0.0385, -0.0745,  0.1814,  0.2202,\n",
      "        -0.0950,  0.1873,  0.4130,  0.4355, -0.0488, -0.7661,  0.3951,  0.7281,\n",
      "        -0.7628, -0.5893, -0.4123, -0.1902,  0.6947,  0.2751,  0.3844, -0.3987,\n",
      "         0.9224, -0.4019, -0.0656,  0.0763,  0.5347, -0.1967,  0.1986,  0.9248,\n",
      "         1.1375, -0.6969,  0.1604,  0.0457,  0.2358,  0.0977, -0.1480, -0.1763,\n",
      "         0.1533, -0.2250, -0.4700,  0.1557, -0.5988,  0.2091, -0.0121, -0.8547,\n",
      "        -0.5809, -0.3051, -0.2018, -0.4334, -0.3659, -0.0786, -0.5292,  0.2270,\n",
      "        -0.6211, -0.1883,  0.3341, -0.4942,  0.5829, -0.3673, -0.5281,  0.4560,\n",
      "        -0.1616,  0.5754, -0.1892, -0.1253, -0.2302, -0.1566,  0.0106, -0.3944,\n",
      "        -0.6738,  0.0839, -0.1305,  0.0973, -0.2500,  0.3967,  0.7505, -0.5102,\n",
      "        -0.3909, -0.5869, -0.0271, -0.2274, -0.0879, -0.3385, -0.5280,  0.6582,\n",
      "        -0.5126, -0.6393, -0.1669,  0.3135,  0.3865,  0.6810,  0.0296, -0.0826,\n",
      "         0.5543,  0.0821, -0.3050, -0.0143, -0.4426,  0.4875, -0.3707, -0.6304,\n",
      "         0.4387,  0.2576,  0.5348, -0.2654, -0.0259, -0.2420, -0.4128, -0.0034,\n",
      "        -1.2858, -0.6336,  0.6562,  0.0269, -0.5494, -0.7374,  0.6519, -0.0224,\n",
      "         0.9157, -0.1824, -0.6402, -0.7269,  0.7041, -0.7598, -0.2396,  0.6658,\n",
      "        -0.0066, -0.5949, -0.0722, -0.1237,  0.1080, -0.2252, -0.2095,  0.1222,\n",
      "        -0.1600, -0.1601,  0.9281,  0.6495, -0.5046, -0.0995, -0.5764,  0.2486,\n",
      "         0.0432,  0.0576, -0.0399,  0.0739,  0.2286, -0.0849,  0.6853,  0.7350])\n",
      "Embedding of the third token:          tensor([ 0.3331, -0.2620, -0.3063, -0.5439,  0.8106, -0.4460,  1.0239, -0.6775,\n",
      "        -0.4734, -0.5606, -0.3795, -0.1544, -0.3464, -0.8751, -0.5137,  0.1951,\n",
      "         0.0213, -0.6724,  0.0666, -0.4933,  0.6685,  0.0364,  0.2395,  0.1205,\n",
      "        -0.1533, -0.0316, -0.0341,  0.5771, -0.3612,  0.4541,  0.3556,  0.0157,\n",
      "         1.1209,  0.1963,  0.2112, -0.5041,  0.2594, -0.0656,  0.3035, -0.6089,\n",
      "         0.0758, -0.4115,  0.1515, -0.2266,  0.2931, -0.7615,  0.0069,  0.5591,\n",
      "         0.3641, -0.3545, -0.0189, -0.5971, -0.0730,  0.0633, -0.5460,  0.0995,\n",
      "        -0.6591, -0.4547, -0.0869, -0.2660,  0.5757,  0.7158,  0.1510, -0.5002,\n",
      "        -0.0524, -0.2304,  0.1845, -0.3542,  0.1543, -0.0710,  0.5154,  1.0097,\n",
      "        -0.1523, -0.1009,  0.8945, -0.1043, -0.0835,  0.2364,  0.3929, -0.3225,\n",
      "        -0.2808, -0.0475, -0.2264,  1.0498, -0.2833,  0.1256, -0.0264, -0.0516,\n",
      "         0.6142,  0.5408, -0.1407,  0.4647,  0.5439, -0.4959, -0.3515, -0.0633,\n",
      "        -0.6112, -0.4391, -0.1610, -0.1392, -0.6934,  0.1987, -0.1883, -0.2487,\n",
      "         0.2530, -0.2703,  0.0961, -0.1240,  0.4625,  0.2580, -0.7035, -0.2144,\n",
      "         0.2286, -0.0114, -0.2783, -0.6601, -0.0240,  0.3302,  0.3343,  0.1939,\n",
      "         0.2161,  0.0833, -0.7662, -0.5521, -0.2194, -0.3101,  0.8647, -0.1874,\n",
      "        -0.1513, -0.5095, -0.3232, -0.0719,  0.0658,  0.3446, -0.6260,  0.0280,\n",
      "        -0.2457, -0.6825,  0.4528, -0.0144,  0.2911, -0.6128,  0.0694, -0.4801,\n",
      "         0.5754,  0.0126, -0.2084,  0.7834,  0.0980, -0.1105,  0.0055,  0.7244,\n",
      "         0.5433, -0.3520, -0.1667,  0.6089,  0.9216,  0.4707,  0.4225, -0.2153,\n",
      "        -0.7032, -0.2076, -0.3525,  0.0307,  0.5283, -0.4214,  0.0522, -0.0416,\n",
      "        -0.7793, -0.5933, -0.3718,  1.2716,  0.0487, -0.3561, -0.3261,  0.7382,\n",
      "        -0.4508, -0.2626,  0.1532, -0.1941,  0.2061, -0.5317,  0.0261, -0.5156,\n",
      "        -0.8815,  0.5057, -0.1631, -0.2799,  0.3800,  0.3446,  0.4959,  0.6540,\n",
      "        -0.0427,  0.2100, -0.1214, -0.5776,  0.4401, -0.0130,  0.2312,  0.1966,\n",
      "         0.9126,  0.1927,  0.0515, -0.3712, -0.1628, -0.1941, -0.4149, -0.1727,\n",
      "         0.1532, -0.4074,  0.0277,  1.0066, -0.1065, -0.0537,  0.2684, -0.3354,\n",
      "         0.2712, -0.2990,  0.4885, -0.1801, -0.6239, -0.0439,  0.4698, -0.5002,\n",
      "        -0.0683,  0.6168,  0.2656, -0.0071,  0.0150, -0.1124, -0.4586, -0.2400,\n",
      "        -0.1953,  0.6174, -0.3787,  0.3311, -0.3980,  0.2271, -0.3037,  0.2817,\n",
      "        -0.1474, -0.2255, -0.5001, -0.3096,  0.0894,  0.5421, -0.1089, -0.6424,\n",
      "         0.0580, -0.1203,  0.1222,  0.7457,  0.0418, -0.6495, -0.1687, -0.2811,\n",
      "        -0.7522,  0.3527, -0.4241, -0.7004, -0.5621, -0.3011,  0.5176,  0.0108,\n",
      "         0.5358, -0.5472,  0.8274,  0.0959, -0.3284,  0.0293, -0.0163, -0.5545,\n",
      "         0.5192,  0.1233, -0.4879, -0.6926, -0.4156,  0.2635, -0.4502, -0.0656,\n",
      "        -0.2131, -0.0268, -0.5123, -0.8367,  0.3606, -0.4018,  0.4246,  0.3756,\n",
      "        -0.1113, -0.1386,  0.0658, -0.2037,  0.1330, -0.8612,  0.2820, -0.0561,\n",
      "         0.0118,  0.2379, -0.3212,  0.7229, -0.0806, -0.5222,  0.1934,  0.2042,\n",
      "         0.0695, -0.1040, -0.0668,  0.3854, -0.1953,  0.3257,  0.3393, -0.4048,\n",
      "         0.0074,  0.2307,  0.9652, -0.2604, -0.6872,  0.6596,  0.2264,  0.3557,\n",
      "        -0.9440, -0.0184, -0.3526, -0.4125, -0.1959, -0.8787,  0.2339,  0.2365,\n",
      "         0.6923, -0.5813,  0.1975,  0.5842,  0.2069, -0.5515,  0.5526, -0.7294,\n",
      "        -0.3563, -0.7417, -0.2674, -1.0351, -0.0739,  0.4488, -0.0061,  0.1699,\n",
      "        -0.1696, -0.9579, -0.4783, -0.4755, -0.2853,  0.0737, -0.5087, -0.1490,\n",
      "        -0.8204, -0.4560,  0.3135,  1.1205,  0.1392,  0.0845,  0.2417, -0.1143,\n",
      "        -0.3269,  0.1323, -0.2460, -0.7989, -0.6591,  0.3644, -0.7646, -0.2693,\n",
      "        -0.1076,  0.1995,  0.4647,  0.6372, -0.3102,  0.1171,  0.6310, -0.2128,\n",
      "        -0.5902, -0.1747, -0.4125, -0.6306, -0.3171,  0.1489,  0.1701,  0.4692,\n",
      "         0.3772,  0.0372,  0.9656, -0.3417,  0.8213, -0.5035, -0.6291,  0.4913,\n",
      "         0.3061, -0.1256, -0.2568,  0.1345, -1.1085, -0.3577, -0.1250,  0.0027,\n",
      "         0.9355,  0.1850, -0.6219, -0.6595,  0.4012,  0.8228, -0.0126,  0.8176,\n",
      "        -0.3083, -0.0217,  0.0316, -0.6498,  0.3061, -0.4961,  0.2898,  0.1267,\n",
      "        -0.4188, -0.2416, -0.1918,  0.4386,  0.3444, -0.1187,  0.1336,  0.4606,\n",
      "        -0.1912, -0.1160,  0.1509,  0.4231, -0.8350,  0.2926,  0.2277, -0.1501,\n",
      "         0.4754, -0.2172,  0.1880, -0.2701,  0.2190,  0.5155,  0.0900,  0.2422,\n",
      "        -0.5339,  0.1440, -0.2519,  0.0800,  0.0864, -0.1673, -0.3816, -0.2752,\n",
      "        -0.3093, -0.3974, -0.2039, -0.4161, -0.0754,  0.7653,  0.0094, -0.7052,\n",
      "         0.0299, -0.1890,  0.1190,  0.5422,  0.2352,  0.1791, -0.2142,  0.8668,\n",
      "         1.0497,  0.3273,  0.0172, -0.0258, -0.3256,  0.4954,  0.9494,  0.4798,\n",
      "        -0.6083, -0.4803,  0.4568,  0.5727, -0.3947, -0.7241, -0.0334, -0.0058,\n",
      "         0.7818, -0.2235, -0.1147,  0.0482,  0.6321,  0.1422, -0.6277,  0.0937,\n",
      "        -0.8542, -0.4716, -0.5613, -0.0273,  0.7190, -0.2274, -0.5217, -0.1899,\n",
      "         0.3168,  0.2987, -0.1376,  0.9553, -0.1156, -0.0685,  0.0290, -0.0856,\n",
      "         0.0778, -0.0777, -0.3529,  0.6018,  0.7136, -0.5033, -0.3459,  0.6840])\n",
      "Embedding of the fourth token:         tensor([-2.3612e-01, -1.8942e-01,  5.3950e-02, -6.2365e-01, -2.3148e-01,\n",
      "        -9.9448e-01,  9.4195e-01, -1.8332e-01,  3.7333e-02, -4.4000e-02,\n",
      "        -4.8188e-01, -3.2986e-01, -1.7622e-02, -3.1655e-01, -3.8145e-01,\n",
      "        -3.0411e-02, -7.9571e-01,  2.1917e-01,  1.3038e-01,  1.5951e-02,\n",
      "        -8.6027e-02, -7.9237e-02, -1.7374e-02, -8.0302e-03,  5.4692e-02,\n",
      "        -2.6634e-01, -4.7464e-01,  4.9183e-01, -3.1783e-01,  6.5485e-02,\n",
      "        -2.2099e-01,  3.7033e-02,  3.3136e-01, -1.8262e-01,  4.8159e-01,\n",
      "        -5.9304e-01,  3.2286e-01,  8.1917e-02,  7.5661e-02, -1.6478e-01,\n",
      "         2.1804e-01, -4.1819e-02, -2.1919e-01, -6.3573e-02, -7.1458e-02,\n",
      "         2.9954e-01, -1.1784e-01,  1.6410e-01,  1.1527e-01, -2.9021e-01,\n",
      "         3.0821e-01, -1.3829e-01,  1.4537e-01, -2.9233e-01, -3.9539e-01,\n",
      "         3.2309e-01, -1.3407e-01, -4.5807e-01, -1.1021e-01,  3.3921e-01,\n",
      "         1.8619e-02,  4.2482e-01,  1.5214e-01, -1.8999e-01,  1.9527e-02,\n",
      "        -3.3814e-01, -5.9186e-01, -5.9487e-01, -4.3428e-02, -8.2150e-02,\n",
      "        -2.3700e-01,  5.1430e-01,  1.5343e-01,  5.3241e-02, -2.1566e-01,\n",
      "        -2.9733e-01,  2.2224e-01,  5.0850e-01,  1.0039e-01, -6.4851e-01,\n",
      "         1.4747e-01, -2.2359e-01, -1.6699e-01,  1.3596e-01,  1.1596e-02,\n",
      "         2.3212e-01,  5.1456e-01,  1.3598e-01,  1.3279e-01,  1.3865e-01,\n",
      "         1.0850e-01,  1.4317e-02,  1.2502e-01, -8.2380e-01,  3.5883e-02,\n",
      "         3.9454e-01, -3.7468e-01, -5.9672e-02, -4.3821e-01, -3.0587e-01,\n",
      "        -5.7169e-01, -1.4618e-01, -3.4006e-02, -1.6739e-01,  3.8306e-01,\n",
      "        -5.1185e-01,  4.5700e-01, -1.9102e-01, -3.0691e-01,  3.0258e-01,\n",
      "        -7.6415e-01, -6.6102e-01, -1.5515e-01,  3.2676e-01,  7.0205e-02,\n",
      "        -2.6790e-01, -3.5332e-01,  2.0327e-01,  5.7843e-01,  2.4578e-01,\n",
      "         2.6567e-01,  2.2850e-01, -1.4730e-01,  8.9715e-02,  2.6186e-01,\n",
      "        -9.3738e-02,  4.2928e-02,  1.9609e-01, -1.0088e-01,  3.1883e-03,\n",
      "         3.0351e-01, -5.2520e-01,  4.7698e-01, -2.7404e-01,  2.6831e-01,\n",
      "         1.4763e-01, -4.2746e-01, -1.9409e-01,  3.4930e-01, -1.7383e-01,\n",
      "         7.6204e-02, -4.9636e-01,  4.8407e-01,  2.0949e-01,  1.9890e-01,\n",
      "         4.9123e-01,  3.9593e-01,  5.8943e-01,  3.5141e-01, -4.6505e-01,\n",
      "         1.1314e-01,  9.1550e-02, -2.1617e-01, -2.3173e-01, -1.8956e-01,\n",
      "         4.7205e-02,  4.0743e-02,  7.1442e-02, -1.9793e-01, -4.6918e-01,\n",
      "        -4.2462e-01, -3.3880e-01,  3.7865e-01,  1.5319e-01,  4.5409e-01,\n",
      "        -6.9070e-02,  6.9761e-02,  3.9800e-01, -4.5817e-01, -4.7889e-01,\n",
      "         2.7272e-01,  5.7595e-01,  4.4710e-01, -1.2855e-01, -1.9523e-01,\n",
      "        -9.3700e-02, -5.7226e-01, -3.6688e-01,  1.2549e-01,  7.1780e-02,\n",
      "        -2.2806e-01,  1.7524e-03,  5.7712e-01, -6.1080e-01, -3.8635e-01,\n",
      "         4.5638e-02,  4.2222e-01, -2.9230e-01, -2.0557e-01, -3.5527e-01,\n",
      "         3.1229e-01,  2.0803e-01, -3.2425e-01,  9.4215e-02, -3.8129e-04,\n",
      "        -2.2672e-01,  3.4439e-01,  2.8179e-01,  7.8637e-02, -1.0421e-01,\n",
      "         5.3450e-01,  1.9820e-01, -1.9832e-01, -3.1363e-01,  4.2476e-01,\n",
      "         7.2282e-02, -5.2482e-01,  8.8971e-02,  5.2961e-01,  1.0714e-01,\n",
      "         4.2281e-01,  1.7776e-01, -3.2822e-02, -5.3771e-01,  2.3683e-01,\n",
      "        -1.6456e-01, -1.2543e-01,  1.0455e-01,  3.1772e-01, -2.0409e-03,\n",
      "         5.6017e-02,  2.6980e-03,  4.2910e-04, -1.3696e-01,  1.0648e-01,\n",
      "        -3.9401e-01, -1.1648e-01,  3.3896e-01,  1.4735e-01,  3.9615e-01,\n",
      "         1.9727e-02,  1.9707e-01, -1.2013e-01,  8.7218e-02, -4.0701e-01,\n",
      "        -3.0893e-02, -1.0775e-01, -1.4964e-01, -1.6832e-01,  4.4353e-01,\n",
      "        -1.6435e-01, -2.1350e-01, -1.0646e-01,  2.7356e-01, -2.2307e-03,\n",
      "        -1.3851e-01, -6.0612e-01,  2.8105e-01, -6.1041e-02, -4.6253e-01,\n",
      "        -1.9870e-01, -3.7073e-01,  1.9520e-02, -3.3832e-01, -1.0951e-01,\n",
      "         2.8885e-01,  3.0532e-01, -1.7244e-01,  4.9248e-02, -6.4885e-01,\n",
      "        -1.2633e-01, -2.5026e-01,  3.2654e-01,  3.6494e-02,  9.7247e-02,\n",
      "        -3.1264e-01,  1.1885e-01,  2.7275e-01, -2.4235e-01,  9.4643e-02,\n",
      "         2.9588e-01, -5.7868e-01,  3.0859e-01,  2.8543e-01,  2.8855e-01,\n",
      "        -1.4769e-01,  1.8235e-01,  1.8908e-01, -2.2347e-01, -5.1766e-01,\n",
      "         6.4412e-02, -5.1274e-01, -1.8640e-01,  5.6707e-01,  1.7794e-02,\n",
      "        -2.9587e-01,  1.1681e-01,  3.6603e-01,  6.2482e-02, -6.4384e-01,\n",
      "         1.7928e-02,  1.1538e-01,  3.9431e-01, -7.1984e-01,  5.0264e-01,\n",
      "         2.3489e-01, -9.1409e-02,  3.0969e-01, -1.2950e-01, -3.1255e-01,\n",
      "         4.3394e-01,  2.9957e-01,  1.1476e-01,  7.0993e-02,  2.5026e-02,\n",
      "        -9.1863e-02,  4.4275e-01,  1.0854e-01,  2.0187e-01, -1.2399e-03,\n",
      "         5.7614e-01, -5.1287e-02,  3.5876e-01,  4.0109e-01,  7.5093e-02,\n",
      "        -2.6580e-01, -3.6926e-01,  5.5107e-01, -4.6667e-01,  1.2459e-01,\n",
      "        -1.6389e-01, -1.2550e-01, -1.0540e-01, -4.0448e-01,  3.5873e-02,\n",
      "        -1.6957e-01,  2.2560e-02,  2.1133e-01, -3.3195e-02, -9.8663e-02,\n",
      "         5.3579e-02, -4.1971e-02, -2.0120e-01,  1.3073e-01,  1.1372e-01,\n",
      "         3.4796e-01, -7.4880e-01,  3.9749e-02,  4.4876e-01, -4.0800e-01,\n",
      "         2.9097e-01, -2.4966e-02, -1.0659e-01, -2.0717e-02, -3.3239e-01,\n",
      "        -3.4934e-01, -1.5817e-02, -7.5289e-01, -2.1893e-01,  2.4843e-01,\n",
      "         3.3365e-01, -1.2591e-01,  2.8731e-01,  1.2711e-01,  1.2512e-01,\n",
      "         5.5414e-01, -2.0887e-01, -6.4136e-02, -4.1304e-01, -6.1288e-01,\n",
      "        -6.3049e-02,  4.9010e-02,  4.5016e-02, -1.7453e-01,  1.9864e-01,\n",
      "        -2.2297e-01,  1.5721e-01,  2.4619e-01, -3.0447e-02,  5.5250e-02,\n",
      "        -7.4040e-02, -2.0243e-01, -3.8133e-01, -3.9808e-01,  1.0297e-01,\n",
      "         1.3639e-02, -3.0427e-01,  3.8257e-01, -1.1167e-01,  7.8346e-02,\n",
      "         7.5485e-03,  2.8276e-01,  8.4183e-01, -6.4050e-02,  4.9021e-01,\n",
      "         2.8116e-01,  1.3975e-01,  4.0766e-01,  1.7529e-01,  4.0071e-02,\n",
      "        -1.1481e-01,  5.7538e-02,  1.9763e-01, -6.7492e-01,  4.2668e-02,\n",
      "        -1.7170e-01,  3.8572e-01, -2.0391e-01,  2.0045e-01, -2.2275e-01,\n",
      "         4.2087e-01,  7.4726e-02,  3.0045e-01,  2.3292e-01, -9.0567e-02,\n",
      "         7.4276e-01,  2.3301e-01, -2.1850e-01, -6.2267e-01,  6.5710e-02,\n",
      "         1.0704e-01, -6.6616e-02,  2.2800e-01, -3.1646e-02, -3.2205e-01,\n",
      "         3.8414e-01, -3.8999e-01,  1.5362e-01,  1.8878e-01,  2.3556e-01,\n",
      "         2.4032e-01, -4.7375e-01, -3.9769e-02,  7.0354e-01,  3.5769e-01,\n",
      "         3.4411e-01, -5.8001e-02,  3.3535e-01, -6.6371e-01, -1.2277e-01,\n",
      "         3.5330e-02, -3.7649e-01,  2.4173e-01, -5.3192e-01, -6.2533e-01,\n",
      "        -4.6472e-02,  3.7009e-01,  3.2136e-01,  7.2270e-01, -2.5761e-02,\n",
      "        -1.7400e-01,  1.1991e-01, -3.0944e-01, -1.5370e-01,  2.1295e-01,\n",
      "        -1.5385e-01, -2.5248e-01, -3.8098e-02, -5.4895e-01, -8.8825e-01,\n",
      "         1.7136e-01,  4.9291e-01,  3.5882e-01,  1.0463e-01, -2.9504e-01,\n",
      "        -6.4014e-01, -2.4630e-01,  3.2595e-01,  4.7254e-02, -1.1460e-01,\n",
      "        -1.4440e-01,  2.7160e-01,  1.3647e-01,  9.4767e-03,  1.7150e-01,\n",
      "         1.8989e-01,  1.0130e-01, -4.8114e-01, -2.4605e-01, -4.2095e-01,\n",
      "         9.5732e-02, -7.3388e-02,  1.4776e-01, -3.5995e-01, -2.6694e-02,\n",
      "         3.1305e-01, -3.8268e-01, -3.3334e-01,  9.9196e-03, -4.0838e-01,\n",
      "         1.3626e-01,  1.0704e-01, -1.6161e-01, -2.6829e-01,  4.4456e-01,\n",
      "        -2.2477e-01, -2.5188e-01,  3.7629e-01,  1.0065e-01,  3.3116e-02,\n",
      "        -3.9075e-02,  1.5545e-01,  1.6138e-01,  9.0295e-03,  9.3665e-02,\n",
      "        -1.5335e-02,  3.1135e-01, -1.8291e-01,  8.1895e-01,  3.0505e-01,\n",
      "        -1.8019e-01, -1.3208e-01, -1.7643e-01, -4.5036e-03, -3.1228e-01,\n",
      "         4.4814e-01,  3.4694e-01,  8.2748e-02,  2.8009e-01, -6.1785e-02,\n",
      "         2.6555e-01, -1.0731e-01])\n",
      "Embedding of the fifth token:          tensor([ 6.5353e-03,  1.0530e-01,  2.0694e-01, -2.8327e-01,  3.8333e-02,\n",
      "        -6.5962e-01,  5.8342e-01,  1.5648e-01, -1.4024e-01,  1.3889e-01,\n",
      "        -2.7393e-01, -2.6412e-01, -2.2530e-02, -1.3766e-01, -3.3079e-01,\n",
      "        -1.2314e-02, -6.0573e-01, -1.8378e-01, -9.2155e-02,  2.9450e-01,\n",
      "        -6.5520e-02, -2.5540e-01, -1.5659e-01, -9.5264e-02, -4.9506e-02,\n",
      "        -1.9102e-01,  1.2348e-01,  4.2129e-01, -6.2711e-02,  2.6615e-01,\n",
      "        -1.6393e-01,  6.3226e-02,  2.3958e-01,  4.5401e-02, -1.8763e-01,\n",
      "        -2.4649e-01, -1.7720e-01, -1.1588e-01,  1.5835e-01,  7.2457e-02,\n",
      "        -1.4719e-01, -2.0871e-03, -2.4871e-01,  3.9449e-01,  2.3077e-02,\n",
      "        -1.7890e-01, -3.3120e-01, -1.1063e-01,  1.9404e-01, -1.8189e-01,\n",
      "         3.6442e-01,  1.0741e-01, -2.6766e-01, -3.0482e-01, -2.8682e-01,\n",
      "         1.8597e-02,  2.4410e-01, -2.2239e-01, -8.1524e-02,  1.4830e-01,\n",
      "         1.1611e-01,  5.3671e-01,  6.1772e-02,  4.8027e-02, -1.0724e-01,\n",
      "         2.0951e-01, -1.1897e-01, -2.6693e-01, -3.3617e-02, -2.5043e-02,\n",
      "        -1.0738e-01,  5.6707e-02, -6.4886e-02,  4.0177e-01,  1.5906e-02,\n",
      "         2.2814e-01,  1.5764e-02,  1.7893e-01,  3.8642e-02, -3.0460e-01,\n",
      "        -6.1610e-02,  2.2828e-01,  1.0192e-01,  2.9203e-01, -1.1477e-02,\n",
      "         4.7556e-01, -1.2861e-01,  1.3266e-01, -1.0355e-01,  4.8470e-01,\n",
      "        -3.1390e-02, -3.9748e-01, -1.2747e-01, -2.3307e-01,  2.1445e-01,\n",
      "         1.6783e-01, -1.8124e-01, -1.0606e-01, -3.4613e-01, -2.4870e-01,\n",
      "        -3.1852e-01,  8.1227e-02,  3.2381e-01, -2.1372e-01,  2.7977e-01,\n",
      "        -4.1864e-01,  3.0652e-01, -1.0629e-03, -3.4127e-03,  6.1509e-02,\n",
      "        -5.5688e-01, -3.8640e-01,  3.2454e-01, -9.9492e-02,  2.5456e-02,\n",
      "        -1.5857e-01, -1.1711e-02, -1.4081e-01,  4.0028e-01, -1.7234e-01,\n",
      "        -6.7671e-02,  1.6670e-01, -4.2900e-02,  5.3827e-03,  6.5357e-02,\n",
      "         1.3464e-01,  3.5174e-01, -5.1641e-02, -4.5528e-01,  1.1010e-01,\n",
      "         5.7050e-01, -4.5795e-01,  3.9706e-01,  1.5094e-01, -4.1333e-02,\n",
      "        -1.9631e-01, -1.9982e-01, -2.3572e-01,  2.1976e-01,  8.1949e-04,\n",
      "         2.7487e-01, -1.9977e-01,  1.5517e-01,  1.7528e-01,  3.0903e-02,\n",
      "         1.0288e-01,  1.3336e-01,  3.9422e-01,  3.2791e-01, -4.3589e-02,\n",
      "        -4.1839e-02, -1.8083e-03,  1.8013e-01, -1.7211e-01,  3.2659e-01,\n",
      "        -1.8237e-01, -1.2356e-01,  8.2543e-02,  3.2095e-01, -3.7693e-01,\n",
      "        -9.8445e-02, -4.1347e-01,  4.7000e-02,  1.1310e-01,  2.4411e-01,\n",
      "        -3.0770e-01,  1.5861e-01, -8.3637e-02, -4.4800e-01, -1.5277e-01,\n",
      "         1.0772e-01,  5.9898e-01,  4.6290e-01,  2.9650e-01, -2.9010e-01,\n",
      "         3.4341e-01, -4.4880e-01, -9.3115e-02, -1.7506e-01, -1.9143e-01,\n",
      "        -2.6586e-01, -6.0086e-03, -3.7469e-02, -1.5456e-01, -3.2778e-01,\n",
      "         3.7411e-01,  9.3874e-02, -1.3589e-01,  1.0658e-01, -2.4925e-02,\n",
      "         7.4583e-02, -6.0890e-03, -1.1567e-01,  5.9551e-01,  1.9921e-02,\n",
      "        -3.9064e-01,  8.9530e-02,  1.5019e-01,  2.6147e-01, -2.2074e-01,\n",
      "         2.7997e-01, -1.2289e-01, -3.0710e-01, -3.4052e-01, -2.6620e-01,\n",
      "        -4.3599e-02, -2.4006e-01,  8.2609e-02,  1.9431e-01, -4.0131e-01,\n",
      "         3.1820e-01, -1.5857e-01,  8.7313e-02, -9.1381e-02,  2.3675e-01,\n",
      "        -4.9302e-01,  2.3549e-01,  1.6526e-01,  3.1106e-01,  1.9320e-01,\n",
      "         1.9493e-01, -2.9868e-02,  9.9630e-02, -2.5611e-01,  2.0167e-01,\n",
      "        -4.9769e-01, -7.4745e-02,  7.4257e-02,  1.0974e-01,  3.0368e-01,\n",
      "         1.2012e-01,  2.2523e-01, -3.1590e-01,  6.6861e-01, -3.9270e-01,\n",
      "         3.1000e-01, -5.4359e-01, -1.5069e-01,  1.3563e-01,  9.8366e-02,\n",
      "        -2.6072e-01, -3.6909e-01,  7.6112e-02,  2.9544e-01, -2.3780e-01,\n",
      "         2.3711e-01, -3.7170e-01, -1.0412e-01, -1.5040e-01, -2.0976e-01,\n",
      "        -1.8511e-01, -3.8705e-01,  1.7739e-01, -2.2011e-01, -1.5035e-01,\n",
      "        -1.5324e-02, -9.4920e-02,  8.6627e-02, -1.5843e-01, -4.0303e-01,\n",
      "         6.1643e-02, -3.6164e-02, -6.2834e-02,  3.1023e-02,  1.4721e-01,\n",
      "        -1.2282e-01,  1.8248e-01,  1.0860e-01,  9.7682e-02,  4.1257e-01,\n",
      "        -2.5741e-01, -6.5022e-01,  7.7702e-03,  4.6730e-02, -1.3709e-01,\n",
      "        -7.0805e-02,  2.3759e-02,  3.5226e-01, -3.8639e-03, -1.8925e-01,\n",
      "        -6.7740e-02,  1.3800e-01, -2.7615e-01,  5.3746e-02,  4.6807e-01,\n",
      "        -1.9535e-01, -1.3860e-01,  3.3421e-01,  3.7621e-01, -1.6431e-01,\n",
      "        -2.8149e-01, -3.6592e-02,  4.0777e-01, -2.4396e-01,  1.2129e-01,\n",
      "         4.5567e-01, -2.0012e-01,  1.3598e-01, -5.3490e-02,  2.3637e-01,\n",
      "         1.4584e-01,  2.3732e-01, -6.1074e-02,  1.0254e-01, -1.1382e-01,\n",
      "         1.9575e-02,  1.8199e-01, -1.3274e-01,  2.3795e-01,  1.3064e-01,\n",
      "         6.7151e-01, -5.0775e-01,  4.6278e-01,  4.6146e-01,  1.2195e-01,\n",
      "        -1.3172e-01, -2.8987e-01,  6.0855e-01,  1.1229e-01,  3.0321e-01,\n",
      "        -3.9474e-01, -3.2719e-02, -2.7978e-01, -3.0640e-01,  2.1386e-01,\n",
      "        -8.3771e-02,  1.5414e-01, -3.3903e-01,  4.3478e-01, -3.9662e-01,\n",
      "         1.2011e-03,  5.2856e-02, -3.2616e-01,  3.8401e-02, -2.9339e-01,\n",
      "        -2.1435e-01, -1.5309e-01,  2.0165e-01,  5.0021e-01, -1.7775e-01,\n",
      "         2.4552e-01, -4.8510e-02, -2.3090e-01,  1.0587e-01,  1.5288e-01,\n",
      "        -1.3510e-01, -2.0533e-01, -7.7409e-01, -2.6870e-01,  3.1719e-02,\n",
      "         1.4259e-01,  9.6814e-02,  8.3990e-02,  1.9653e-01, -2.2477e-01,\n",
      "         8.2862e-01,  2.0101e-01,  1.3356e-01, -4.7264e-01, -1.1754e-01,\n",
      "        -3.2035e-01,  2.7893e-01,  1.4213e-01, -3.1031e-01, -3.0311e-01,\n",
      "        -3.4970e-01, -4.1018e-01,  3.5237e-01,  1.5555e-01,  3.5677e-01,\n",
      "         2.5926e-01,  9.6389e-02,  3.3509e-03, -3.9559e-01, -1.2689e-01,\n",
      "         1.5994e-01, -5.5636e-01, -3.7270e-01,  9.9115e-02,  1.8943e-01,\n",
      "         2.1483e-01,  4.4472e-01,  7.3921e-01, -2.6421e-01,  2.0046e-01,\n",
      "        -2.0475e-01,  5.3294e-02, -2.3534e-01,  4.1254e-01, -2.3580e-01,\n",
      "        -5.2034e-02,  2.5387e-01,  2.5212e-01, -3.3181e-01,  5.3497e-03,\n",
      "         3.4345e-02, -6.7820e-02,  5.7526e-02,  1.5422e-01,  8.4398e-02,\n",
      "         8.9778e-02,  3.6124e-01,  1.9393e-01,  1.2960e-01, -1.4379e-02,\n",
      "         7.0830e-02,  7.9905e-02, -3.5129e-01, -7.9506e-01,  4.2106e-01,\n",
      "        -4.8812e-01, -1.9220e-01, -1.0802e-01, -4.3687e-01, -2.2330e-01,\n",
      "         1.4538e-01,  6.7424e-02, -2.0197e-01,  9.6227e-02,  1.8175e-01,\n",
      "         1.1562e-01,  3.5314e-02, -1.6472e-01,  2.9572e-01, -4.3273e-02,\n",
      "         5.4439e-02, -5.6302e-02,  2.0110e-01, -4.9642e-01, -1.6450e-01,\n",
      "        -2.6020e-01, -3.3562e-02, -2.6030e-01, -1.2661e-01, -2.8111e-01,\n",
      "         1.1765e-01,  6.0542e-01,  2.6678e-01,  4.2117e-01, -2.1279e-05,\n",
      "         5.1978e-02,  2.8748e-01, -5.4717e-02, -3.2058e-01,  2.8531e-01,\n",
      "         2.1838e-01, -1.5696e-01, -9.4805e-02, -5.0846e-01, -4.8544e-01,\n",
      "         2.4041e-01,  5.4395e-01, -1.1810e-01,  5.8747e-01, -1.7902e-01,\n",
      "        -1.5054e-01, -8.0681e-02,  4.1534e-01,  8.2457e-02,  3.0453e-01,\n",
      "        -8.8712e-02,  2.3773e-01, -7.7485e-07,  3.8184e-01,  2.0498e-02,\n",
      "         4.2463e-01,  8.6346e-02, -2.4502e-01,  5.4246e-02, -1.7383e-01,\n",
      "         3.9362e-01,  2.6157e-03, -2.9438e-01, -4.2049e-01,  2.4852e-01,\n",
      "         3.3695e-01, -3.3998e-01, -1.0680e-01, -4.5202e-02, -5.1116e-01,\n",
      "         5.2292e-01, -6.4511e-01, -3.3521e-01, -1.2393e-01,  2.2509e-01,\n",
      "        -2.3625e-02, -7.3986e-02,  4.9607e-01,  9.7819e-02, -3.6219e-01,\n",
      "        -9.8277e-02, -1.8702e-01, -6.1555e-02, -1.5850e-01, -2.8541e-01,\n",
      "         1.5597e-01, -1.7547e-01,  1.3693e-01,  7.0585e-01,  1.3522e-01,\n",
      "        -8.5137e-02,  5.3894e-02,  2.9945e-01, -1.3193e-02, -1.2259e-01,\n",
      "         4.4587e-01, -9.4485e-02, -2.0004e-01,  1.7769e-01, -6.3311e-02,\n",
      "         5.2169e-02,  1.2363e-01])\n",
      "Embedding of the sixth token:          tensor([-3.7413e+00, -1.7520e+00,  1.0879e+00, -2.6189e-01,  1.2209e+00,\n",
      "        -2.1202e+00,  2.4666e-01, -1.6095e+00, -1.4584e+00, -2.8486e-01,\n",
      "        -4.0003e-02,  5.5437e-01, -3.2988e-01, -1.7677e+00,  1.8240e-02,\n",
      "         7.1387e-01, -2.4287e+00, -5.1276e-01, -1.0957e+00, -1.0621e-01,\n",
      "        -3.9150e-01, -6.3917e-01, -1.6922e+00,  4.6370e-01,  2.7983e+00,\n",
      "        -2.1735e+00,  2.2633e-01, -1.5044e+00,  2.5043e+00,  2.8687e+00,\n",
      "        -5.4855e-01, -1.2493e+00, -1.0729e+00,  4.8591e-01,  6.2581e-01,\n",
      "         7.3945e-01, -2.7724e-01,  3.2579e-01, -3.9838e-01, -1.5016e-01,\n",
      "         4.0103e-01, -1.4956e-01, -1.3609e-01,  1.7004e-01, -3.6427e-01,\n",
      "         2.1721e-01,  6.5192e-02,  8.9158e-01,  1.9804e-01, -7.5707e-01,\n",
      "        -3.0202e-01, -2.5986e-01, -3.5698e+00, -2.7806e-01, -1.1644e+00,\n",
      "        -2.4118e-01, -3.8254e-01,  9.2729e-02,  1.1562e+00, -3.6127e-01,\n",
      "         3.6679e-01, -4.3867e-01, -7.8185e-01, -5.4513e-01,  1.3964e-01,\n",
      "        -7.3291e-01, -6.2211e-01, -2.6527e-01, -1.8252e-01, -1.5734e-01,\n",
      "         8.6029e-01,  1.1440e+00,  2.4581e-01, -5.7757e-01,  1.6544e-02,\n",
      "        -1.2038e-01,  3.0286e-01,  6.6625e-01, -7.7792e-01, -2.0005e-01,\n",
      "         1.6162e-01, -2.8081e-01, -2.4011e-01, -3.9464e-01,  3.5476e-01,\n",
      "        -1.8748e-01,  2.8218e-01, -4.5322e-01, -1.3274e-01, -4.5206e-01,\n",
      "         4.6736e-01,  5.2971e-01,  3.1713e-01, -2.5321e-01,  8.5581e-02,\n",
      "        -2.8297e-01, -7.2676e-01, -2.8202e-02, -7.3362e-01, -9.7544e-01,\n",
      "        -4.3956e-01,  1.7114e-01, -7.3933e-01,  5.1941e-01,  1.0717e+00,\n",
      "        -7.8227e-01,  1.4384e+00,  6.7075e-01, -2.2129e-01,  1.0321e+00,\n",
      "        -4.5814e-01, -5.4777e-01,  7.8797e-03,  7.2205e-01,  5.7920e-01,\n",
      "        -6.2781e-01,  4.7623e-02,  5.9258e-01,  5.4644e-01,  9.5627e-01,\n",
      "         7.7211e-01,  6.6021e-01, -7.4602e-01, -9.2431e-01,  1.4840e-01,\n",
      "        -4.4212e-01,  3.3725e-01,  6.2384e-02, -5.4018e-03,  9.6277e-02,\n",
      "         3.2993e-01, -5.9718e-01,  1.0119e+00,  2.5620e-01,  1.1206e+00,\n",
      "        -7.6352e-01, -8.2646e-01, -4.8976e-01,  7.2192e-01, -1.2910e-01,\n",
      "         3.4164e-01, -6.8374e-01,  7.3341e-01,  9.6608e-01, -5.8582e-01,\n",
      "         1.2115e-01, -4.2796e-02,  5.1138e-01,  4.1575e-01,  5.9013e-01,\n",
      "         6.3282e-01,  2.0153e-01,  2.6146e-01, -3.2249e-01, -5.9907e-01,\n",
      "         4.7676e-01,  3.2556e-01,  5.3057e-01, -7.6710e-02, -6.4532e-01,\n",
      "        -4.8919e-01,  1.9144e-01,  2.0949e-01,  5.0758e-01,  6.0826e-02,\n",
      "         7.0925e-02,  2.9674e-01,  3.8082e-01, -3.2900e-01, -8.4011e-01,\n",
      "         1.0623e-03,  5.4540e-02,  4.0313e-01,  2.6696e-01,  6.9018e-01,\n",
      "         2.3334e-01, -1.4693e-01,  3.9469e-01, -9.0735e-02,  6.1203e-01,\n",
      "         1.1399e-02, -4.8602e-01,  6.0451e-02, -5.1124e-01, -3.3937e-01,\n",
      "        -5.8241e-01, -4.4668e-01, -1.2441e-01,  8.3781e-01,  3.0757e-03,\n",
      "         8.4054e-01, -5.2071e-01, -3.3596e-01,  5.2715e-02, -1.4874e-01,\n",
      "        -2.6059e-02, -5.8843e-02,  5.8849e-02, -3.9703e-01,  3.4632e-01,\n",
      "         1.2173e-01, -3.1256e-01,  6.2137e-01, -5.7000e-01,  2.5655e-01,\n",
      "         4.0296e-01, -8.4895e-01,  3.3583e-01,  8.7760e-01, -3.7603e-01,\n",
      "         3.2345e-01,  9.9356e-01,  1.0282e+00, -8.1056e-01,  7.9826e-01,\n",
      "        -7.5651e-01,  2.5411e-01, -5.1109e-02,  4.4499e-01,  1.8826e-01,\n",
      "        -5.4997e-01, -6.2395e-01, -6.5825e-02, -4.1653e-01, -1.1921e-01,\n",
      "        -1.6639e-01,  2.2670e-01,  5.3948e-02, -1.0959e-01, -4.6128e-01,\n",
      "        -5.8965e-01, -1.5696e-01, -1.3991e-01,  3.1363e-01,  3.1763e-01,\n",
      "         8.8565e-03, -1.2458e-01,  2.0110e-01, -3.0728e-01,  3.3006e-02,\n",
      "        -6.4018e-01, -3.2276e-01, -4.6880e-01,  4.6960e-01, -1.1032e-01,\n",
      "         4.9715e-01, -5.2288e-02, -4.9376e-01, -4.6493e-02,  3.0779e-01,\n",
      "        -1.8611e-01, -2.0577e-01,  4.9825e-01, -2.4776e-01, -2.1613e-01,\n",
      "         7.2030e-01, -4.4467e-02, -4.5000e-02, -3.3078e-01, -1.1163e-01,\n",
      "        -4.0930e-01,  4.9919e-01, -4.2928e-01,  1.1394e-01,  2.9357e-01,\n",
      "        -1.8478e-01,  4.8979e-01,  3.9731e-01,  1.8359e-01,  5.8851e-03,\n",
      "        -1.1300e-01,  5.3262e-01,  1.2057e-01, -5.6521e-02, -4.3332e-01,\n",
      "         5.4306e-03, -1.9340e-01,  2.1756e-01,  3.2480e-01, -4.9140e-01,\n",
      "         3.8860e-01,  1.8530e-01, -1.8853e-01,  4.1961e-01,  5.1867e-01,\n",
      "         8.3060e-01,  4.5078e-01,  5.5106e-01, -8.6119e-02,  1.0205e-01,\n",
      "         4.8085e-01, -4.2561e-01, -3.5402e-01,  1.9525e-01,  3.9231e-01,\n",
      "        -1.7803e-01,  1.9181e-01, -3.3194e-01, -3.6218e-01,  4.5180e-01,\n",
      "        -1.1076e-01,  1.2105e-01,  1.3646e-02, -3.4898e-01, -1.1954e-03,\n",
      "        -3.8473e-01,  8.7495e-02, -4.6495e-01, -3.1510e-01, -3.5296e-01,\n",
      "         5.7931e-01, -3.1476e-01, -9.4901e-02, -3.9190e-01, -6.9694e-01,\n",
      "        -1.0447e-02,  2.3938e-01, -1.7280e-01, -5.3704e-01,  1.0612e+00,\n",
      "         5.0175e-01, -1.1020e-02,  3.8966e-01,  4.2643e-01, -9.9510e-03,\n",
      "         2.0185e-01, -4.3824e-01,  3.2814e-01, -4.8346e-01, -3.2599e-01,\n",
      "         1.8513e-01, -4.6976e-02,  4.5525e-01,  6.4502e-01, -3.2612e-02,\n",
      "         8.7192e-02, -2.1008e-01,  2.8687e-02,  1.1076e-01, -3.0017e-01,\n",
      "        -5.0154e-02,  3.8217e-01,  2.6532e-02,  1.2623e-01,  1.0394e-01,\n",
      "         4.3786e-02, -1.1118e-01,  2.8052e-02, -6.8033e-03, -5.0394e-03,\n",
      "         4.3601e-01,  8.1189e-02, -7.4181e-02, -3.6765e-01, -1.9569e-02,\n",
      "         1.4345e-01,  2.8632e-01, -2.6073e-01,  3.6885e-02, -3.4718e-01,\n",
      "         5.2495e-01,  3.8395e-01,  1.5273e-01, -2.1628e-01,  3.4222e-01,\n",
      "        -3.4717e-01,  1.3506e-01, -2.9593e-01, -4.3897e-01,  6.6618e-02,\n",
      "        -6.0420e-01,  4.6465e-01, -4.1210e-01,  6.5812e-01,  3.3184e-01,\n",
      "         2.2699e-01,  1.5620e-01,  3.7608e-01, -3.8086e-02, -3.2624e-01,\n",
      "        -1.5369e-01, -4.2839e-01, -4.0469e-01, -8.9543e-03, -6.6050e-01,\n",
      "         1.9467e-01,  8.0796e-01, -4.7144e-01, -6.3811e-01,  3.5103e-01,\n",
      "        -3.6293e-01, -2.7268e-01, -3.6439e-01,  5.5532e-02,  1.3991e-01,\n",
      "         8.0138e-01, -5.7100e-01,  1.0608e+00,  2.4445e-01, -7.6118e-01,\n",
      "         2.2254e-02,  1.3983e-02,  6.2986e-01, -7.3683e-02,  8.0212e-01,\n",
      "        -2.8109e-01,  6.6694e-01, -6.7035e-01, -9.6533e-02, -1.9628e-02,\n",
      "         1.3905e-01,  5.9583e-02,  4.8975e-02,  5.6256e-01, -2.9041e-01,\n",
      "         5.6460e-02,  4.1767e-01, -7.3050e-01,  2.9958e-02, -5.7242e-01,\n",
      "         3.7911e-01,  8.6302e-01, -8.6317e-01, -9.3029e-01, -8.1289e-02,\n",
      "        -7.8378e-02, -2.2158e-01, -3.6283e-02,  5.3579e-01, -5.3185e-01,\n",
      "         5.7798e-01,  3.4648e-01,  2.6285e-01, -1.8939e-01, -1.6681e-01,\n",
      "         7.8349e-01,  1.9662e-01,  1.2939e-01, -4.4017e-01,  4.0976e-01,\n",
      "         4.2476e-01, -4.9217e-01,  6.2978e-01, -1.4613e-01, -1.6054e-01,\n",
      "         7.4404e-01,  1.6765e-01, -3.2840e-01,  4.1663e-01,  6.0308e-01,\n",
      "        -1.0861e+00,  5.4388e-01, -9.2215e-01, -4.8429e-01, -1.3947e+00,\n",
      "         2.5982e-01,  1.4184e+00,  3.7120e-01, -9.2127e-01, -1.4159e+00,\n",
      "         1.6531e-02, -6.3792e-01,  5.1167e-01,  3.6167e-01, -1.3585e-01,\n",
      "         6.1094e-01,  2.7156e-01,  4.9552e-01, -1.0393e+00,  2.9700e-01,\n",
      "        -6.7169e-01, -7.1377e-01,  2.5848e-01,  7.3621e-02,  5.0331e-02,\n",
      "         2.2472e-01, -3.8532e-01,  9.9617e-03, -6.7879e-02, -7.7955e-02,\n",
      "        -4.4956e-01,  1.8528e+00,  1.5305e+00,  2.2598e-01, -5.8522e-01,\n",
      "         4.8994e-01,  6.9847e-01, -3.4993e-01,  4.0096e-01,  1.5367e+00,\n",
      "        -1.2456e-01, -9.0872e-01, -4.7142e-01, -1.1398e+00,  1.6423e+00,\n",
      "        -6.9061e-01, -8.8025e-01, -1.3386e+00, -5.0809e-01, -3.1707e-01,\n",
      "         2.0900e-01, -2.8918e-01, -3.6064e-01,  1.5625e-01,  6.1036e-02,\n",
      "        -4.7900e-01,  9.4345e-01,  3.1026e-01, -5.6936e-01,  5.1809e-01,\n",
      "         9.4103e-01, -4.7187e-01])\n",
      "Embedding of the twenty-fifth token:   tensor([-3.7413e+00, -1.7520e+00,  1.0879e+00, -2.6189e-01,  1.2209e+00,\n",
      "        -2.1202e+00,  2.4666e-01, -1.6095e+00, -1.4584e+00, -2.8486e-01,\n",
      "        -4.0003e-02,  5.5437e-01, -3.2988e-01, -1.7677e+00,  1.8240e-02,\n",
      "         7.1387e-01, -2.4287e+00, -5.1276e-01, -1.0957e+00, -1.0621e-01,\n",
      "        -3.9150e-01, -6.3917e-01, -1.6922e+00,  4.6370e-01,  2.7983e+00,\n",
      "        -2.1735e+00,  2.2633e-01, -1.5044e+00,  2.5043e+00,  2.8687e+00,\n",
      "        -5.4855e-01, -1.2493e+00, -1.0729e+00,  4.8591e-01,  6.2581e-01,\n",
      "         7.3945e-01, -2.7724e-01,  3.2579e-01, -3.9838e-01, -1.5016e-01,\n",
      "         4.0103e-01, -1.4956e-01, -1.3609e-01,  1.7004e-01, -3.6427e-01,\n",
      "         2.1721e-01,  6.5192e-02,  8.9158e-01,  1.9804e-01, -7.5707e-01,\n",
      "        -3.0202e-01, -2.5986e-01, -3.5698e+00, -2.7806e-01, -1.1644e+00,\n",
      "        -2.4118e-01, -3.8254e-01,  9.2729e-02,  1.1562e+00, -3.6127e-01,\n",
      "         3.6679e-01, -4.3867e-01, -7.8185e-01, -5.4513e-01,  1.3964e-01,\n",
      "        -7.3291e-01, -6.2211e-01, -2.6527e-01, -1.8252e-01, -1.5734e-01,\n",
      "         8.6029e-01,  1.1440e+00,  2.4581e-01, -5.7757e-01,  1.6544e-02,\n",
      "        -1.2038e-01,  3.0286e-01,  6.6625e-01, -7.7792e-01, -2.0005e-01,\n",
      "         1.6162e-01, -2.8081e-01, -2.4011e-01, -3.9464e-01,  3.5476e-01,\n",
      "        -1.8748e-01,  2.8218e-01, -4.5322e-01, -1.3274e-01, -4.5206e-01,\n",
      "         4.6736e-01,  5.2971e-01,  3.1713e-01, -2.5321e-01,  8.5581e-02,\n",
      "        -2.8297e-01, -7.2676e-01, -2.8202e-02, -7.3362e-01, -9.7544e-01,\n",
      "        -4.3956e-01,  1.7114e-01, -7.3933e-01,  5.1941e-01,  1.0717e+00,\n",
      "        -7.8227e-01,  1.4384e+00,  6.7075e-01, -2.2129e-01,  1.0321e+00,\n",
      "        -4.5814e-01, -5.4777e-01,  7.8797e-03,  7.2205e-01,  5.7920e-01,\n",
      "        -6.2781e-01,  4.7623e-02,  5.9258e-01,  5.4644e-01,  9.5627e-01,\n",
      "         7.7211e-01,  6.6021e-01, -7.4602e-01, -9.2431e-01,  1.4840e-01,\n",
      "        -4.4212e-01,  3.3725e-01,  6.2384e-02, -5.4018e-03,  9.6277e-02,\n",
      "         3.2993e-01, -5.9718e-01,  1.0119e+00,  2.5620e-01,  1.1206e+00,\n",
      "        -7.6352e-01, -8.2646e-01, -4.8976e-01,  7.2192e-01, -1.2910e-01,\n",
      "         3.4164e-01, -6.8374e-01,  7.3341e-01,  9.6608e-01, -5.8582e-01,\n",
      "         1.2115e-01, -4.2796e-02,  5.1138e-01,  4.1575e-01,  5.9013e-01,\n",
      "         6.3282e-01,  2.0153e-01,  2.6146e-01, -3.2249e-01, -5.9907e-01,\n",
      "         4.7676e-01,  3.2556e-01,  5.3057e-01, -7.6710e-02, -6.4532e-01,\n",
      "        -4.8919e-01,  1.9144e-01,  2.0949e-01,  5.0758e-01,  6.0826e-02,\n",
      "         7.0925e-02,  2.9674e-01,  3.8082e-01, -3.2900e-01, -8.4011e-01,\n",
      "         1.0623e-03,  5.4540e-02,  4.0313e-01,  2.6696e-01,  6.9018e-01,\n",
      "         2.3334e-01, -1.4693e-01,  3.9469e-01, -9.0735e-02,  6.1203e-01,\n",
      "         1.1399e-02, -4.8602e-01,  6.0451e-02, -5.1124e-01, -3.3937e-01,\n",
      "        -5.8241e-01, -4.4668e-01, -1.2441e-01,  8.3781e-01,  3.0757e-03,\n",
      "         8.4054e-01, -5.2071e-01, -3.3596e-01,  5.2715e-02, -1.4874e-01,\n",
      "        -2.6059e-02, -5.8843e-02,  5.8849e-02, -3.9703e-01,  3.4632e-01,\n",
      "         1.2173e-01, -3.1256e-01,  6.2137e-01, -5.7000e-01,  2.5655e-01,\n",
      "         4.0296e-01, -8.4895e-01,  3.3583e-01,  8.7760e-01, -3.7603e-01,\n",
      "         3.2345e-01,  9.9356e-01,  1.0282e+00, -8.1056e-01,  7.9826e-01,\n",
      "        -7.5651e-01,  2.5411e-01, -5.1109e-02,  4.4499e-01,  1.8826e-01,\n",
      "        -5.4997e-01, -6.2395e-01, -6.5825e-02, -4.1653e-01, -1.1921e-01,\n",
      "        -1.6639e-01,  2.2670e-01,  5.3948e-02, -1.0959e-01, -4.6128e-01,\n",
      "        -5.8965e-01, -1.5696e-01, -1.3991e-01,  3.1363e-01,  3.1763e-01,\n",
      "         8.8565e-03, -1.2458e-01,  2.0110e-01, -3.0728e-01,  3.3006e-02,\n",
      "        -6.4018e-01, -3.2276e-01, -4.6880e-01,  4.6960e-01, -1.1032e-01,\n",
      "         4.9715e-01, -5.2288e-02, -4.9376e-01, -4.6493e-02,  3.0779e-01,\n",
      "        -1.8611e-01, -2.0577e-01,  4.9825e-01, -2.4776e-01, -2.1613e-01,\n",
      "         7.2030e-01, -4.4467e-02, -4.5000e-02, -3.3078e-01, -1.1163e-01,\n",
      "        -4.0930e-01,  4.9919e-01, -4.2928e-01,  1.1394e-01,  2.9357e-01,\n",
      "        -1.8478e-01,  4.8979e-01,  3.9731e-01,  1.8359e-01,  5.8851e-03,\n",
      "        -1.1300e-01,  5.3262e-01,  1.2057e-01, -5.6521e-02, -4.3332e-01,\n",
      "         5.4306e-03, -1.9340e-01,  2.1756e-01,  3.2480e-01, -4.9140e-01,\n",
      "         3.8860e-01,  1.8530e-01, -1.8853e-01,  4.1961e-01,  5.1867e-01,\n",
      "         8.3060e-01,  4.5078e-01,  5.5106e-01, -8.6119e-02,  1.0205e-01,\n",
      "         4.8085e-01, -4.2561e-01, -3.5402e-01,  1.9525e-01,  3.9231e-01,\n",
      "        -1.7803e-01,  1.9181e-01, -3.3194e-01, -3.6218e-01,  4.5180e-01,\n",
      "        -1.1076e-01,  1.2105e-01,  1.3646e-02, -3.4898e-01, -1.1954e-03,\n",
      "        -3.8473e-01,  8.7495e-02, -4.6495e-01, -3.1510e-01, -3.5296e-01,\n",
      "         5.7931e-01, -3.1476e-01, -9.4901e-02, -3.9190e-01, -6.9694e-01,\n",
      "        -1.0447e-02,  2.3938e-01, -1.7280e-01, -5.3704e-01,  1.0612e+00,\n",
      "         5.0175e-01, -1.1020e-02,  3.8966e-01,  4.2643e-01, -9.9510e-03,\n",
      "         2.0185e-01, -4.3824e-01,  3.2814e-01, -4.8346e-01, -3.2599e-01,\n",
      "         1.8513e-01, -4.6976e-02,  4.5525e-01,  6.4502e-01, -3.2612e-02,\n",
      "         8.7192e-02, -2.1008e-01,  2.8687e-02,  1.1076e-01, -3.0017e-01,\n",
      "        -5.0154e-02,  3.8217e-01,  2.6532e-02,  1.2623e-01,  1.0394e-01,\n",
      "         4.3786e-02, -1.1118e-01,  2.8052e-02, -6.8033e-03, -5.0394e-03,\n",
      "         4.3601e-01,  8.1189e-02, -7.4181e-02, -3.6765e-01, -1.9569e-02,\n",
      "         1.4345e-01,  2.8632e-01, -2.6073e-01,  3.6885e-02, -3.4718e-01,\n",
      "         5.2495e-01,  3.8395e-01,  1.5273e-01, -2.1628e-01,  3.4222e-01,\n",
      "        -3.4717e-01,  1.3506e-01, -2.9593e-01, -4.3897e-01,  6.6618e-02,\n",
      "        -6.0420e-01,  4.6465e-01, -4.1210e-01,  6.5812e-01,  3.3184e-01,\n",
      "         2.2699e-01,  1.5620e-01,  3.7608e-01, -3.8086e-02, -3.2624e-01,\n",
      "        -1.5369e-01, -4.2839e-01, -4.0469e-01, -8.9543e-03, -6.6050e-01,\n",
      "         1.9467e-01,  8.0796e-01, -4.7144e-01, -6.3811e-01,  3.5103e-01,\n",
      "        -3.6293e-01, -2.7268e-01, -3.6439e-01,  5.5532e-02,  1.3991e-01,\n",
      "         8.0138e-01, -5.7100e-01,  1.0608e+00,  2.4445e-01, -7.6118e-01,\n",
      "         2.2254e-02,  1.3983e-02,  6.2986e-01, -7.3683e-02,  8.0212e-01,\n",
      "        -2.8109e-01,  6.6694e-01, -6.7035e-01, -9.6533e-02, -1.9628e-02,\n",
      "         1.3905e-01,  5.9583e-02,  4.8975e-02,  5.6256e-01, -2.9041e-01,\n",
      "         5.6460e-02,  4.1767e-01, -7.3050e-01,  2.9958e-02, -5.7242e-01,\n",
      "         3.7911e-01,  8.6302e-01, -8.6317e-01, -9.3029e-01, -8.1289e-02,\n",
      "        -7.8378e-02, -2.2158e-01, -3.6283e-02,  5.3579e-01, -5.3185e-01,\n",
      "         5.7798e-01,  3.4648e-01,  2.6285e-01, -1.8939e-01, -1.6681e-01,\n",
      "         7.8349e-01,  1.9662e-01,  1.2939e-01, -4.4017e-01,  4.0976e-01,\n",
      "         4.2476e-01, -4.9217e-01,  6.2978e-01, -1.4613e-01, -1.6054e-01,\n",
      "         7.4404e-01,  1.6765e-01, -3.2840e-01,  4.1663e-01,  6.0308e-01,\n",
      "        -1.0861e+00,  5.4388e-01, -9.2215e-01, -4.8429e-01, -1.3947e+00,\n",
      "         2.5982e-01,  1.4184e+00,  3.7120e-01, -9.2127e-01, -1.4159e+00,\n",
      "         1.6531e-02, -6.3792e-01,  5.1167e-01,  3.6167e-01, -1.3585e-01,\n",
      "         6.1094e-01,  2.7156e-01,  4.9552e-01, -1.0393e+00,  2.9700e-01,\n",
      "        -6.7169e-01, -7.1377e-01,  2.5848e-01,  7.3621e-02,  5.0331e-02,\n",
      "         2.2472e-01, -3.8532e-01,  9.9617e-03, -6.7879e-02, -7.7955e-02,\n",
      "        -4.4956e-01,  1.8528e+00,  1.5305e+00,  2.2598e-01, -5.8522e-01,\n",
      "         4.8994e-01,  6.9847e-01, -3.4993e-01,  4.0096e-01,  1.5367e+00,\n",
      "        -1.2456e-01, -9.0872e-01, -4.7142e-01, -1.1398e+00,  1.6423e+00,\n",
      "        -6.9061e-01, -8.8025e-01, -1.3386e+00, -5.0809e-01, -3.1707e-01,\n",
      "         2.0900e-01, -2.8918e-01, -3.6064e-01,  1.5625e-01,  6.1036e-02,\n",
      "        -4.7900e-01,  9.4345e-01,  3.1026e-01, -5.6936e-01,  5.1809e-01,\n",
      "         9.4103e-01, -4.7187e-01])\n",
      "Embedding of the fortieth token:       tensor([-3.7413e+00, -1.7520e+00,  1.0879e+00, -2.6189e-01,  1.2209e+00,\n",
      "        -2.1202e+00,  2.4666e-01, -1.6095e+00, -1.4584e+00, -2.8486e-01,\n",
      "        -4.0003e-02,  5.5437e-01, -3.2988e-01, -1.7677e+00,  1.8240e-02,\n",
      "         7.1387e-01, -2.4287e+00, -5.1276e-01, -1.0957e+00, -1.0621e-01,\n",
      "        -3.9150e-01, -6.3917e-01, -1.6922e+00,  4.6370e-01,  2.7983e+00,\n",
      "        -2.1735e+00,  2.2633e-01, -1.5044e+00,  2.5043e+00,  2.8687e+00,\n",
      "        -5.4855e-01, -1.2493e+00, -1.0729e+00,  4.8591e-01,  6.2581e-01,\n",
      "         7.3945e-01, -2.7724e-01,  3.2579e-01, -3.9838e-01, -1.5016e-01,\n",
      "         4.0103e-01, -1.4956e-01, -1.3609e-01,  1.7004e-01, -3.6427e-01,\n",
      "         2.1721e-01,  6.5192e-02,  8.9158e-01,  1.9804e-01, -7.5707e-01,\n",
      "        -3.0202e-01, -2.5986e-01, -3.5698e+00, -2.7806e-01, -1.1644e+00,\n",
      "        -2.4118e-01, -3.8254e-01,  9.2729e-02,  1.1562e+00, -3.6127e-01,\n",
      "         3.6679e-01, -4.3867e-01, -7.8185e-01, -5.4513e-01,  1.3964e-01,\n",
      "        -7.3291e-01, -6.2211e-01, -2.6527e-01, -1.8252e-01, -1.5734e-01,\n",
      "         8.6029e-01,  1.1440e+00,  2.4581e-01, -5.7757e-01,  1.6544e-02,\n",
      "        -1.2038e-01,  3.0286e-01,  6.6625e-01, -7.7792e-01, -2.0005e-01,\n",
      "         1.6162e-01, -2.8081e-01, -2.4011e-01, -3.9464e-01,  3.5476e-01,\n",
      "        -1.8748e-01,  2.8218e-01, -4.5322e-01, -1.3274e-01, -4.5206e-01,\n",
      "         4.6736e-01,  5.2971e-01,  3.1713e-01, -2.5321e-01,  8.5581e-02,\n",
      "        -2.8297e-01, -7.2676e-01, -2.8202e-02, -7.3362e-01, -9.7544e-01,\n",
      "        -4.3956e-01,  1.7114e-01, -7.3933e-01,  5.1941e-01,  1.0717e+00,\n",
      "        -7.8227e-01,  1.4384e+00,  6.7075e-01, -2.2129e-01,  1.0321e+00,\n",
      "        -4.5814e-01, -5.4777e-01,  7.8797e-03,  7.2205e-01,  5.7920e-01,\n",
      "        -6.2781e-01,  4.7623e-02,  5.9258e-01,  5.4644e-01,  9.5627e-01,\n",
      "         7.7211e-01,  6.6021e-01, -7.4602e-01, -9.2431e-01,  1.4840e-01,\n",
      "        -4.4212e-01,  3.3725e-01,  6.2384e-02, -5.4018e-03,  9.6277e-02,\n",
      "         3.2993e-01, -5.9718e-01,  1.0119e+00,  2.5620e-01,  1.1206e+00,\n",
      "        -7.6352e-01, -8.2646e-01, -4.8976e-01,  7.2192e-01, -1.2910e-01,\n",
      "         3.4164e-01, -6.8374e-01,  7.3341e-01,  9.6608e-01, -5.8582e-01,\n",
      "         1.2115e-01, -4.2796e-02,  5.1138e-01,  4.1575e-01,  5.9013e-01,\n",
      "         6.3282e-01,  2.0153e-01,  2.6146e-01, -3.2249e-01, -5.9907e-01,\n",
      "         4.7676e-01,  3.2556e-01,  5.3057e-01, -7.6710e-02, -6.4532e-01,\n",
      "        -4.8919e-01,  1.9144e-01,  2.0949e-01,  5.0758e-01,  6.0826e-02,\n",
      "         7.0925e-02,  2.9674e-01,  3.8082e-01, -3.2900e-01, -8.4011e-01,\n",
      "         1.0623e-03,  5.4540e-02,  4.0313e-01,  2.6696e-01,  6.9018e-01,\n",
      "         2.3334e-01, -1.4693e-01,  3.9469e-01, -9.0735e-02,  6.1203e-01,\n",
      "         1.1399e-02, -4.8602e-01,  6.0451e-02, -5.1124e-01, -3.3937e-01,\n",
      "        -5.8241e-01, -4.4668e-01, -1.2441e-01,  8.3781e-01,  3.0757e-03,\n",
      "         8.4054e-01, -5.2071e-01, -3.3596e-01,  5.2715e-02, -1.4874e-01,\n",
      "        -2.6059e-02, -5.8843e-02,  5.8849e-02, -3.9703e-01,  3.4632e-01,\n",
      "         1.2173e-01, -3.1256e-01,  6.2137e-01, -5.7000e-01,  2.5655e-01,\n",
      "         4.0296e-01, -8.4895e-01,  3.3583e-01,  8.7760e-01, -3.7603e-01,\n",
      "         3.2345e-01,  9.9356e-01,  1.0282e+00, -8.1056e-01,  7.9826e-01,\n",
      "        -7.5651e-01,  2.5411e-01, -5.1109e-02,  4.4499e-01,  1.8826e-01,\n",
      "        -5.4997e-01, -6.2395e-01, -6.5825e-02, -4.1653e-01, -1.1921e-01,\n",
      "        -1.6639e-01,  2.2670e-01,  5.3948e-02, -1.0959e-01, -4.6128e-01,\n",
      "        -5.8965e-01, -1.5696e-01, -1.3991e-01,  3.1363e-01,  3.1763e-01,\n",
      "         8.8565e-03, -1.2458e-01,  2.0110e-01, -3.0728e-01,  3.3006e-02,\n",
      "        -6.4018e-01, -3.2276e-01, -4.6880e-01,  4.6960e-01, -1.1032e-01,\n",
      "         4.9715e-01, -5.2288e-02, -4.9376e-01, -4.6493e-02,  3.0779e-01,\n",
      "        -1.8611e-01, -2.0577e-01,  4.9825e-01, -2.4776e-01, -2.1613e-01,\n",
      "         7.2030e-01, -4.4467e-02, -4.5000e-02, -3.3078e-01, -1.1163e-01,\n",
      "        -4.0930e-01,  4.9919e-01, -4.2928e-01,  1.1394e-01,  2.9357e-01,\n",
      "        -1.8478e-01,  4.8979e-01,  3.9731e-01,  1.8359e-01,  5.8851e-03,\n",
      "        -1.1300e-01,  5.3262e-01,  1.2057e-01, -5.6521e-02, -4.3332e-01,\n",
      "         5.4306e-03, -1.9340e-01,  2.1756e-01,  3.2480e-01, -4.9140e-01,\n",
      "         3.8860e-01,  1.8530e-01, -1.8853e-01,  4.1961e-01,  5.1867e-01,\n",
      "         8.3060e-01,  4.5078e-01,  5.5106e-01, -8.6119e-02,  1.0205e-01,\n",
      "         4.8085e-01, -4.2561e-01, -3.5402e-01,  1.9525e-01,  3.9231e-01,\n",
      "        -1.7803e-01,  1.9181e-01, -3.3194e-01, -3.6218e-01,  4.5180e-01,\n",
      "        -1.1076e-01,  1.2105e-01,  1.3646e-02, -3.4898e-01, -1.1954e-03,\n",
      "        -3.8473e-01,  8.7495e-02, -4.6495e-01, -3.1510e-01, -3.5296e-01,\n",
      "         5.7931e-01, -3.1476e-01, -9.4901e-02, -3.9190e-01, -6.9694e-01,\n",
      "        -1.0447e-02,  2.3938e-01, -1.7280e-01, -5.3704e-01,  1.0612e+00,\n",
      "         5.0175e-01, -1.1020e-02,  3.8966e-01,  4.2643e-01, -9.9510e-03,\n",
      "         2.0185e-01, -4.3824e-01,  3.2814e-01, -4.8346e-01, -3.2599e-01,\n",
      "         1.8513e-01, -4.6976e-02,  4.5525e-01,  6.4502e-01, -3.2612e-02,\n",
      "         8.7192e-02, -2.1008e-01,  2.8687e-02,  1.1076e-01, -3.0017e-01,\n",
      "        -5.0154e-02,  3.8217e-01,  2.6532e-02,  1.2623e-01,  1.0394e-01,\n",
      "         4.3786e-02, -1.1118e-01,  2.8052e-02, -6.8033e-03, -5.0394e-03,\n",
      "         4.3601e-01,  8.1189e-02, -7.4181e-02, -3.6765e-01, -1.9569e-02,\n",
      "         1.4345e-01,  2.8632e-01, -2.6073e-01,  3.6885e-02, -3.4718e-01,\n",
      "         5.2495e-01,  3.8395e-01,  1.5273e-01, -2.1628e-01,  3.4222e-01,\n",
      "        -3.4717e-01,  1.3506e-01, -2.9593e-01, -4.3897e-01,  6.6618e-02,\n",
      "        -6.0420e-01,  4.6465e-01, -4.1210e-01,  6.5812e-01,  3.3184e-01,\n",
      "         2.2699e-01,  1.5620e-01,  3.7608e-01, -3.8086e-02, -3.2624e-01,\n",
      "        -1.5369e-01, -4.2839e-01, -4.0469e-01, -8.9543e-03, -6.6050e-01,\n",
      "         1.9467e-01,  8.0796e-01, -4.7144e-01, -6.3811e-01,  3.5103e-01,\n",
      "        -3.6293e-01, -2.7268e-01, -3.6439e-01,  5.5532e-02,  1.3991e-01,\n",
      "         8.0138e-01, -5.7100e-01,  1.0608e+00,  2.4445e-01, -7.6118e-01,\n",
      "         2.2254e-02,  1.3983e-02,  6.2986e-01, -7.3683e-02,  8.0212e-01,\n",
      "        -2.8109e-01,  6.6694e-01, -6.7035e-01, -9.6533e-02, -1.9628e-02,\n",
      "         1.3905e-01,  5.9583e-02,  4.8975e-02,  5.6256e-01, -2.9041e-01,\n",
      "         5.6460e-02,  4.1767e-01, -7.3050e-01,  2.9958e-02, -5.7242e-01,\n",
      "         3.7911e-01,  8.6302e-01, -8.6317e-01, -9.3029e-01, -8.1289e-02,\n",
      "        -7.8378e-02, -2.2158e-01, -3.6283e-02,  5.3579e-01, -5.3185e-01,\n",
      "         5.7798e-01,  3.4648e-01,  2.6285e-01, -1.8939e-01, -1.6681e-01,\n",
      "         7.8349e-01,  1.9662e-01,  1.2939e-01, -4.4017e-01,  4.0976e-01,\n",
      "         4.2476e-01, -4.9217e-01,  6.2978e-01, -1.4613e-01, -1.6054e-01,\n",
      "         7.4404e-01,  1.6765e-01, -3.2840e-01,  4.1663e-01,  6.0308e-01,\n",
      "        -1.0861e+00,  5.4388e-01, -9.2215e-01, -4.8429e-01, -1.3947e+00,\n",
      "         2.5982e-01,  1.4184e+00,  3.7120e-01, -9.2127e-01, -1.4159e+00,\n",
      "         1.6531e-02, -6.3792e-01,  5.1167e-01,  3.6167e-01, -1.3585e-01,\n",
      "         6.1094e-01,  2.7156e-01,  4.9552e-01, -1.0393e+00,  2.9700e-01,\n",
      "        -6.7169e-01, -7.1377e-01,  2.5848e-01,  7.3621e-02,  5.0331e-02,\n",
      "         2.2472e-01, -3.8532e-01,  9.9617e-03, -6.7879e-02, -7.7955e-02,\n",
      "        -4.4956e-01,  1.8528e+00,  1.5305e+00,  2.2598e-01, -5.8522e-01,\n",
      "         4.8994e-01,  6.9847e-01, -3.4993e-01,  4.0096e-01,  1.5367e+00,\n",
      "        -1.2456e-01, -9.0872e-01, -4.7142e-01, -1.1398e+00,  1.6423e+00,\n",
      "        -6.9061e-01, -8.8025e-01, -1.3386e+00, -5.0809e-01, -3.1707e-01,\n",
      "         2.0900e-01, -2.8918e-01, -3.6064e-01,  1.5625e-01,  6.1036e-02,\n",
      "        -4.7900e-01,  9.4345e-01,  3.1026e-01, -5.6936e-01,  5.1809e-01,\n",
      "         9.4103e-01, -4.7187e-01])\n",
      "Embedding of the fiftieth token:       tensor([-3.7413e+00, -1.7520e+00,  1.0879e+00, -2.6189e-01,  1.2209e+00,\n",
      "        -2.1202e+00,  2.4666e-01, -1.6095e+00, -1.4584e+00, -2.8486e-01,\n",
      "        -4.0003e-02,  5.5437e-01, -3.2988e-01, -1.7677e+00,  1.8240e-02,\n",
      "         7.1387e-01, -2.4287e+00, -5.1276e-01, -1.0957e+00, -1.0621e-01,\n",
      "        -3.9150e-01, -6.3917e-01, -1.6922e+00,  4.6370e-01,  2.7983e+00,\n",
      "        -2.1735e+00,  2.2633e-01, -1.5044e+00,  2.5043e+00,  2.8687e+00,\n",
      "        -5.4855e-01, -1.2493e+00, -1.0729e+00,  4.8591e-01,  6.2581e-01,\n",
      "         7.3945e-01, -2.7724e-01,  3.2579e-01, -3.9838e-01, -1.5016e-01,\n",
      "         4.0103e-01, -1.4956e-01, -1.3609e-01,  1.7004e-01, -3.6427e-01,\n",
      "         2.1721e-01,  6.5192e-02,  8.9158e-01,  1.9804e-01, -7.5707e-01,\n",
      "        -3.0202e-01, -2.5986e-01, -3.5698e+00, -2.7806e-01, -1.1644e+00,\n",
      "        -2.4118e-01, -3.8254e-01,  9.2729e-02,  1.1562e+00, -3.6127e-01,\n",
      "         3.6679e-01, -4.3867e-01, -7.8185e-01, -5.4513e-01,  1.3964e-01,\n",
      "        -7.3291e-01, -6.2211e-01, -2.6527e-01, -1.8252e-01, -1.5734e-01,\n",
      "         8.6029e-01,  1.1440e+00,  2.4581e-01, -5.7757e-01,  1.6544e-02,\n",
      "        -1.2038e-01,  3.0286e-01,  6.6625e-01, -7.7792e-01, -2.0005e-01,\n",
      "         1.6162e-01, -2.8081e-01, -2.4011e-01, -3.9464e-01,  3.5476e-01,\n",
      "        -1.8748e-01,  2.8218e-01, -4.5322e-01, -1.3274e-01, -4.5206e-01,\n",
      "         4.6736e-01,  5.2971e-01,  3.1713e-01, -2.5321e-01,  8.5581e-02,\n",
      "        -2.8297e-01, -7.2676e-01, -2.8202e-02, -7.3362e-01, -9.7544e-01,\n",
      "        -4.3956e-01,  1.7114e-01, -7.3933e-01,  5.1941e-01,  1.0717e+00,\n",
      "        -7.8227e-01,  1.4384e+00,  6.7075e-01, -2.2129e-01,  1.0321e+00,\n",
      "        -4.5814e-01, -5.4777e-01,  7.8797e-03,  7.2205e-01,  5.7920e-01,\n",
      "        -6.2781e-01,  4.7623e-02,  5.9258e-01,  5.4644e-01,  9.5627e-01,\n",
      "         7.7211e-01,  6.6021e-01, -7.4602e-01, -9.2431e-01,  1.4840e-01,\n",
      "        -4.4212e-01,  3.3725e-01,  6.2384e-02, -5.4018e-03,  9.6277e-02,\n",
      "         3.2993e-01, -5.9718e-01,  1.0119e+00,  2.5620e-01,  1.1206e+00,\n",
      "        -7.6352e-01, -8.2646e-01, -4.8976e-01,  7.2192e-01, -1.2910e-01,\n",
      "         3.4164e-01, -6.8374e-01,  7.3341e-01,  9.6608e-01, -5.8582e-01,\n",
      "         1.2115e-01, -4.2796e-02,  5.1138e-01,  4.1575e-01,  5.9013e-01,\n",
      "         6.3282e-01,  2.0153e-01,  2.6146e-01, -3.2249e-01, -5.9907e-01,\n",
      "         4.7676e-01,  3.2556e-01,  5.3057e-01, -7.6710e-02, -6.4532e-01,\n",
      "        -4.8919e-01,  1.9144e-01,  2.0949e-01,  5.0758e-01,  6.0826e-02,\n",
      "         7.0925e-02,  2.9674e-01,  3.8082e-01, -3.2900e-01, -8.4011e-01,\n",
      "         1.0623e-03,  5.4540e-02,  4.0313e-01,  2.6696e-01,  6.9018e-01,\n",
      "         2.3334e-01, -1.4693e-01,  3.9469e-01, -9.0735e-02,  6.1203e-01,\n",
      "         1.1399e-02, -4.8602e-01,  6.0451e-02, -5.1124e-01, -3.3937e-01,\n",
      "        -5.8241e-01, -4.4668e-01, -1.2441e-01,  8.3781e-01,  3.0757e-03,\n",
      "         8.4054e-01, -5.2071e-01, -3.3596e-01,  5.2715e-02, -1.4874e-01,\n",
      "        -2.6059e-02, -5.8843e-02,  5.8849e-02, -3.9703e-01,  3.4632e-01,\n",
      "         1.2173e-01, -3.1256e-01,  6.2137e-01, -5.7000e-01,  2.5655e-01,\n",
      "         4.0296e-01, -8.4895e-01,  3.3583e-01,  8.7760e-01, -3.7603e-01,\n",
      "         3.2345e-01,  9.9356e-01,  1.0282e+00, -8.1056e-01,  7.9826e-01,\n",
      "        -7.5651e-01,  2.5411e-01, -5.1109e-02,  4.4499e-01,  1.8826e-01,\n",
      "        -5.4997e-01, -6.2395e-01, -6.5825e-02, -4.1653e-01, -1.1921e-01,\n",
      "        -1.6639e-01,  2.2670e-01,  5.3948e-02, -1.0959e-01, -4.6128e-01,\n",
      "        -5.8965e-01, -1.5696e-01, -1.3991e-01,  3.1363e-01,  3.1763e-01,\n",
      "         8.8565e-03, -1.2458e-01,  2.0110e-01, -3.0728e-01,  3.3006e-02,\n",
      "        -6.4018e-01, -3.2276e-01, -4.6880e-01,  4.6960e-01, -1.1032e-01,\n",
      "         4.9715e-01, -5.2288e-02, -4.9376e-01, -4.6493e-02,  3.0779e-01,\n",
      "        -1.8611e-01, -2.0577e-01,  4.9825e-01, -2.4776e-01, -2.1613e-01,\n",
      "         7.2030e-01, -4.4467e-02, -4.5000e-02, -3.3078e-01, -1.1163e-01,\n",
      "        -4.0930e-01,  4.9919e-01, -4.2928e-01,  1.1394e-01,  2.9357e-01,\n",
      "        -1.8478e-01,  4.8979e-01,  3.9731e-01,  1.8359e-01,  5.8851e-03,\n",
      "        -1.1300e-01,  5.3262e-01,  1.2057e-01, -5.6521e-02, -4.3332e-01,\n",
      "         5.4306e-03, -1.9340e-01,  2.1756e-01,  3.2480e-01, -4.9140e-01,\n",
      "         3.8860e-01,  1.8530e-01, -1.8853e-01,  4.1961e-01,  5.1867e-01,\n",
      "         8.3060e-01,  4.5078e-01,  5.5106e-01, -8.6119e-02,  1.0205e-01,\n",
      "         4.8085e-01, -4.2561e-01, -3.5402e-01,  1.9525e-01,  3.9231e-01,\n",
      "        -1.7803e-01,  1.9181e-01, -3.3194e-01, -3.6218e-01,  4.5180e-01,\n",
      "        -1.1076e-01,  1.2105e-01,  1.3646e-02, -3.4898e-01, -1.1954e-03,\n",
      "        -3.8473e-01,  8.7495e-02, -4.6495e-01, -3.1510e-01, -3.5296e-01,\n",
      "         5.7931e-01, -3.1476e-01, -9.4901e-02, -3.9190e-01, -6.9694e-01,\n",
      "        -1.0447e-02,  2.3938e-01, -1.7280e-01, -5.3704e-01,  1.0612e+00,\n",
      "         5.0175e-01, -1.1020e-02,  3.8966e-01,  4.2643e-01, -9.9510e-03,\n",
      "         2.0185e-01, -4.3824e-01,  3.2814e-01, -4.8346e-01, -3.2599e-01,\n",
      "         1.8513e-01, -4.6976e-02,  4.5525e-01,  6.4502e-01, -3.2612e-02,\n",
      "         8.7192e-02, -2.1008e-01,  2.8687e-02,  1.1076e-01, -3.0017e-01,\n",
      "        -5.0154e-02,  3.8217e-01,  2.6532e-02,  1.2623e-01,  1.0394e-01,\n",
      "         4.3786e-02, -1.1118e-01,  2.8052e-02, -6.8033e-03, -5.0394e-03,\n",
      "         4.3601e-01,  8.1189e-02, -7.4181e-02, -3.6765e-01, -1.9569e-02,\n",
      "         1.4345e-01,  2.8632e-01, -2.6073e-01,  3.6885e-02, -3.4718e-01,\n",
      "         5.2495e-01,  3.8395e-01,  1.5273e-01, -2.1628e-01,  3.4222e-01,\n",
      "        -3.4717e-01,  1.3506e-01, -2.9593e-01, -4.3897e-01,  6.6618e-02,\n",
      "        -6.0420e-01,  4.6465e-01, -4.1210e-01,  6.5812e-01,  3.3184e-01,\n",
      "         2.2699e-01,  1.5620e-01,  3.7608e-01, -3.8086e-02, -3.2624e-01,\n",
      "        -1.5369e-01, -4.2839e-01, -4.0469e-01, -8.9543e-03, -6.6050e-01,\n",
      "         1.9467e-01,  8.0796e-01, -4.7144e-01, -6.3811e-01,  3.5103e-01,\n",
      "        -3.6293e-01, -2.7268e-01, -3.6439e-01,  5.5532e-02,  1.3991e-01,\n",
      "         8.0138e-01, -5.7100e-01,  1.0608e+00,  2.4445e-01, -7.6118e-01,\n",
      "         2.2254e-02,  1.3983e-02,  6.2986e-01, -7.3683e-02,  8.0212e-01,\n",
      "        -2.8109e-01,  6.6694e-01, -6.7035e-01, -9.6533e-02, -1.9628e-02,\n",
      "         1.3905e-01,  5.9583e-02,  4.8975e-02,  5.6256e-01, -2.9041e-01,\n",
      "         5.6460e-02,  4.1767e-01, -7.3050e-01,  2.9958e-02, -5.7242e-01,\n",
      "         3.7911e-01,  8.6302e-01, -8.6317e-01, -9.3029e-01, -8.1289e-02,\n",
      "        -7.8378e-02, -2.2158e-01, -3.6283e-02,  5.3579e-01, -5.3185e-01,\n",
      "         5.7798e-01,  3.4648e-01,  2.6285e-01, -1.8939e-01, -1.6681e-01,\n",
      "         7.8349e-01,  1.9662e-01,  1.2939e-01, -4.4017e-01,  4.0976e-01,\n",
      "         4.2476e-01, -4.9217e-01,  6.2978e-01, -1.4613e-01, -1.6054e-01,\n",
      "         7.4404e-01,  1.6765e-01, -3.2840e-01,  4.1663e-01,  6.0308e-01,\n",
      "        -1.0861e+00,  5.4388e-01, -9.2215e-01, -4.8429e-01, -1.3947e+00,\n",
      "         2.5982e-01,  1.4184e+00,  3.7120e-01, -9.2127e-01, -1.4159e+00,\n",
      "         1.6531e-02, -6.3792e-01,  5.1167e-01,  3.6167e-01, -1.3585e-01,\n",
      "         6.1094e-01,  2.7156e-01,  4.9552e-01, -1.0393e+00,  2.9700e-01,\n",
      "        -6.7169e-01, -7.1377e-01,  2.5848e-01,  7.3621e-02,  5.0331e-02,\n",
      "         2.2472e-01, -3.8532e-01,  9.9617e-03, -6.7879e-02, -7.7955e-02,\n",
      "        -4.4956e-01,  1.8528e+00,  1.5305e+00,  2.2598e-01, -5.8522e-01,\n",
      "         4.8994e-01,  6.9847e-01, -3.4993e-01,  4.0096e-01,  1.5367e+00,\n",
      "        -1.2456e-01, -9.0872e-01, -4.7142e-01, -1.1398e+00,  1.6423e+00,\n",
      "        -6.9061e-01, -8.8025e-01, -1.3386e+00, -5.0809e-01, -3.1707e-01,\n",
      "         2.0900e-01, -2.8918e-01, -3.6064e-01,  1.5625e-01,  6.1036e-02,\n",
      "        -4.7900e-01,  9.4345e-01,  3.1026e-01, -5.6936e-01,  5.1809e-01,\n",
      "         9.4103e-01, -4.7187e-01])\n"
     ]
    }
   ],
   "source": [
    "sequence = TrainTweetsDataset[0][0]\n",
    "sequence_embeddings = pretrained_embeddings_layer(sequence)\n",
    "\n",
    "print('Embedding of the first token:          {}'.format(sequence_embeddings[0]))\n",
    "print('Embedding of the second token:         {}'.format(sequence_embeddings[1]))\n",
    "print('Embedding of the third token:          {}'.format(sequence_embeddings[2]))\n",
    "print('Embedding of the fourth token:         {}'.format(sequence_embeddings[3]))\n",
    "print('Embedding of the fifth token:          {}'.format(sequence_embeddings[4]))\n",
    "print('Embedding of the sixth token:          {}'.format(sequence_embeddings[5]))\n",
    "print('Embedding of the twenty-fifth token:   {}'.format(sequence_embeddings[24]))\n",
    "print('Embedding of the fortieth token:       {}'.format(sequence_embeddings[39]))\n",
    "print('Embedding of the fiftieth token:       {}'.format(sequence_embeddings[44]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Network with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF column to a torch tensor\n",
    "X_train = torch.FloatTensor(tweets_train['TFIDF'].tolist())\n",
    "# Convert the target column to a torch tensor\n",
    "y_train = torch.FloatTensor(tweets_train['target'].tolist()).unsqueeze(1)\n",
    "\n",
    "# Convert the TF-IDF column to a torch tensor\n",
    "X_test = torch.FloatTensor(tweets_test['TFIDF'].tolist())\n",
    "# Convert the target column to a torch tensor\n",
    "y_test = torch.FloatTensor(tweets_test['target'].tolist()).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch Dataset object\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# Create a torch DataLoader object\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1024)\n",
    "\n",
    "# Create a torch Dataset object\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "# Create a torch DataLoader object\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customFCNN = CustomFCNN(input_size = X_train.shape[1], hidden_size = 2048, dropout_rate = 0.5).to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customFCNN.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customFCNN, train_loader, test_loader, optimizer, criterion, epochs = 32, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LSTM Neural Network with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T15:34:50.418593600Z",
     "start_time": "2023-12-21T15:34:50.412589100Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = skipgram_model,\n",
    "                                  hidden_size = 64, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 1, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6774 | Accuracy = 59.02% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 46.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6843\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7019 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 83.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6768 | Accuracy = 59.02% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7065 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 91.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6840\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7041 | Accuracy = 50.82% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6839\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7041 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 89.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6835\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7111 | Accuracy = 47.54% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7010 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 93.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6907 | Accuracy = 54.10% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6838\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6987 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 88.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6834\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6674 | Accuracy = 62.30% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7021 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6619 | Accuracy = 63.93% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7028 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 90.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6417 | Accuracy = 70.49% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6838\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7027 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7000 | Accuracy = 50.82% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6837\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7003 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 90.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7133 | Accuracy = 47.54% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7024 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 89.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7021 | Accuracy = 49.18% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6983 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 90.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6835\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6760 | Accuracy = 60.66% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 52.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6837\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6972 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 90.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6838\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6725 | Accuracy = 60.66% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6836\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7017 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 88.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6567 | Accuracy = 67.21% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6997 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 90.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7049 | Accuracy = 49.18% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 53.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6835\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7003 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 89.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6832\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6873 | Accuracy = 55.74% | F1-Score = 0.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:02<00:00, 54.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6834\n",
      "Training Accuracy = 57.03%\n",
      "Training F1-Score = 0.00%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7028 | Accuracy = 50.79% | F1-Score = 0.00% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6833\n",
      "Test Accuracy = 57.03%\n",
      "Test F1-Score = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = skipgram_model, \n",
    "                                                      hidden_size = 64, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = False).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_Attention.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6268 | Accuracy = 65.57% | F1-Score = 16.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6743\n",
      "Training Accuracy = 57.40%\n",
      "Training F1-Score = 8.31%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6510 | Accuracy = 55.56% | F1-Score = 22.22% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6568\n",
      "Test Accuracy = 62.43%\n",
      "Test F1-Score = 28.72%\n",
      "\n",
      "Epoch 2/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5190 | Accuracy = 77.05% | F1-Score = 75.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5731\n",
      "Training Accuracy = 72.90%\n",
      "Training F1-Score = 61.80%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3937 | Accuracy = 87.30% | F1-Score = 86.67% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5265\n",
      "Test Accuracy = 75.73%\n",
      "Test F1-Score = 71.12%\n",
      "\n",
      "Epoch 3/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5158 | Accuracy = 75.41% | F1-Score = 66.67% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4592\n",
      "Training Accuracy = 79.43%\n",
      "Training F1-Score = 74.29%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3264 | Accuracy = 90.48% | F1-Score = 89.66% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 96.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4708\n",
      "Test Accuracy = 79.25%\n",
      "Test F1-Score = 73.19%\n",
      "\n",
      "Epoch 4/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3705 | Accuracy = 81.97% | F1-Score = 77.55% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4173\n",
      "Training Accuracy = 81.75%\n",
      "Training F1-Score = 77.51%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3227 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 92.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4593\n",
      "Test Accuracy = 79.68%\n",
      "Test F1-Score = 73.34%\n",
      "\n",
      "Epoch 5/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3323 | Accuracy = 85.25% | F1-Score = 83.02% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3887\n",
      "Training Accuracy = 83.19%\n",
      "Training F1-Score = 79.37%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3224 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4539\n",
      "Test Accuracy = 79.96%\n",
      "Test F1-Score = 73.94%\n",
      "\n",
      "Epoch 6/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3259 | Accuracy = 86.89% | F1-Score = 84.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3627\n",
      "Training Accuracy = 84.64%\n",
      "Training F1-Score = 81.26%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3266 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4537\n",
      "Test Accuracy = 79.93%\n",
      "Test F1-Score = 73.73%\n",
      "\n",
      "Epoch 7/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3547 | Accuracy = 86.89% | F1-Score = 80.95% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3325\n",
      "Training Accuracy = 86.34%\n",
      "Training F1-Score = 83.41%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3401 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4651\n",
      "Test Accuracy = 79.74%\n",
      "Test F1-Score = 73.34%\n",
      "\n",
      "Epoch 8/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3238 | Accuracy = 85.25% | F1-Score = 82.35% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3041\n",
      "Training Accuracy = 87.53%\n",
      "Training F1-Score = 84.94%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3464 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 97.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4673\n",
      "Test Accuracy = 78.85%\n",
      "Test F1-Score = 74.16%\n",
      "\n",
      "Epoch 9/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3289 | Accuracy = 86.89% | F1-Score = 80.00% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2686\n",
      "Training Accuracy = 89.52%\n",
      "Training F1-Score = 87.51%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3749 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4905\n",
      "Test Accuracy = 79.01%\n",
      "Test F1-Score = 74.16%\n",
      "\n",
      "Epoch 10/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.2779 | Accuracy = 86.89% | F1-Score = 88.89% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.2313\n",
      "Training Accuracy = 91.21%\n",
      "Training F1-Score = 89.53%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4015 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5230\n",
      "Test Accuracy = 78.70%\n",
      "Test F1-Score = 73.05%\n",
      "\n",
      "Epoch 11/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1556 | Accuracy = 91.80% | F1-Score = 91.23% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1961\n",
      "Training Accuracy = 92.55%\n",
      "Training F1-Score = 91.18%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4473 | Accuracy = 87.30% | F1-Score = 86.21% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5887\n",
      "Test Accuracy = 77.54%\n",
      "Test F1-Score = 70.43%\n",
      "\n",
      "Epoch 12/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1139 | Accuracy = 96.72% | F1-Score = 97.22% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1638\n",
      "Training Accuracy = 94.22%\n",
      "Training F1-Score = 93.20%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4767 | Accuracy = 84.13% | F1-Score = 83.33% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 103.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6200\n",
      "Test Accuracy = 77.14%\n",
      "Test F1-Score = 70.77%\n",
      "\n",
      "Epoch 13/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1330 | Accuracy = 93.44% | F1-Score = 91.30% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1370\n",
      "Training Accuracy = 95.32%\n",
      "Training F1-Score = 94.51%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5336 | Accuracy = 80.95% | F1-Score = 80.65% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 98.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6587\n",
      "Test Accuracy = 75.39%\n",
      "Test F1-Score = 70.55%\n",
      "\n",
      "Epoch 14/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0745 | Accuracy = 98.36% | F1-Score = 97.30% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.79it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1192\n",
      "Training Accuracy = 95.77%\n",
      "Training F1-Score = 95.04%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5117 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 99.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6985\n",
      "Test Accuracy = 76.03%\n",
      "Test F1-Score = 70.04%\n",
      "\n",
      "Epoch 15/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1669 | Accuracy = 95.08% | F1-Score = 94.12% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.78it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.1016\n",
      "Training Accuracy = 96.40%\n",
      "Training F1-Score = 95.78%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5561 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 102.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7562\n",
      "Test Accuracy = 75.82%\n",
      "Test F1-Score = 69.41%\n",
      "\n",
      "Epoch 16/16\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.1382 | Accuracy = 96.72% | F1-Score = 95.65% | Batch ID = 119 : 100%|██████████| 119/119 [00:04<00:00, 27.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.0899\n",
      "Training Accuracy = 96.86%\n",
      "Training F1-Score = 96.33%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5597 | Accuracy = 85.71% | F1-Score = 84.75% | Batch ID = 51 : 100%|██████████| 51/51 [00:00<00:00, 100.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.7993\n",
      "Test Accuracy = 75.61%\n",
      "Test F1-Score = 69.36%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM_Attention, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "\n",
    "customPreTrainedLSTM_MultiheadAttention = CustomLSTM_MultiHeadAttention(word2vec_model = skipgram_model,\n",
    "                                                                        hidden_size = 1024, \n",
    "                                                                        output_size = 1, \n",
    "                                                                        dropout = 0.1,\n",
    "                                                                        num_layers = 1, \n",
    "                                                                        bidirectional = True,\n",
    "                                                                        freeze_embeddings = True,\n",
    "                                                                        num_heads = 16).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM_MultiheadAttention.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(model = customPreTrainedLSTM_MultiheadAttention, \n",
    "                                                       train_loader = TrainDataLoader_SkipGram, \n",
    "                                                       test_loader = TestDataLoader_SkipGram, \n",
    "                                                       optimizer = optimizer, \n",
    "                                                       loss_func = criterion, \n",
    "                                                       epochs = 16, \n",
    "                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM = CustomLSTM(word2vec_model = cbow_model,\n",
    "                                  hidden_size = 64, \n",
    "                                  output_size = 1, \n",
    "                                  num_layers = 1, \n",
    "                                  bidirectional = True,\n",
    "                                  freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedLSTM.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedLSTM_Attention = CustomLSTM_Attention(word2vec_model = cbow_model, \n",
    "                                                      hidden_size = 64, \n",
    "                                                      output_size = 1, \n",
    "                                                      num_layers = 1, \n",
    "                                                      bidirectional = True,\n",
    "                                                      freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(customPreTrainedLSTM_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedLSTM_Attention, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GRU Neural Network with Custom Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_SkipGram = TweetsDataset(tweets_train, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_SkipGram, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_SkipGram = TweetsDataset(tweets_test, 'skipgram')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_SkipGram = torch.utils.data.DataLoader(dataset = TestTweetsDataset_SkipGram, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = skipgram_model, \n",
    "                                hidden_size = 64, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 1, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = skipgram_model, \n",
    "                                                    hidden_size = 64, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 1, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU_Attention, TrainDataLoader_SkipGram, TestDataLoader_SkipGram, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTweetsDataset_CBOW = TweetsDataset(tweets_train, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TrainDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TrainTweetsDataset_CBOW, batch_size = 64, shuffle = True)\n",
    "\n",
    "TestTweetsDataset_CBOW = TweetsDataset(tweets_test, 'cbow')\n",
    "# Create a dataloade for the training dataset\n",
    "TestDataLoader_CBOW = torch.utils.data.DataLoader(dataset = TestTweetsDataset_CBOW, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU = CustomGRU(word2vec_model = cbow_model, \n",
    "                                hidden_size = 64, \n",
    "                                output_size = 1, \n",
    "                                num_layers = 1, \n",
    "                                bidirectional = True,\n",
    "                                freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "customPreTrainedGRU_Attention = CustomGRU_Attention(word2vec_model = cbow_model, \n",
    "                                                    hidden_size = 64, \n",
    "                                                    output_size = 1, \n",
    "                                                    num_layers = 1, \n",
    "                                                    bidirectional = True,\n",
    "                                                    freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(customPreTrainedGRU_Attention.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train(customPreTrainedGRU_Attention, TrainDataLoader_CBOW, TestDataLoader_CBOW, optimizer, criterion, epochs = 8, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistilBERTbase_name = \"distilbert-base-uncased\"\n",
    "DistilBERTbase_tokenizer = DistilBertTokenizer.from_pretrained(DistilBERTbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = DistilBERTbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = DistilBERTbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a model savel with extentions .bin\n",
    "DistilBERTbase = DistilBertForSequenceClassification.from_pretrained(DistilBERTbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "DistilBERTbase = DistilBERTbase.to(device)\n",
    "optimizer = torch.optim.AdamW(DistilBERTbase.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(DistilBERTbase, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "# Reference: https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base (110M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTbase_name = \"bert-base-uncased\"\n",
    "BERTbase_tokenizer = BertTokenizer.from_pretrained(BERTbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = BERTbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = BERTbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model for sequence classification\n",
    "BERTbase = BertForSequenceClassification.from_pretrained(BERTbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTbase = BERTbase.to(device)\n",
    "optimizer = torch.optim.AdamW(BERTbase.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(BERTbase, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT large (340M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815ac4716084ad69fdd51b0deed1990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208634a2dcfe496eae0822afbc326136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18d0d462e5746d39e748e95d94c14dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BERTlarge_name = \"bert-large-uncased\"\n",
    "BERTlarge_tokenizer = BertTokenizer.from_pretrained(BERTlarge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = BERTlarge_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = BERTlarge_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa5f9cec3a541f39c73961f4f096dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model for sequence classification\n",
    "BERTlarge = BertForSequenceClassification.from_pretrained(BERTlarge_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTlarge = BERTlarge.to(device)\n",
    "optimizer = torch.optim.AdamW(BERTlarge.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(BERTlarge, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTAbase_name = \"roberta-base\"\n",
    "ROBERTAbase_tokenizer = RobertaTokenizer.from_pretrained(ROBERTAbase_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = ROBERTAbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = ROBERTAbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the roBERTa model for sequence classification\n",
    "ROBERTAbase = RobertaForSequenceClassification.from_pretrained(ROBERTAbase_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "ROBERTAbase = ROBERTAbase.to(device)\n",
    "optimizer = torch.optim.AdamW(ROBERTAbase.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(ROBERTAbase, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roBERTa Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTAlarge_name = \"roberta-large\"\n",
    "ROBERTAlarge_tokenizer = RobertaTokenizer.from_pretrained(ROBERTAlarge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERT(tokenizer = ROBERTAlarge_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERT(tokenizer = ROBERTAlarge_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the roBERTa model for sequence classification\n",
    "ROBERTAlarge = RobertaForSequenceClassification.from_pretrained(ROBERTAlarge_name, num_labels = 2, output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "ROBERTAlarge = ROBERTAlarge.to(device)\n",
    "optimizer = torch.optim.AdamW(ROBERTAlarge.parameters(), lr = 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERT(ROBERTAlarge, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "BERTweetbase_name = \"vinai/bertweet-base\"\n",
    "BERTweetbase_tokenizer = AutoTokenizer.from_pretrained(BERTweetbase_name, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERTweet(tokenizer = BERTweetbase_tokenizer, df = tweets_train, batch_size = 32, shuffle = True)\n",
    "TestDataset, TestDataLoader = TokenizeBERTweet(tokenizer = BERTweetbase_tokenizer, df = tweets_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERTweetbase = AutoModel.from_pretrained(BERTweetbase_name)\n",
    "BERTweetbase = BERTweetForSequenceClassification(bertweet_model = BERTweetbase, hidden_size = 768, output_size = 1, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTweetbase = BERTweetbase.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(BERTweetbase.parameters(), lr = 5e-5, weight_decay = 0.05)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoader) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoader) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3734 | Accuracy = 86.21% | F1-Score = 81.82% | Batch ID = 238 : 100%|██████████| 238/238 [01:06<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4613\n",
      "Training Accuracy = 79.13%\n",
      "Training F1-Score = 74.86%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3537 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.4097\n",
      "Test Accuracy = 82.41%\n",
      "Test F1-Score = 79.32%\n",
      "\n",
      "Epoch 2/2\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4774 | Accuracy = 82.76% | F1-Score = 80.00% | Batch ID = 238 : 100%|██████████| 238/238 [01:06<00:00,  3.59it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.3253\n",
      "Training Accuracy = 87.22%\n",
      "Training F1-Score = 84.34%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.4642 | Accuracy = 87.10% | F1-Score = 90.48% | Batch ID = 102 : 100%|██████████| 102/102 [00:06<00:00, 16.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.3962\n",
      "Test Accuracy = 83.88%\n",
      "Test F1-Score = 79.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERTweet(model = BERTweetbase, \n",
    "                                                                train_loader = TrainDataLoader, \n",
    "                                                                test_loader = TestDataLoader, \n",
    "                                                                optimizer = optimizer, \n",
    "                                                                scheduler = scheduler,\n",
    "                                                                loss_func = criterion, \n",
    "                                                                epochs = 2, \n",
    "                                                                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "BERTweetlarge_name = \"vinai/bertweet-large\"\n",
    "BERTweetlarge_tokenizer = AutoTokenizer.from_pretrained(BERTweetlarge_name, use_fast = False)\n",
    "\n",
    "# Initialize base BERTweet tokenizer to normalize tweets (large version doesn't do it)\n",
    "BERTweetbase_name = \"vinai/bertweet-base\"\n",
    "BERTweetbase_tokenizer = AutoTokenizer.from_pretrained(BERTweetbase_name, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "TrainDataset, TrainDataLoader = TokenizeBERTweet(tokenizer = BERTweetlarge_tokenizer, \n",
    "                                                 df = tweets_train, \n",
    "                                                 batch_size = 32, \n",
    "                                                 shuffle = True, \n",
    "                                                 tokenizer_normalizeTweet = BERTweetbase_tokenizer)\n",
    "\n",
    "TestDataset, TestDataLoader = TokenizeBERTweet(tokenizer = BERTweetlarge_tokenizer, \n",
    "                                               df = tweets_test, \n",
    "                                               batch_size = 32, \n",
    "                                               shuffle = False, \n",
    "                                               tokenizer_normalizeTweet = BERTweetbase_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERTweetlarge = AutoModelForSequenceClassification.from_pretrained(BERTweetlarge_name)\n",
    "BERTweetlarge = AutoModel.from_pretrained(BERTweetlarge_name)\n",
    "BERTweetlarge = BERTweetForSequenceClassification(bertweet_model = BERTweetlarge, hidden_size = 1024, output_size = 1, dropout_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "device = set_device()\n",
    "BERTweetlarge = BERTweetlarge.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(BERTweetlarge.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERTweet(BERTweetlarge, TrainDataLoader, TestDataLoader, optimizer, criterion, epochs = 2, device = device)\n",
    "#train_losses, train_f1s, test_losses, test_f1s = train_BERT(BERTweetlarge, TrainDataLoader, TestDataLoader, optimizer, epochs = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTweetbase_8115f1 = torch.load('/Users/luish/Desktop/BERTweetbase_0.8115f1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:06<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true, y_hat = predict(BERTweetbase_8115f1, TestDataLoader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "╒═══════════════════════╤════════════════════════════╤══════════════════════╕\n",
      "│                       │  Predicted Not a Disaster  │  Predicted Disaster  │\n",
      "╞═══════════════════════╪════════════════════════════╪══════════════════════╡\n",
      "│ Actual Not a Disaster │            1665            │         311          │\n",
      "├───────────────────────┼────────────────────────────┼──────────────────────┤\n",
      "│    Actual Disaster    │            196             │         1091         │\n",
      "╘═══════════════════════╧════════════════════════════╧══════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "ComputeConfusionMatrix(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════╤═════════╕\n",
      "│       Metric       │  Value  │\n",
      "╞════════════════════╪═════════╡\n",
      "│      Accuracy      │ 84.46%  │\n",
      "├────────────────────┼─────────┤\n",
      "│ Weighted Precision │ 77.82%  │\n",
      "├────────────────────┼─────────┤\n",
      "│  Weighted Recall   │ 84.77%  │\n",
      "├────────────────────┼─────────┤\n",
      "│ Weighted F1 Score  │ 81.15%  │\n",
      "╘════════════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "ComputeClassificationMetrics(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERTweet model saved locally\n",
    "BERTweetbase_8115f1 = torch.load('/Users/luish/Desktop/BERTweetbase_0.8115f1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ComputeAttentionMaps(dataset = TestDataset, model = BERTweetbase_8115f1, input_id = 0, num_head = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transformer Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T23:11:27.031219500Z",
     "start_time": "2023-12-29T23:11:27.022593400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Output layer for binary classification\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_mask = self.generate_mask(src)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Global average pooling across sequence length\n",
    "        pooled_output = F.adaptive_avg_pool1d(enc_output.permute(0, 2, 1), 1).view(enc_output.size(0), -1)\n",
    "\n",
    "        # Output layer with sigmoid activation for binary classification\n",
    "        output = self.fc(pooled_output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDatasetEncoderTransformer(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, word2vec_model, attn_masks):\n",
    "        self.df = df\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.attn_masks = attn_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx, -1 if self.word2vec_model == 'skipgram' else -2]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        attn_mask = self.attn_masks[idx]\n",
    "\n",
    "        # Convert sequence to a 1D tensor\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Convert label to a 1D tensor (scalar)\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        # Convert attention mask to a 1D tensor (scalar)\n",
    "        attn_mask_tensor = attn_mask.flatten().bool()\n",
    "\n",
    "        return sequence_tensor, attn_mask_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_train, 'skipgram', attn_masks_train)\n",
    "TestDatasetEncoder = TweetsDatasetEncoderTransformer(tweets_test, 'skipgram', attn_masks_test)\n",
    "\n",
    "TrainDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TrainDatasetEncoder, batch_size = 64, shuffle = True)\n",
    "TestDataLoaderEncoder = torch.utils.data.DataLoader(dataset = TestDatasetEncoder, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T23:11:42.408492600Z",
     "start_time": "2023-12-29T23:11:42.384459100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderTransformer(torch.nn.Module):\n",
    "    def __init__(self, word2vec_model, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout = 0.1, freeze_embeddings = True):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze = freeze_embeddings)\n",
    "        self.position_embeddings = torch.nn.Embedding(max_seq_length, d_model)\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(d_model, eps = 1e-8, elementwise_affine = True)\n",
    "        self.BatchNorm = torch.nn.BatchNorm1d(d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers, norm = self.LayerNorm)\n",
    "\n",
    "        self.dense = torch.nn.Linear(d_model, d_model)\n",
    "        self.classifier = torch.nn.Linear(d_model, 1)\n",
    "\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.Tanh = torch.nn.Tanh()\n",
    "        self.LeakyReLU = torch.nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, tweet, attention_mask):\n",
    "    \n",
    "        # Get the embeddings of the tokens\n",
    "        embeddings = self.embeddings(tweet)\n",
    "        # Get the positional embeddings\n",
    "        position_ids = torch.arange(tweet.size(1)).unsqueeze(0).to(tweet.device)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Add the positional embeddings to the token embeddings\n",
    "        embeddings += position_embeddings\n",
    "        # Apply layer normalization and dropout\n",
    "        embeddings = self.dropout(self.LayerNorm(embeddings))\n",
    "\n",
    "        # Apply the transformer encoder\n",
    "        encoder_output = self.encoder(embeddings, src_key_padding_mask = attention_mask.T)\n",
    "\n",
    "        # Apply global average pooling across the sequence length\n",
    "        pooled_output = torch.nn.functional.adaptive_avg_pool1d(encoder_output.permute(0, 2, 1), 1).view(encoder_output.size(0), -1)\n",
    "\n",
    "        # Apply a dense layer followed by a non-linear activation function\n",
    "        output = self.BatchNorm(self.ReLU(self.dense(pooled_output)))\n",
    "        # Apply dropour\n",
    "        output = self.dropout(output)\n",
    "        # Apply the classifier layer\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = set_device()\n",
    "TransformerEncoder = EncoderTransformer(word2vec_model = skipgram_model,\n",
    "                                        d_model = 512,\n",
    "                                        num_heads = 16,\n",
    "                                        num_layers = 8,\n",
    "                                        d_ff = 2048,\n",
    "                                        max_seq_length = 45,\n",
    "                                        dropout = 0.1,\n",
    "                                        freeze_embeddings = True).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(TransformerEncoder.parameters(), lr = 5e-5, weight_decay = 0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer, \n",
    "                                            num_warmup_steps = int(0.1 * len(TrainDataLoaderEncoder) * 2), \n",
    "                                            num_training_steps = len(TrainDataLoaderEncoder) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.6171 | Accuracy = 63.93% | F1-Score = 62.07% | Batch ID = 119 : 100%|██████████| 119/119 [00:19<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.6741\n",
      "Training Accuracy = 57.72%\n",
      "Training F1-Score = 52.52%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5298 | Accuracy = 73.02% | F1-Score = 62.22% | Batch ID = 51 : 100%|██████████| 51/51 [00:02<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.6799\n",
      "Test Accuracy = 62.09%\n",
      "Test F1-Score = 21.56%\n",
      "\n",
      "Epoch 2/4\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5169 | Accuracy = 81.97% | F1-Score = 81.97% | Batch ID = 119 : 100%|██████████| 119/119 [00:19<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5370\n",
      "Training Accuracy = 73.79%\n",
      "Training F1-Score = 69.35%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.7990 | Accuracy = 69.84% | F1-Score = 55.81% | Batch ID = 51 : 100%|██████████| 51/51 [00:02<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 1.1802\n",
      "Test Accuracy = 61.42%\n",
      "Test F1-Score = 18.62%\n",
      "\n",
      "Epoch 3/4\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5291 | Accuracy = 73.77% | F1-Score = 70.37% | Batch ID = 119 : 100%|██████████| 119/119 [00:18<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.5019\n",
      "Training Accuracy = 75.98%\n",
      "Training F1-Score = 71.48%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3467 | Accuracy = 87.30% | F1-Score = 87.50% | Batch ID = 51 : 100%|██████████| 51/51 [00:02<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5293\n",
      "Test Accuracy = 73.67%\n",
      "Test F1-Score = 70.45%\n",
      "\n",
      "Epoch 4/4\n",
      "======== Training phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.5886 | Accuracy = 70.49% | F1-Score = 57.14% | Batch ID = 119 : 100%|██████████| 119/119 [00:18<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss = 0.4944\n",
      "Training Accuracy = 76.82%\n",
      "Training F1-Score = 72.27%\n",
      "======== Validation phase ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.3464 | Accuracy = 88.89% | F1-Score = 87.27% | Batch ID = 51 : 100%|██████████| 51/51 [00:02<00:00, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cross-Entropy Loss = 0.5844\n",
      "Test Accuracy = 72.97%\n",
      "Test F1-Score = 57.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_f1s, test_losses, test_f1s = train_BERTweet(model = TransformerEncoder, \n",
    "                                                                train_loader = TrainDataLoaderEncoder, \n",
    "                                                                test_loader = TestDataLoaderEncoder, \n",
    "                                                                optimizer = optimizer, \n",
    "                                                                scheduler = scheduler,\n",
    "                                                                loss_func = criterion, \n",
    "                                                                epochs = 4, \n",
    "                                                                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, test_loader, num_epochs=5, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for data in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs}\"):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().detach().numpy())\n",
    "            true_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader, desc=\"Validation\"):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_predictions.extend(outputs.cpu().detach().numpy())\n",
    "                val_true_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_accuracy = accuracy_score(true_labels, np.round(predictions))\n",
    "        val_accuracy = accuracy_score(val_true_labels, np.round(val_predictions))\n",
    "\n",
    "        train_f1 = f1_score(true_labels, np.round(predictions))\n",
    "        val_f1 = f1_score(val_true_labels, np.round(val_predictions))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\"\n",
    "              f\" Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f},\"\n",
    "              f\" Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
